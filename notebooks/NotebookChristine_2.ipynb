{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# ChatGPT API\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gpt4all\n",
    "#!pip install gpt4all langchain==0.0.342\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'api key'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key  = API_KEY\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f\"\"\"\n",
    "One side of the armed conflicts is composed mainly of the Sudanese military and the Janjaweed , \\\n",
    "a Sudanese militia group recruited mostly from the Afro-Arab Abbala tribes of the northern Rizeigat region in Sudan .\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You will simplify this text in 6 different levels for the dyslexia patience\\\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "level 1 - ...\n",
    "level 2 - …\n",
    "…\n",
    "Step N - …\n",
    "\n",
    "If the text does not contain a sequence of instructions, \\\n",
    "then simply write \\\"No steps provided.\\\"\n",
    "\n",
    "\\\"\\\"\\\"{text}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = f\"\"\"\n",
    "When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landscape.\n",
    "The floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. The numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. Also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches.\n",
    "At each end of the room, on the wall, hung a beautiful bear-skin rug.\n",
    "These rugs were for prizes, one for the girls and one for the boys. And this was the game.\n",
    "The girls were gathered at one end of the room and the boys at the other, and one end was called the North Pole, and the other the South Pole. Each player was given a small flag which they were to plant on reaching the Pole.\n",
    "This would have been an easy matter, but each traveller was obliged to wear snowshoes.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will simplify the text in 3 different difficulty level for dyslexia people:\n",
    "0 is not dyslexia\n",
    "1 is  dyslexia\n",
    "\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Level 1- ...\n",
    "Level 2 - …\n",
    "…\n",
    "Level N - …\n",
    "\n",
    "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text 2:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = f\"\"\"\n",
    "When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landscape.\n",
    "The floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. The numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. Also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches.\n",
    "At each end of the room, on the wall, hung a beautiful bear-skin rug.\n",
    "These rugs were for prizes, one for the girls and one for the boys. And this was the game.\n",
    "The girls were gathered at one end of the room and the boys at the other, and one end was called the North Pole, and the other the South Pole. Each player was given a small flag which they were to plant on reaching the Pole.\n",
    "This would have been an easy matter, but each traveller was obliged to wear snowshoes.\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will simplify the text in 4 different difficulty levels  :\n",
    "Level 1 : Child\n",
    "Level 2 : Teen\n",
    "Level 3 : College students\n",
    "Level 4 : Grad Students\n",
    "Level 5 : Expert\n",
    "\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Level 1- ...\n",
    "Level 2 - …\n",
    "…\n",
    "Level N - …\n",
    "\n",
    "\n",
    "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text 2:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = f\"\"\"\n",
    "Hans Holbein the Younger acted as a matchmaker to Henry VIII,\n",
    "tasked with painting the portraits of potential wives across Europe in a bid to safeguard the future of the throne. \\\n",
    "As two new exhibitions open, Deborah Nicholls-Lee looks at how he was the pre-eminent image-maker of the 16th Century.\\\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "You will simplify the text in 4 different difficulty levels for people who have dyslexia :\n",
    "Level 1 : Child\n",
    "Level 2 : Teen\n",
    "Level 3 : College students\n",
    "Level 4 : Grad Students\n",
    "Level 5 : Expert\n",
    "\n",
    "\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Level 1- ...\n",
    "Level 2 - …\n",
    "…\n",
    "Level N - …\n",
    "\n",
    "\n",
    "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text 2:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_2 = f\"\"\"\n",
    "Hans Holbein the Younger acted as a matchmaker to Henry VIII,\n",
    "tasked with painting the portraits of potential wives across Europe in a bid to safeguard the future of the throne. \\\n",
    "As two new exhibitions open, Deborah Nicholls-Lee looks at how he was the pre-eminent image-maker of the 16th Century.\\\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "\n",
    "you will based on read ability level from pervious model to simplified the text_2:\n",
    "\n",
    "the criteria is the following:\n",
    "\n",
    "normal text : one side of the armed conflicts is composed mainly of the sudanese military and the janjaweed , a sudanese militia group recruited mostly from the afro-arab abbala tribes of the northern rizeigat region in sudan .\n",
    "jeddah is the principal gateway to mecca , islam 's holiest city , which able-bodied muslims are required to visit at least once in their lifetime .\n",
    "the great dark spot is thought to represent a hole in the methane cloud deck of neptune .\n",
    "his next work , saturday , follows an especially eventful day in the life of a successful neurosurgeon .\n",
    "the tarantula , the trickster character , spun a black cord and , attaching it to the ball , crawled away fast to the east , pulling on the cord with all his strength .\n",
    "there he died six weeks later , on 13 january 888.\n",
    "\n",
    "simplified text : who is involved in the conflict one side of the conflict is composed mainly of the janjaweed , a militia group recruited from the arab tribes who move from place to place herding camels .\n",
    "jeddah is the main gateway to mecca , the holiest city of islam , where able-bodied muslims must go to at least once in a lifetime .\n",
    "the great dark spot is thought to be a hole in the methane cloud deck of neptune .\n",
    "his latest novel , saturday , follows an especially eventful day in the life of a neurosurgeon .\n",
    "the tarantula , who knew what to do , spun a black cord and , attaching it to the ball , crawled away fast to the east , pulling on the cord with all his strength .\n",
    "he died a few weeks later in january 888.\n",
    "\n",
    "simplified text level 0 : one side of the armed conflicts consist of the sudanese military and the sudanese militia group janjaweed .\n",
    "jeddah is the main gateway to mecca , islam 's holiest city , which able-bodied muslims are supposed to viisit at least once in their lifetime .\n",
    "the great dark spot is thought to show a hole in the methane cloud deck of neptune .\n",
    "his next work is saturday which is an eventful day in the life of a neurosuregon .\n",
    "the tricky tarantula spun a black web and attached it to the ball . afterwards , it crawled away and pulled the web with him .\n",
    "he died there six weeks later on january 13 , 888.\n",
    "\n",
    "simplified text level 1 : one side of the armed conflicts is mainly sudanese military and the janjaweed , which recruited from the afro-arab abbala tribes .\n",
    "jeddah is the main entrance to mecca , islam 's holiest city , which pure muslims are required to visit at least once in their lifetime .\n",
    "the great dark spot is thought to be a hole in the methane cloud deck of neptune .\n",
    "his next work , saturday , follows a fun day in the life of a good doctor .\n",
    "the tarantula spun a black cord and crawled away fast to the east while pulling on the cord with its strength .\n",
    "here he died six weeks later , on 13 january 888.\n",
    "\n",
    "simplified text level 2 : one side of the armed conflicts is composed mainly of the sudanese military and the janjaweed , a sudanese militia group recruited mostly from the afro-arab abbala tribes in sudan .\n",
    "jeddah is the gateway to mecca which is islam 's holiest city and muslims are required to visit at least once .\n",
    "the great dark spot is thought to represent a hole in the methane .\n",
    "his next work , saturday , will show how very busy and varying the life of a brain doctor is .\n",
    "the trickster spun a black cord and , attaching it to the ball , crawled away fast to the east , pulling on the cord with all his strength .\n",
    "he died six weeks later on january 13th 888.\n",
    "\n",
    "\n",
    "you will based on the citeria provided above to generate the simplified text for dyslexia people:\n",
    "\n",
    "Level 1 : simplified text level 2\n",
    "Level 2 : simplified text level 1\n",
    "Level 3 : simplified text level 0\n",
    "Level 4 : normal text\n",
    "\n",
    "\n",
    "re-write those instructions in the following format:\n",
    "\n",
    "Level 1- ...\n",
    "Level 2 - …\n",
    "…\n",
    "Level N - …\n",
    "\n",
    "\n",
    "\\\"\\\"\\\"{text_2}\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(\"Completion for Text 2:\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# LangChain\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use GPT4ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_path=(\"/Users/christine/Downloads/mistral-7b-openorca.Q4_0.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "\n",
    "# Verbose is required to pass to the callback manager\n",
    "llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create LLM chain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The young people came back into the ballroom and saw it had changed. It looked like a winter scene with lots of snow on the floor. There were many palms and evergreens, covered in fake snow (flour) and cotton to look like real snow. They even added diamond dust and crystal icicles for decoration. At each end of the room there was a bear-skin rug which would be given as prizes - one for girls and one for boys. The game was to reach your pole, either North or South Pole, while wearing snowshoes."
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The young people came back into the ballroom and saw it had changed. It looked like a winter scene with lots of snow on the floor. There were many palms and evergreens, covered in fake snow (flour) and cotton to look like real snow. They even added diamond dust and crystal icicles for decoration. At each end of the room there was a bear-skin rug which would be given as prizes - one for girls and one for boys. The game was to reach your pole, either North or South Pole, while wearing snowshoes.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# request 1\n",
    "question1 = \"\"\"can you simplify this text for dyslexia people:\n",
    "When the young people returned to the ballroom, it presented a decidedly changed appearance. Instead of an interior scene, it was a winter landscape.\n",
    "The floor was covered with snow-white canvas, not laid on smoothly, but rumpled over bumps and hillocks, like a real snow field. The numerous palms and evergreens that had decorated the room, were powdered with flour and strewn with tufts of cotton, like snow. Also diamond dust had been lightly sprinkled on them, and glittering crystal icicles hung from the branches.\n",
    "At each end of the room, on the wall, hung a beautiful bear-skin rug.\n",
    "These rugs were for prizes, one for the girls and one for the boys. And this was the game.\n",
    "The girls were gathered at one end of the room and the boys at the other, and one end was called the North Pole, and the other the South Pole. Each player was given a small flag which they were to plant on reaching the Pole.\n",
    "This would have been an easy matter, but each traveller was obliged to wear snowshoes.\"\"\"\n",
    "\n",
    "llm_chain.run(question1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sure, I can simplify the text for dyslexia people. Here is a simplified version of the text:\n",
      "\n",
      "Owls are birds that belong to the order Strigiformes. They eat small mammals, insects, and other birds. Some owls specialize in hunting fish."
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nSure, I can simplify the text for dyslexia people. Here is a simplified version of the text:\\n\\nOwls are birds that belong to the order Strigiformes. They eat small mammals, insects, and other birds. Some owls specialize in hunting fish.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# request 2\n",
    "question2 = \"\"\"can you simplify this text for dyslexia people:\n",
    "Owls are the order Strigiformes, comprising 200 bird of prey species.\n",
    "Owls hunt mostly small mammals, insects, and other birds though some species specialize in hunting fish.\"\"\"\n",
    "\n",
    "llm_chain.run(question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "\n",
    "# Load content from Wikipedia using WikipediaLoader\n",
    "loader = WikipediaLoader(\"Machine_learning\")\n",
    "docs= loader.load()\n",
    "\n",
    "#split in chunks of 2000 characters - max input size for GPT 2000 and a bit\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(docs)\n",
    "\n",
    "#get embeddings\n",
    "embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "persist_directory = 'db/chroma_3/'\n",
    "\n",
    "# Create the vector store\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Please generate a short summary of machine learning\n",
    "for the dyslexic people.\n",
    "\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Human: {question}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "input_variables=[\"context\",  \"question\"], template=template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieval\n",
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectordb.as_retriever(search_kwargs={\"k\": 5}),\n",
    "    return_source_documents=True,\n",
    "    verbose=False,\n",
    "    chain_type_kwargs={\n",
    "        \"verbose\": False,\n",
    "        \"prompt\": prompt\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = qa(\"generate a 100 words simplifiy text of Machine_learning for dyslexic students\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "model_path = \"/Users/christine/Downloads/gpt4all-falcon-q4_0.gguf\"\n",
    "llm = GPT4All(model=model_path, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nMr. Grimes was supposed to visit Sir John Harthover's house the next morning for his old chimney sweep had gone to prison and the chimneys needed sweeping. Tom was curious about why the chimney sweep went to prison but did not have time to ask before Mr. Grimes rode away. The groom, who looked very neat and clean, with his drab gaiters, drab breeches, drab jacket, snow-white tie with a smart pin in it, and clean round ruddy face, made Tom feel offended and disgusted at his appearance. He considered the groom a stuck-up fellow who gave himself airs because he wore smart clothes, and other people paid for them. Tom went behind the wall to fetch the half-brick after all, but did not remember that he had come in the way of business, and was, as it were, under a flag of truce.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = \" please simplify the following text for dyslexic patients :\\\n",
    "Mr. Grimes was to come up next morning to Sir John Harthover's, \\\n",
    "at the Place, for his old chimney-sweep was gone to prison, \\\n",
    "and the chimneys wanted sweeping. And so he rode away, \\\n",
    "not giving Tom time to ask what the sweep had gone to prison for, \\\n",
    "which was a matter of interest to Tom, as he had been in prison once or twice himself. \\\n",
    "Moreover, the groom looked so very neat and clean, with his drab gaiters, drab breeches, \\\n",
    "drab jacket, snow-white tie with a smart pin in it, and clean round ruddy face, that Tom was offended and disgusted at his appearance, and considered him a stuck-up fellow, who gave himself airs because he wore smart clothes, and other people paid for them; and went behind the wall to fetch the half-brick after all; but did not, remembering that he had come in the way of business, and was, as it were, under a flag of truce.\"\n",
    "\n",
    "llm = GPT4All(model=model_path, verbose=False)\n",
    "llm_chain = LLMChain(llm=llm, prompt=PromptTemplate.from_template(prompt_template))\n",
    "llm_chain.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"/Users/christine/Downloads/test.pdf\")\n",
    "documents = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)\n",
    "text = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='1START  ENTREPRENEURIAL MANAGEMENT Building a startup is an exercise in ins>tu>on building; thus, it necessarily involves management. This oHen comes as a surprise to aspiring entrepreneurs, because their associa>ons with these two words are so diametrically opposed. Entrepreneurs are rightly wary of implemen>ng tradi>onal management prac>ces early on in a startup, afraid that they will invite bureaucracy or s>ﬂe crea>vity. Entrepreneurs have been trying to ﬁt the square peg of their unique problems into the round hole of general management for decades. As a result, many entrepreneurs take a “just do it” aRtude, avoiding all forms of management, process, and discipline. Unfortunately, this approach leads to chaos more oHen than it does to success. I should know: my ﬁrst startup failures were all of this kind. The tremendous success of general management over the last century has provided unprecedented material abundance, but those management principles are ill suited to handle the chaos and uncertainty that', metadata={'source': '/Users/christine/Downloads/test.pdf', 'page': 0}), Document(page_content='are ill suited to handle the chaos and uncertainty that startups must face. I believe that entrepreneurship requires a managerial discipline to harness the entrepreneurial opportunity we have been given. There are more entrepreneurs opera>ng today than at any previous >me in history. This has been made possible by drama>c previous >me in history. This has been made possible by drama>c changes in the global economy. To cite but one example, one oHen hears commentators lament the loss of manufacturing jobs in the United States over the previous two decades, but one rarely hears about a corresponding loss of manufacturing capability. That’s because total manufacturing output in the United States is increasing (by 15 percent in the last decade) even as jobs con>nue to be lost (see the charts below). In eﬀect, the huge produc>vity increases made possible by modern management and technology have created more produc>ve capacity than ﬁrms know what to do with.1 We are living through an unprecedented worldwide', metadata={'source': '/Users/christine/Downloads/test.pdf', 'page': 0}), Document(page_content='to do with.1 We are living through an unprecedented worldwide entrepreneurial renaissance, but this opportunity is laced with peril. Because we lack a coherent management paradigm for new innova>ve ventures, we’re throwing our excess capacity around with wild abandon. Despite this lack of rigor, we are ﬁnding some ways to make money, but for every success there are far too many failures: products pulled from shelves mere weeks aHer being launched, high-proﬁle startups lauded in the press and forgo\\\\en a few months later, and new products that wind up being used by nobody. What makes these failures par>cularly painful is not just the economic damage done to individual employees, companies, and investors; they are also a colossal waste of our civiliza>on’s most precious resource: the >me, passion, and skill of its people. The Lean Startup movement is dedicated to preven>ng these failures.', metadata={'source': '/Users/christine/Downloads/test.pdf', 'page': 0})]\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "db = Chroma.from_documents(text, embeddings, persist_directory=\"db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import GPT4All\n",
    "model_path = \"/Users/christine/Downloads/gpt4all-falcon-q4_0.gguf\"\n",
    "llm = GPT4All(model=model_path, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=db.as_retriever(search_kwargs={\"k\": 3}),\n",
    "    return_source_documents=True,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLaMA ERROR: The prompt is 2071 tokens and the context window is 2048!\n"
     ]
    }
   ],
   "source": [
    "res = qa('''\n",
    "        Please simplify the text in the PDF I just gave you. These are the criteria for the text difficulty level.\n",
    "\n",
    "        Here is an example of an easy text:\n",
    "        \"The boys left the capitol and made their way down the long hill to the main business part of the town. \\\n",
    "        As they struck onto the main business street, Garry noticed the familiar blue bell sign of the telephone company.\\\n",
    "        \"Say, boys, I have an idea. Let's stop in here and put in long distance calls and say hello to our folks. \\\n",
    "        How does the idea strike you?\" said Garry, almost in one breath.\\\n",
    "        \"Ripping,\" shouted Phil, while Dick didn't wait to make any remark, but dived in through the door, \\\n",
    "        and in a trice was putting in his call. Phil followed suit, while Garry waited, as he would talk when Dick had finished.\\\n",
    "        This pleasant duty done, they went to a restaurant for dinner. Here they attracted no little attention, \\\n",
    "        for their khaki clothes looked almost like uniforms. Added to this was the fact that they wore forest shoepacks, \\\n",
    "        those high laced moccasins with an extra leather sole, and felt campaign hats.\"\n",
    "\n",
    "        \\\"\\\"\\\"{db}\\\"\\\"\\\"\n",
    "\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: The prompt size exceeds the context window size and cannot be processed.\n"
     ]
    }
   ],
   "source": [
    "simplification = res.get('result', None) if isinstance(res.get('result', None), str) else None\n",
    "print(simplification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroCraft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
