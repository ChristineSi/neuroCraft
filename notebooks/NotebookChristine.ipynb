{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Sentence Simplification using BERT-to-GPT2 \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## neuroCraft Project\n",
    "***\n",
    "<br>\n",
    "\n",
    "Christine Sigrist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "import click\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from data import WikiDataset\n",
    "\n",
    "from transformers import BertTokenizer, GPT2Tokenizer\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from transformers import EncoderDecoderModel, BertConfig, EncoderDecoderConfig, GPT2Tokenizer, BertModel, GPT2Model\n",
    "\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "import logging\n",
    "import gc\n",
    "import shutil\n",
    "import sari\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data\n",
      "(138413, 1)\n",
      "(138413, 1)\n",
      "Source: Quincy in 1767 was the `` north precinct '' of Braintree , Massachusetts .\n",
      "Target: He was born in Braintree , Massachusetts , in 1767 .\n",
      " \n",
      "validation data\n",
      "(2000, 1)\n",
      "(2000, 1)\n",
      "Source: Since 1980 the senior pastor has been John Piper .\n",
      "Target: Since 1980 the main pastor has been John Piper .\n",
      " \n",
      "test data\n",
      "(359, 1)\n",
      "(359, 1)\n",
      "Source: Alessandro ( \" Sandro \" ) Mazzola ( born 8 November 1942 ) is an Italian former football player .\n",
      "Target: Alessandro Mazzola is an Italian former football player.\n"
     ]
    }
   ],
   "source": [
    "# DATA LOADING\n",
    "\"\"\"\n",
    "DATA INFO\n",
    "\n",
    "Training:\n",
    "Wiki dataset comprising of parallel corpus of normal sentences and simple sentences is used to train the model.\n",
    "The original dataset consists of around 167k English sentence pairs from the Wikipedia articles.\n",
    "The dataset comprises of mapping of one-to-many, one-to-one and many-to-one sentence pairs.\n",
    "But the dataset was not suitable for the training without preprocessing.\n",
    "Upon tokenizing the sentences, sentences having token length of more than 80 were removed keeping the maximum token length of sentences to 80.\n",
    "The resulting training dataset became 138k from 167k.\n",
    "\n",
    "Testing:\n",
    "For the evaluation and testing purpose, TurkCorpus is used.\n",
    "The dataset consists of 2k manually prepared sentence pairs with 8 reference sentences and 300 sentences for testing purpose which also has 8 reference sentences.\n",
    "\"\"\"\n",
    "\n",
    "# file opener\n",
    "def open_file(file_path, ref=False):\n",
    "    data = []\n",
    "    if ref:\n",
    "        ref_data = pickle.load(open(file_path, 'rb'))\n",
    "        return ref_data\n",
    "    else:\n",
    "        with open(file_path, 'r', encoding=\"utf8\") as f:\n",
    "            sents = f.readlines()\n",
    "            for s in sents:\n",
    "                data.append(s.strip())\n",
    "        return data\n",
    "\n",
    "# data loader\n",
    "def load_dataset(src_path, tgt_path=None, ref_path=None, ref=False):\n",
    "    src = open_file(src_path)\n",
    "    tgt = None\n",
    "    ref = None\n",
    "    if tgt_path is not None:\n",
    "        tgt = open_file(tgt_path)\n",
    "    if ref_path is not None:\n",
    "        ref = open_file(ref_path, ref)\n",
    "    return src, tgt, ref\n",
    "\n",
    "# training data\n",
    "src_train_file = 'dataset/src_train.txt'\n",
    "tgt_train_file = 'dataset/tgt_train.txt'\n",
    "\n",
    "# original train data and target train data (no ref data)\n",
    "src_train, tgt_train, ref_train = load_dataset(src_train_file, tgt_train_file, ref=False)\n",
    "\n",
    "print('training data')\n",
    "print(pd.DataFrame(src_train).shape)\n",
    "print(pd.DataFrame(tgt_train).shape)\n",
    "\n",
    "sample_index = 1000\n",
    "src_sample = src_train[sample_index]\n",
    "tgt_sample = tgt_train[sample_index]\n",
    "\n",
    "print(\"Source:\", src_sample)\n",
    "print(\"Target:\", tgt_sample)\n",
    "\n",
    "# validation data\n",
    "src_valid_file = 'dataset/src_valid.txt'\n",
    "tgt_valid_file = 'dataset/tgt_valid.txt'\n",
    "\n",
    "# original train data and target train data (no ref data)\n",
    "src_valid, tgt_valid, ref_valid = load_dataset(src_valid_file, tgt_valid_file, ref=False)\n",
    "\n",
    "print(' ')\n",
    "print('validation data')\n",
    "print(pd.DataFrame(src_valid).shape)\n",
    "print(pd.DataFrame(tgt_valid).shape)\n",
    "\n",
    "sample_index = 99\n",
    "src_sample = src_valid[sample_index]\n",
    "tgt_sample = tgt_valid[sample_index]\n",
    "\n",
    "print(\"Source:\", src_sample)\n",
    "print(\"Target:\", tgt_sample)\n",
    "\n",
    "# testing data\n",
    "src_test_file = 'dataset/src_test.txt'\n",
    "tgt_test_file = 'dataset/tgt_test.txt'\n",
    "\n",
    "# original train data and target train data (no ref data)\n",
    "src_test, tgt_test, ref_test = load_dataset(src_test_file, tgt_test_file, ref=False)\n",
    "\n",
    "print(' ')\n",
    "print('test data')\n",
    "print(pd.DataFrame(src_test).shape)\n",
    "print(pd.DataFrame(tgt_test).shape)\n",
    "\n",
    "sample_index = 10\n",
    "src_sample = src_test[sample_index]\n",
    "tgt_sample = tgt_test[sample_index]\n",
    "\n",
    "print(\"Source:\", src_sample)\n",
    "print(\"Target:\", tgt_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PROCESSING\n",
    "# function for encoding (tokenizing) the data\n",
    "def encode_batch(batch, max_len=80):\n",
    "    '''\n",
    "    Inputs:\n",
    "    batch: This function expects a list containing two elements:\n",
    "    The first element is a string representing the source text.\n",
    "    The second element is a string representing the target text.\n",
    "\n",
    "    Tokenization:\n",
    "    src_tokens: The source text (first element of batch) is tokenized using the BERT tokenizer (bert_tokenizer).\n",
    "    It is processed to generate tokens (input_ids) and an attention mask for the source text.\n",
    "    BERT tokenization includes adding special tokens, padding to a maximum length, and truncating if needed.\n",
    "    tgdt_tokens: The target text (second element of batch) is tokenized using the GPT-2 tokenizer (gpt2_tokenizer). Similar to the source, tokens (input_ids) and an attention mask for the target text are generated.\n",
    "\n",
    "    Creating Labels:\n",
    "    labels: This is created from the tgt_tokens.input_ids tensor. It's used for calculating loss during training.\n",
    "    The tgt_tokens.attention_mask is used to identify where the padding is in the target tokens and sets those positions in the labels tensor to -100.\n",
    "\n",
    "    Output:\n",
    "    The function returns five values:\n",
    "    src_tokens.input_ids: Tensor containing tokenized representation of the source text.\n",
    "    src_tokens.attention_mask: Tensor containing attention mask for the source text.\n",
    "    tgt_tokens.input_ids: Tensor containing tokenized representation of the target text.\n",
    "    tgt_tokens.attention_mask: Tensor containing attention mask for the target text.\n",
    "\n",
    "    Labels:\n",
    "    Tensor containing the labels for the target text, modified with -100 in places corresponding to padding.\n",
    "    '''\n",
    "    src_tokens = bert_tokenizer(batch[0], max_length=max_len, add_special_tokens=True,\n",
    "                                return_token_type_ids=False, padding=\"max_length\", truncation=True,\n",
    "                                return_attention_mask=True, return_tensors=\"pt\")\n",
    "\n",
    "    tgt_tokens = gpt2_tokenizer(batch[1], max_length=max_len, add_special_tokens=True,\n",
    "                                return_token_type_ids=False, padding=\"max_length\", truncation=True,\n",
    "                                return_attention_mask=True, return_tensors=\"pt\")\n",
    "\n",
    "    labels = tgt_tokens.input_ids.clone()\n",
    "    labels[tgt_tokens.attention_mask == 0] = -100\n",
    "\n",
    "    return src_tokens.input_ids, src_tokens.attention_mask, tgt_tokens.input_ids, tgt_tokens.attention_mask, labels\n",
    "\n",
    "# function for decoding the tokenized sentences to human readable text\n",
    "def decode_sent_tokens(data):\n",
    "    '''\n",
    "    This function takes a list of tokenized sentences (data) and decodes them back into human-readable sentences.\n",
    "\n",
    "    Inputs:\n",
    "    data: A list containing tokenized sentences.\n",
    "\n",
    "    Functionality:\n",
    "    It iterates through each tokenized sentence in the data list.\n",
    "    For each tokenized sentence, it uses the GPT-2 tokenizer (gpt2_tokenizer) to decode the tokens into a human-readable sentence (s), skipping special tokens and cleaning up tokenization spaces.\n",
    "    The decoded sentences (s) are added to a list (sents_list).\n",
    "\n",
    "    Output:\n",
    "    The function returns a list (sents_list) containing the decoded sentences\n",
    "    '''\n",
    "    sents_list = []\n",
    "    for sent in data:\n",
    "        s = gpt2_tokenizer.decode(sent, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        sents_list.append(s)\n",
    "\n",
    "    return sents_list\n",
    "\n",
    "# function\n",
    "def get_sent_tokens(sents):\n",
    "    '''\n",
    "    This function tokenizes input sentences and prepares them for downstream processing, perhaps for model input or further manipulation.\n",
    "\n",
    "    Inputs:\n",
    "    sents: A list or string containing sentences.\n",
    "\n",
    "    Functionality:\n",
    "    It tokenizes the input sentences using the GPT-2 tokenizer (gpt2_tokenizer) with specific settings:\n",
    "    Adding special tokens.\n",
    "    Not returning token type IDs.\n",
    "    Truncating sequences if needed.\n",
    "    Padding to the longest sequence in the batch.\n",
    "    Not returning attention masks (only input tensors are returned).\n",
    "\n",
    "    Output:\n",
    "    The function returns a list (ref) containing lists of tokenized sentences, where each inner list represents the tokenized form of a sentence from the input.\n",
    "    '''\n",
    "    ref = []\n",
    "    tokens = gpt2_tokenizer(sents, add_special_tokens=True,\n",
    "                            return_token_type_ids=False, truncation=True, padding=\"longest\",\n",
    "                            return_attention_mask=False, return_tensors=\"pt\")\n",
    "\n",
    "    for tok in tokens.input_ids.tolist():\n",
    "        ref.append([tok])\n",
    "\n",
    "    return ref\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "# function to evaluate model\n",
    "def evaluate_model(model, data_loader, criterion, target_tokenizer):\n",
    "    '''\n",
    "    Inputs:\n",
    "    model: The trained seq2seq model to evaluate.\n",
    "    data_loader: The data loader providing batches of evaluation data. It should contain pairs of source and target sequences.\n",
    "\n",
    "    criterion: The loss function used for training the model, typically nn.CrossEntropyLoss() or similar.\n",
    "    target_tokenizer: The tokenizer used for tokenizing the target sequences.\n",
    "\n",
    "    Functionality:\n",
    "    Sets the model to evaluation mode (model.eval()).\n",
    "    Iterates through the evaluation data in batches.\n",
    "    Passes the source sequences through the model to generate predictions.\n",
    "    Computes the loss between predicted sequences and actual target sequences.\n",
    "    Converts model predictions and target sequences from token IDs to text.\n",
    "    Stores references (actual target sentences) and hypotheses (predicted sentences) to calculate BLEU score.\n",
    "    Calculates the average loss and BLEU score over the evaluation dataset.\n",
    "\n",
    "    Outputs:\n",
    "    avg_loss: Average loss over the evaluation dataset.\n",
    "    bleu_score: BLEU score indicating the quality of the model's translations compared to the ground truth.\n",
    "    '''\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch_source = batch[0]\n",
    "            batch_target = batch[1]\n",
    "\n",
    "            outputs = model(input_ids=batch_source, decoder_input_ids=batch_target)\n",
    "\n",
    "            logits_flat = outputs.logits.view(-1, outputs.logits.size(-1))\n",
    "            target_flat = batch_target.view(-1)\n",
    "\n",
    "            loss = criterion(logits_flat, target_flat)\n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "\n",
    "            predicted_ids = outputs.logits.argmax(-1)\n",
    "            predicted_sentences = [target_tokenizer.decode(ids, skip_special_tokens=True) for ids in predicted_ids]\n",
    "            target_sentences = [target_tokenizer.decode(ids, skip_special_tokens=True) for ids in batch_target]\n",
    "\n",
    "            references.extend([sent.split() for sent in target_sentences])\n",
    "            hypotheses.extend([sent.split() for sent in predicted_sentences])\n",
    "\n",
    "    avg_loss = total_loss / total_batches\n",
    "\n",
    "    # Calculate BLEU score with smoothing\n",
    "    smooth_func = SmoothingFunction().method4\n",
    "    bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smooth_func)\n",
    "\n",
    "    return avg_loss, bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare validation dataset\n",
    "\n",
    "# Set up tokenizers\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.unk_token\n",
    "\n",
    "# function to load evaluation data\n",
    "def load_eval_data(src_path, tgt_path):\n",
    "    src_data = open_file(src_path)\n",
    "    tgt_data = open_file(tgt_path)\n",
    "    return src_data, tgt_data\n",
    "\n",
    "# evaluation data paths\n",
    "src_eval_file = 'dataset/src_valid.txt'\n",
    "tgt_eval_file = 'dataset/tgt_valid.txt'\n",
    "\n",
    "# load evaluation data\n",
    "src_eval, tgt_eval = load_eval_data(src_eval_file, tgt_eval_file)\n",
    "\n",
    "# prepare evaluation data tensors using encode_batch function\n",
    "source_eval_data_encoded = encode_batch((src_eval))\n",
    "target_eval_data_encoded = encode_batch((tgt_eval))\n",
    "\n",
    "# create evaluation data loader\n",
    "eval_dataset = TensorDataset(*source_eval_data_encoded, *target_eval_data_encoded)\n",
    "eval_data_loader = DataLoader(eval_dataset, batch_size=3, shuffle=False)\n",
    "\n",
    "# ref sentences\n",
    "# Load data from the pickle file\n",
    "with open('dataset/ref_valid.pkl', 'rb') as file:\n",
    "    ref_sentences = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.3.crossattention.c_attn.weight', 'h.5.ln_cross_attn.weight', 'h.6.ln_cross_attn.weight', 'h.9.crossattention.q_attn.bias', 'h.9.ln_cross_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.6.crossattention.q_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.3.ln_cross_attn.bias', 'h.7.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.2.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.bias', 'h.11.crossattention.q_attn.bias', 'h.4.ln_cross_attn.weight', 'h.0.ln_cross_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.7.crossattention.q_attn.bias', 'h.5.crossattention.c_proj.bias', 'h.1.ln_cross_attn.weight', 'h.4.crossattention.q_attn.bias', 'h.1.crossattention.c_proj.weight', 'h.10.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.4.ln_cross_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.bias', 'h.11.crossattention.c_proj.weight', 'h.6.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.4.crossattention.c_proj.weight', 'h.5.crossattention.c_attn.bias', 'h.7.crossattention.c_proj.bias', 'h.10.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.8.ln_cross_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.5.ln_cross_attn.bias', 'h.9.ln_cross_attn.bias', 'h.1.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.2.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.weight', 'h.0.crossattention.c_proj.weight', 'h.8.crossattention.c_attn.bias', 'h.2.ln_cross_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.11.ln_cross_attn.bias', 'h.1.crossattention.c_attn.bias', 'h.8.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.0.ln_cross_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.1.crossattention.q_attn.weight', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.weight', 'h.10.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.1.crossattention.q_attn.bias', 'h.7.crossattention.c_proj.weight', 'h.10.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.9.crossattention.c_proj.bias', 'h.7.ln_cross_attn.bias', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.6.crossattention.c_proj.weight', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.11.ln_cross_attn.weight', 'h.8.crossattention.q_attn.weight', 'h.5.crossattention.q_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.8.crossattention.q_attn.bias', 'h.8.ln_cross_attn.bias', 'h.8.crossattention.c_proj.bias', 'h.6.crossattention.q_attn.bias', 'h.10.ln_cross_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.11.crossattention.c_attn.weight', 'h.8.crossattention.c_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Evaluation Loss: 1.3135, Evaluation BLEU: 0.0000\n",
      "Epoch [2/2000], Evaluation Loss: 4.9308, Evaluation BLEU: 0.0000\n",
      "Epoch [3/2000], Evaluation Loss: 8.6167, Evaluation BLEU: 0.0000\n",
      "Epoch [4/2000], Evaluation Loss: 8.1539, Evaluation BLEU: 0.0000\n",
      "Epoch [5/2000], Evaluation Loss: 9.7744, Evaluation BLEU: 0.0000\n",
      "Epoch [6/2000], Evaluation Loss: 5.3611, Evaluation BLEU: 0.0000\n",
      "Epoch [7/2000], Evaluation Loss: 5.2928, Evaluation BLEU: 0.0000\n",
      "Epoch [8/2000], Evaluation Loss: 13.3926, Evaluation BLEU: 0.0000\n",
      "Epoch [9/2000], Evaluation Loss: 6.7120, Evaluation BLEU: 0.0000\n",
      "Epoch [10/2000], Evaluation Loss: 10.0990, Evaluation BLEU: 0.0000\n",
      "Epoch [11/2000], Evaluation Loss: 9.5260, Evaluation BLEU: 0.0000\n",
      "Epoch [12/2000], Evaluation Loss: 8.7591, Evaluation BLEU: 0.0000\n",
      "Epoch [13/2000], Evaluation Loss: 8.6172, Evaluation BLEU: 0.0000\n",
      "Epoch [14/2000], Evaluation Loss: 9.0559, Evaluation BLEU: 0.0000\n",
      "Epoch [15/2000], Evaluation Loss: 9.2633, Evaluation BLEU: 0.0000\n",
      "Epoch [16/2000], Evaluation Loss: 9.4647, Evaluation BLEU: 0.0000\n",
      "Epoch [17/2000], Evaluation Loss: 9.6736, Evaluation BLEU: 0.0000\n",
      "Epoch [18/2000], Evaluation Loss: 9.5711, Evaluation BLEU: 0.0000\n",
      "Epoch [19/2000], Evaluation Loss: 9.6535, Evaluation BLEU: 0.0000\n",
      "Epoch [20/2000], Evaluation Loss: 9.5636, Evaluation BLEU: 0.0000\n",
      "Epoch [21/2000], Evaluation Loss: 9.5798, Evaluation BLEU: 0.0005\n",
      "Epoch [22/2000], Evaluation Loss: 9.5961, Evaluation BLEU: 0.0000\n",
      "Epoch [23/2000], Evaluation Loss: 9.5012, Evaluation BLEU: 0.0000\n",
      "Epoch [24/2000], Evaluation Loss: 9.2994, Evaluation BLEU: 0.0000\n",
      "Epoch [25/2000], Evaluation Loss: 9.1307, Evaluation BLEU: 0.0000\n",
      "Epoch [26/2000], Evaluation Loss: 9.1679, Evaluation BLEU: 0.0000\n",
      "Epoch [27/2000], Evaluation Loss: 9.3277, Evaluation BLEU: 0.0000\n",
      "Epoch [28/2000], Evaluation Loss: 9.1922, Evaluation BLEU: 0.0000\n",
      "Epoch [29/2000], Evaluation Loss: 9.0526, Evaluation BLEU: 0.0000\n",
      "Epoch [30/2000], Evaluation Loss: 9.1751, Evaluation BLEU: 0.0000\n",
      "Epoch [31/2000], Evaluation Loss: 10.1909, Evaluation BLEU: 0.0000\n",
      "Epoch [32/2000], Evaluation Loss: 11.7618, Evaluation BLEU: 0.0000\n",
      "Epoch [33/2000], Evaluation Loss: 13.9300, Evaluation BLEU: 0.0000\n",
      "Epoch [34/2000], Evaluation Loss: 11.4601, Evaluation BLEU: 0.0000\n",
      "Epoch [35/2000], Evaluation Loss: 10.7437, Evaluation BLEU: 0.0000\n",
      "Epoch [36/2000], Evaluation Loss: 9.7769, Evaluation BLEU: 0.0000\n",
      "Epoch [37/2000], Evaluation Loss: 9.1975, Evaluation BLEU: 0.0000\n",
      "Epoch [38/2000], Evaluation Loss: 8.9801, Evaluation BLEU: 0.0000\n",
      "Epoch [39/2000], Evaluation Loss: 8.9117, Evaluation BLEU: 0.0000\n",
      "Epoch [40/2000], Evaluation Loss: 8.8913, Evaluation BLEU: 0.0000\n",
      "Epoch [41/2000], Evaluation Loss: 8.9691, Evaluation BLEU: 0.0000\n",
      "Epoch [42/2000], Evaluation Loss: 9.1677, Evaluation BLEU: 0.0000\n",
      "Epoch [43/2000], Evaluation Loss: 9.6563, Evaluation BLEU: 0.0000\n",
      "Epoch [44/2000], Evaluation Loss: 10.8880, Evaluation BLEU: 0.0000\n",
      "Epoch [45/2000], Evaluation Loss: 13.1605, Evaluation BLEU: 0.0000\n",
      "Epoch [46/2000], Evaluation Loss: 15.2916, Evaluation BLEU: 0.0000\n",
      "Epoch [47/2000], Evaluation Loss: 15.8944, Evaluation BLEU: 0.0000\n",
      "Epoch [48/2000], Evaluation Loss: 16.2490, Evaluation BLEU: 0.0000\n",
      "Epoch [49/2000], Evaluation Loss: 17.5691, Evaluation BLEU: 0.0000\n",
      "Epoch [50/2000], Evaluation Loss: 18.2192, Evaluation BLEU: 0.0000\n",
      "Epoch [51/2000], Evaluation Loss: 17.8092, Evaluation BLEU: 0.0000\n",
      "Epoch [52/2000], Evaluation Loss: 18.0772, Evaluation BLEU: 0.0000\n",
      "Epoch [53/2000], Evaluation Loss: 17.6582, Evaluation BLEU: 0.0000\n",
      "Epoch [54/2000], Evaluation Loss: 16.9985, Evaluation BLEU: 0.0000\n",
      "Epoch [55/2000], Evaluation Loss: 17.0762, Evaluation BLEU: 0.0000\n",
      "Epoch [56/2000], Evaluation Loss: 17.7842, Evaluation BLEU: 0.0000\n",
      "Epoch [57/2000], Evaluation Loss: 18.5845, Evaluation BLEU: 0.0000\n",
      "Epoch [58/2000], Evaluation Loss: 19.1365, Evaluation BLEU: 0.0000\n",
      "Epoch [59/2000], Evaluation Loss: 19.3647, Evaluation BLEU: 0.0000\n",
      "Epoch [60/2000], Evaluation Loss: 19.3557, Evaluation BLEU: 0.0000\n",
      "Epoch [61/2000], Evaluation Loss: 19.5297, Evaluation BLEU: 0.0000\n",
      "Epoch [62/2000], Evaluation Loss: 20.5219, Evaluation BLEU: 0.0000\n",
      "Epoch [63/2000], Evaluation Loss: 22.0741, Evaluation BLEU: 0.0000\n",
      "Epoch [64/2000], Evaluation Loss: 23.3577, Evaluation BLEU: 0.0000\n",
      "Epoch [65/2000], Evaluation Loss: 23.7725, Evaluation BLEU: 0.0000\n",
      "Epoch [66/2000], Evaluation Loss: 23.4594, Evaluation BLEU: 0.0000\n",
      "Epoch [67/2000], Evaluation Loss: 22.8957, Evaluation BLEU: 0.0000\n",
      "Epoch [68/2000], Evaluation Loss: 22.4208, Evaluation BLEU: 0.0000\n",
      "Epoch [69/2000], Evaluation Loss: 22.1536, Evaluation BLEU: 0.0000\n",
      "Epoch [70/2000], Evaluation Loss: 22.0773, Evaluation BLEU: 0.0000\n",
      "Epoch [71/2000], Evaluation Loss: 22.1058, Evaluation BLEU: 0.0000\n",
      "Epoch [72/2000], Evaluation Loss: 22.1661, Evaluation BLEU: 0.0000\n",
      "Epoch [73/2000], Evaluation Loss: 22.2196, Evaluation BLEU: 0.0000\n",
      "Epoch [74/2000], Evaluation Loss: 22.2541, Evaluation BLEU: 0.0000\n",
      "Epoch [75/2000], Evaluation Loss: 22.2701, Evaluation BLEU: 0.0000\n",
      "Epoch [76/2000], Evaluation Loss: 22.2724, Evaluation BLEU: 0.0000\n",
      "Epoch [77/2000], Evaluation Loss: 22.2665, Evaluation BLEU: 0.0000\n",
      "Epoch [78/2000], Evaluation Loss: 22.2570, Evaluation BLEU: 0.0000\n",
      "Epoch [79/2000], Evaluation Loss: 22.2470, Evaluation BLEU: 0.0000\n",
      "Epoch [80/2000], Evaluation Loss: 22.2381, Evaluation BLEU: 0.0000\n",
      "Epoch [81/2000], Evaluation Loss: 22.2309, Evaluation BLEU: 0.0000\n",
      "Epoch [82/2000], Evaluation Loss: 22.2250, Evaluation BLEU: 0.0000\n",
      "Epoch [83/2000], Evaluation Loss: 22.2200, Evaluation BLEU: 0.0000\n",
      "Epoch [84/2000], Evaluation Loss: 22.2157, Evaluation BLEU: 0.0000\n",
      "Epoch [85/2000], Evaluation Loss: 22.2120, Evaluation BLEU: 0.0000\n",
      "Epoch [86/2000], Evaluation Loss: 22.2093, Evaluation BLEU: 0.0000\n",
      "Epoch [87/2000], Evaluation Loss: 22.2085, Evaluation BLEU: 0.0000\n",
      "Epoch [88/2000], Evaluation Loss: 22.2102, Evaluation BLEU: 0.0000\n",
      "Epoch [89/2000], Evaluation Loss: 22.2152, Evaluation BLEU: 0.0000\n",
      "Epoch [90/2000], Evaluation Loss: 22.2239, Evaluation BLEU: 0.0000\n",
      "Epoch [91/2000], Evaluation Loss: 22.2365, Evaluation BLEU: 0.0000\n",
      "Epoch [92/2000], Evaluation Loss: 22.2529, Evaluation BLEU: 0.0000\n",
      "Epoch [93/2000], Evaluation Loss: 22.2724, Evaluation BLEU: 0.0000\n",
      "Epoch [94/2000], Evaluation Loss: 22.2943, Evaluation BLEU: 0.0000\n",
      "Epoch [95/2000], Evaluation Loss: 22.3179, Evaluation BLEU: 0.0000\n",
      "Epoch [96/2000], Evaluation Loss: 22.3421, Evaluation BLEU: 0.0000\n",
      "Epoch [97/2000], Evaluation Loss: 22.3662, Evaluation BLEU: 0.0000\n",
      "Epoch [98/2000], Evaluation Loss: 22.3894, Evaluation BLEU: 0.0000\n",
      "Epoch [99/2000], Evaluation Loss: 22.4112, Evaluation BLEU: 0.0000\n",
      "Epoch [100/2000], Evaluation Loss: 22.4311, Evaluation BLEU: 0.0000\n",
      "Epoch [101/2000], Evaluation Loss: 22.4490, Evaluation BLEU: 0.0000\n",
      "Epoch [102/2000], Evaluation Loss: 22.4648, Evaluation BLEU: 0.0000\n",
      "Epoch [103/2000], Evaluation Loss: 22.4787, Evaluation BLEU: 0.0000\n",
      "Epoch [104/2000], Evaluation Loss: 22.4907, Evaluation BLEU: 0.0000\n",
      "Epoch [105/2000], Evaluation Loss: 22.5011, Evaluation BLEU: 0.0000\n",
      "Epoch [106/2000], Evaluation Loss: 22.5101, Evaluation BLEU: 0.0000\n",
      "Epoch [107/2000], Evaluation Loss: 22.5181, Evaluation BLEU: 0.0000\n",
      "Epoch [108/2000], Evaluation Loss: 22.5252, Evaluation BLEU: 0.0000\n",
      "Epoch [109/2000], Evaluation Loss: 22.5317, Evaluation BLEU: 0.0000\n",
      "Epoch [110/2000], Evaluation Loss: 22.5378, Evaluation BLEU: 0.0000\n",
      "Epoch [111/2000], Evaluation Loss: 22.5435, Evaluation BLEU: 0.0000\n",
      "Epoch [112/2000], Evaluation Loss: 22.5491, Evaluation BLEU: 0.0000\n",
      "Epoch [113/2000], Evaluation Loss: 22.5545, Evaluation BLEU: 0.0000\n",
      "Epoch [114/2000], Evaluation Loss: 22.5599, Evaluation BLEU: 0.0000\n",
      "Epoch [115/2000], Evaluation Loss: 22.5653, Evaluation BLEU: 0.0000\n",
      "Epoch [116/2000], Evaluation Loss: 22.5707, Evaluation BLEU: 0.0000\n",
      "Epoch [117/2000], Evaluation Loss: 22.5761, Evaluation BLEU: 0.0000\n",
      "Epoch [118/2000], Evaluation Loss: 22.5815, Evaluation BLEU: 0.0000\n",
      "Epoch [119/2000], Evaluation Loss: 22.5869, Evaluation BLEU: 0.0000\n",
      "Epoch [120/2000], Evaluation Loss: 22.5923, Evaluation BLEU: 0.0000\n",
      "Epoch [121/2000], Evaluation Loss: 22.5975, Evaluation BLEU: 0.0000\n",
      "Epoch [122/2000], Evaluation Loss: 22.6027, Evaluation BLEU: 0.0000\n",
      "Epoch [123/2000], Evaluation Loss: 22.6079, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [124/2000], Evaluation Loss: 22.6129, Evaluation BLEU: 0.0000\n",
      "Epoch [125/2000], Evaluation Loss: 22.6177, Evaluation BLEU: 0.0000\n",
      "Epoch [126/2000], Evaluation Loss: 22.6225, Evaluation BLEU: 0.0000\n",
      "Epoch [127/2000], Evaluation Loss: 22.6271, Evaluation BLEU: 0.0000\n",
      "Epoch [128/2000], Evaluation Loss: 22.6316, Evaluation BLEU: 0.0000\n",
      "Epoch [129/2000], Evaluation Loss: 22.6359, Evaluation BLEU: 0.0000\n",
      "Epoch [130/2000], Evaluation Loss: 22.6401, Evaluation BLEU: 0.0000\n",
      "Epoch [131/2000], Evaluation Loss: 22.6442, Evaluation BLEU: 0.0000\n",
      "Epoch [132/2000], Evaluation Loss: 22.6482, Evaluation BLEU: 0.0000\n",
      "Epoch [133/2000], Evaluation Loss: 22.6521, Evaluation BLEU: 0.0000\n",
      "Epoch [134/2000], Evaluation Loss: 22.6558, Evaluation BLEU: 0.0000\n",
      "Epoch [135/2000], Evaluation Loss: 22.6595, Evaluation BLEU: 0.0000\n",
      "Epoch [136/2000], Evaluation Loss: 22.6632, Evaluation BLEU: 0.0000\n",
      "Epoch [137/2000], Evaluation Loss: 22.6668, Evaluation BLEU: 0.0000\n",
      "Epoch [138/2000], Evaluation Loss: 22.6703, Evaluation BLEU: 0.0000\n",
      "Epoch [139/2000], Evaluation Loss: 22.6737, Evaluation BLEU: 0.0000\n",
      "Epoch [140/2000], Evaluation Loss: 22.6772, Evaluation BLEU: 0.0000\n",
      "Epoch [141/2000], Evaluation Loss: 22.6806, Evaluation BLEU: 0.0000\n",
      "Epoch [142/2000], Evaluation Loss: 22.6839, Evaluation BLEU: 0.0000\n",
      "Epoch [143/2000], Evaluation Loss: 22.6873, Evaluation BLEU: 0.0000\n",
      "Epoch [144/2000], Evaluation Loss: 22.6906, Evaluation BLEU: 0.0000\n",
      "Epoch [145/2000], Evaluation Loss: 22.6939, Evaluation BLEU: 0.0000\n",
      "Epoch [146/2000], Evaluation Loss: 22.6972, Evaluation BLEU: 0.0000\n",
      "Epoch [147/2000], Evaluation Loss: 22.7005, Evaluation BLEU: 0.0000\n",
      "Epoch [148/2000], Evaluation Loss: 22.7037, Evaluation BLEU: 0.0000\n",
      "Epoch [149/2000], Evaluation Loss: 22.7070, Evaluation BLEU: 0.0000\n",
      "Epoch [150/2000], Evaluation Loss: 22.7102, Evaluation BLEU: 0.0000\n",
      "Epoch [151/2000], Evaluation Loss: 22.7134, Evaluation BLEU: 0.0000\n",
      "Epoch [152/2000], Evaluation Loss: 22.7166, Evaluation BLEU: 0.0000\n",
      "Epoch [153/2000], Evaluation Loss: 22.7198, Evaluation BLEU: 0.0000\n",
      "Epoch [154/2000], Evaluation Loss: 22.7229, Evaluation BLEU: 0.0000\n",
      "Epoch [155/2000], Evaluation Loss: 22.7260, Evaluation BLEU: 0.0000\n",
      "Epoch [156/2000], Evaluation Loss: 22.7291, Evaluation BLEU: 0.0000\n",
      "Epoch [157/2000], Evaluation Loss: 22.7323, Evaluation BLEU: 0.0000\n",
      "Epoch [158/2000], Evaluation Loss: 22.7353, Evaluation BLEU: 0.0000\n",
      "Epoch [159/2000], Evaluation Loss: 22.7384, Evaluation BLEU: 0.0000\n",
      "Epoch [160/2000], Evaluation Loss: 22.7414, Evaluation BLEU: 0.0000\n",
      "Epoch [161/2000], Evaluation Loss: 22.7444, Evaluation BLEU: 0.0000\n",
      "Epoch [162/2000], Evaluation Loss: 22.7474, Evaluation BLEU: 0.0000\n",
      "Epoch [163/2000], Evaluation Loss: 22.7503, Evaluation BLEU: 0.0000\n",
      "Epoch [164/2000], Evaluation Loss: 22.7532, Evaluation BLEU: 0.0000\n",
      "Epoch [165/2000], Evaluation Loss: 22.7561, Evaluation BLEU: 0.0000\n",
      "Epoch [166/2000], Evaluation Loss: 22.7590, Evaluation BLEU: 0.0000\n",
      "Epoch [167/2000], Evaluation Loss: 22.7618, Evaluation BLEU: 0.0000\n",
      "Epoch [168/2000], Evaluation Loss: 22.7646, Evaluation BLEU: 0.0000\n",
      "Epoch [169/2000], Evaluation Loss: 22.7674, Evaluation BLEU: 0.0000\n",
      "Epoch [170/2000], Evaluation Loss: 22.7701, Evaluation BLEU: 0.0000\n",
      "Epoch [171/2000], Evaluation Loss: 22.7729, Evaluation BLEU: 0.0000\n",
      "Epoch [172/2000], Evaluation Loss: 22.7756, Evaluation BLEU: 0.0000\n",
      "Epoch [173/2000], Evaluation Loss: 22.7783, Evaluation BLEU: 0.0000\n",
      "Epoch [174/2000], Evaluation Loss: 22.7809, Evaluation BLEU: 0.0000\n",
      "Epoch [175/2000], Evaluation Loss: 22.7835, Evaluation BLEU: 0.0000\n",
      "Epoch [176/2000], Evaluation Loss: 22.7861, Evaluation BLEU: 0.0000\n",
      "Epoch [177/2000], Evaluation Loss: 22.7887, Evaluation BLEU: 0.0000\n",
      "Epoch [178/2000], Evaluation Loss: 22.7912, Evaluation BLEU: 0.0000\n",
      "Epoch [179/2000], Evaluation Loss: 22.7938, Evaluation BLEU: 0.0000\n",
      "Epoch [180/2000], Evaluation Loss: 22.7962, Evaluation BLEU: 0.0000\n",
      "Epoch [181/2000], Evaluation Loss: 22.7987, Evaluation BLEU: 0.0000\n",
      "Epoch [182/2000], Evaluation Loss: 22.8012, Evaluation BLEU: 0.0000\n",
      "Epoch [183/2000], Evaluation Loss: 22.8036, Evaluation BLEU: 0.0000\n",
      "Epoch [184/2000], Evaluation Loss: 22.8060, Evaluation BLEU: 0.0000\n",
      "Epoch [185/2000], Evaluation Loss: 22.8084, Evaluation BLEU: 0.0000\n",
      "Epoch [186/2000], Evaluation Loss: 22.8108, Evaluation BLEU: 0.0000\n",
      "Epoch [187/2000], Evaluation Loss: 22.8131, Evaluation BLEU: 0.0000\n",
      "Epoch [188/2000], Evaluation Loss: 22.8154, Evaluation BLEU: 0.0000\n",
      "Epoch [189/2000], Evaluation Loss: 22.8177, Evaluation BLEU: 0.0000\n",
      "Epoch [190/2000], Evaluation Loss: 22.8200, Evaluation BLEU: 0.0000\n",
      "Epoch [191/2000], Evaluation Loss: 22.8222, Evaluation BLEU: 0.0000\n",
      "Epoch [192/2000], Evaluation Loss: 22.8245, Evaluation BLEU: 0.0000\n",
      "Epoch [193/2000], Evaluation Loss: 22.8267, Evaluation BLEU: 0.0000\n",
      "Epoch [194/2000], Evaluation Loss: 22.8289, Evaluation BLEU: 0.0000\n",
      "Epoch [195/2000], Evaluation Loss: 22.8311, Evaluation BLEU: 0.0000\n",
      "Epoch [196/2000], Evaluation Loss: 22.8333, Evaluation BLEU: 0.0000\n",
      "Epoch [197/2000], Evaluation Loss: 22.8354, Evaluation BLEU: 0.0000\n",
      "Epoch [198/2000], Evaluation Loss: 22.8375, Evaluation BLEU: 0.0000\n",
      "Epoch [199/2000], Evaluation Loss: 22.8397, Evaluation BLEU: 0.0000\n",
      "Epoch [200/2000], Evaluation Loss: 22.8418, Evaluation BLEU: 0.0000\n",
      "Epoch [201/2000], Evaluation Loss: 22.8439, Evaluation BLEU: 0.0000\n",
      "Epoch [202/2000], Evaluation Loss: 22.8460, Evaluation BLEU: 0.0000\n",
      "Epoch [203/2000], Evaluation Loss: 22.8480, Evaluation BLEU: 0.0000\n",
      "Epoch [204/2000], Evaluation Loss: 22.8501, Evaluation BLEU: 0.0000\n",
      "Epoch [205/2000], Evaluation Loss: 22.8521, Evaluation BLEU: 0.0000\n",
      "Epoch [206/2000], Evaluation Loss: 22.8541, Evaluation BLEU: 0.0000\n",
      "Epoch [207/2000], Evaluation Loss: 22.8561, Evaluation BLEU: 0.0000\n",
      "Epoch [208/2000], Evaluation Loss: 22.8581, Evaluation BLEU: 0.0000\n",
      "Epoch [209/2000], Evaluation Loss: 22.8601, Evaluation BLEU: 0.0000\n",
      "Epoch [210/2000], Evaluation Loss: 22.8621, Evaluation BLEU: 0.0000\n",
      "Epoch [211/2000], Evaluation Loss: 22.8641, Evaluation BLEU: 0.0000\n",
      "Epoch [212/2000], Evaluation Loss: 22.8660, Evaluation BLEU: 0.0000\n",
      "Epoch [213/2000], Evaluation Loss: 22.8679, Evaluation BLEU: 0.0000\n",
      "Epoch [214/2000], Evaluation Loss: 22.8698, Evaluation BLEU: 0.0000\n",
      "Epoch [215/2000], Evaluation Loss: 22.8717, Evaluation BLEU: 0.0000\n",
      "Epoch [216/2000], Evaluation Loss: 22.8736, Evaluation BLEU: 0.0000\n",
      "Epoch [217/2000], Evaluation Loss: 22.8755, Evaluation BLEU: 0.0000\n",
      "Epoch [218/2000], Evaluation Loss: 22.8774, Evaluation BLEU: 0.0000\n",
      "Epoch [219/2000], Evaluation Loss: 22.8792, Evaluation BLEU: 0.0000\n",
      "Epoch [220/2000], Evaluation Loss: 22.8811, Evaluation BLEU: 0.0000\n",
      "Epoch [221/2000], Evaluation Loss: 22.8829, Evaluation BLEU: 0.0000\n",
      "Epoch [222/2000], Evaluation Loss: 22.8848, Evaluation BLEU: 0.0000\n",
      "Epoch [223/2000], Evaluation Loss: 22.8866, Evaluation BLEU: 0.0000\n",
      "Epoch [224/2000], Evaluation Loss: 22.8884, Evaluation BLEU: 0.0000\n",
      "Epoch [225/2000], Evaluation Loss: 22.8902, Evaluation BLEU: 0.0000\n",
      "Epoch [226/2000], Evaluation Loss: 22.8920, Evaluation BLEU: 0.0000\n",
      "Epoch [227/2000], Evaluation Loss: 22.8937, Evaluation BLEU: 0.0000\n",
      "Epoch [228/2000], Evaluation Loss: 22.8955, Evaluation BLEU: 0.0000\n",
      "Epoch [229/2000], Evaluation Loss: 22.8972, Evaluation BLEU: 0.0000\n",
      "Epoch [230/2000], Evaluation Loss: 22.8990, Evaluation BLEU: 0.0000\n",
      "Epoch [231/2000], Evaluation Loss: 22.9007, Evaluation BLEU: 0.0000\n",
      "Epoch [232/2000], Evaluation Loss: 22.9024, Evaluation BLEU: 0.0000\n",
      "Epoch [233/2000], Evaluation Loss: 22.9041, Evaluation BLEU: 0.0000\n",
      "Epoch [234/2000], Evaluation Loss: 22.9058, Evaluation BLEU: 0.0000\n",
      "Epoch [235/2000], Evaluation Loss: 22.9075, Evaluation BLEU: 0.0000\n",
      "Epoch [236/2000], Evaluation Loss: 22.9092, Evaluation BLEU: 0.0000\n",
      "Epoch [237/2000], Evaluation Loss: 22.9108, Evaluation BLEU: 0.0000\n",
      "Epoch [238/2000], Evaluation Loss: 22.9125, Evaluation BLEU: 0.0000\n",
      "Epoch [239/2000], Evaluation Loss: 22.9141, Evaluation BLEU: 0.0000\n",
      "Epoch [240/2000], Evaluation Loss: 22.9158, Evaluation BLEU: 0.0000\n",
      "Epoch [241/2000], Evaluation Loss: 22.9174, Evaluation BLEU: 0.0000\n",
      "Epoch [242/2000], Evaluation Loss: 22.9190, Evaluation BLEU: 0.0000\n",
      "Epoch [243/2000], Evaluation Loss: 22.9205, Evaluation BLEU: 0.0000\n",
      "Epoch [244/2000], Evaluation Loss: 22.9221, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [245/2000], Evaluation Loss: 22.9237, Evaluation BLEU: 0.0000\n",
      "Epoch [246/2000], Evaluation Loss: 22.9253, Evaluation BLEU: 0.0000\n",
      "Epoch [247/2000], Evaluation Loss: 22.9268, Evaluation BLEU: 0.0000\n",
      "Epoch [248/2000], Evaluation Loss: 22.9283, Evaluation BLEU: 0.0000\n",
      "Epoch [249/2000], Evaluation Loss: 22.9299, Evaluation BLEU: 0.0000\n",
      "Epoch [250/2000], Evaluation Loss: 22.9314, Evaluation BLEU: 0.0000\n",
      "Epoch [251/2000], Evaluation Loss: 22.9329, Evaluation BLEU: 0.0000\n",
      "Epoch [252/2000], Evaluation Loss: 22.9344, Evaluation BLEU: 0.0000\n",
      "Epoch [253/2000], Evaluation Loss: 22.9359, Evaluation BLEU: 0.0000\n",
      "Epoch [254/2000], Evaluation Loss: 22.9373, Evaluation BLEU: 0.0000\n",
      "Epoch [255/2000], Evaluation Loss: 22.9388, Evaluation BLEU: 0.0000\n",
      "Epoch [256/2000], Evaluation Loss: 22.9403, Evaluation BLEU: 0.0000\n",
      "Epoch [257/2000], Evaluation Loss: 22.9417, Evaluation BLEU: 0.0000\n",
      "Epoch [258/2000], Evaluation Loss: 22.9431, Evaluation BLEU: 0.0000\n",
      "Epoch [259/2000], Evaluation Loss: 22.9446, Evaluation BLEU: 0.0000\n",
      "Epoch [260/2000], Evaluation Loss: 22.9460, Evaluation BLEU: 0.0000\n",
      "Epoch [261/2000], Evaluation Loss: 22.9474, Evaluation BLEU: 0.0000\n",
      "Epoch [262/2000], Evaluation Loss: 22.9488, Evaluation BLEU: 0.0000\n",
      "Epoch [263/2000], Evaluation Loss: 22.9502, Evaluation BLEU: 0.0000\n",
      "Epoch [264/2000], Evaluation Loss: 22.9516, Evaluation BLEU: 0.0000\n",
      "Epoch [265/2000], Evaluation Loss: 22.9530, Evaluation BLEU: 0.0000\n",
      "Epoch [266/2000], Evaluation Loss: 22.9544, Evaluation BLEU: 0.0000\n",
      "Epoch [267/2000], Evaluation Loss: 22.9558, Evaluation BLEU: 0.0000\n",
      "Epoch [268/2000], Evaluation Loss: 22.9572, Evaluation BLEU: 0.0000\n",
      "Epoch [269/2000], Evaluation Loss: 22.9585, Evaluation BLEU: 0.0000\n",
      "Epoch [270/2000], Evaluation Loss: 22.9599, Evaluation BLEU: 0.0000\n",
      "Epoch [271/2000], Evaluation Loss: 22.9612, Evaluation BLEU: 0.0000\n",
      "Epoch [272/2000], Evaluation Loss: 22.9626, Evaluation BLEU: 0.0000\n",
      "Epoch [273/2000], Evaluation Loss: 22.9640, Evaluation BLEU: 0.0000\n",
      "Epoch [274/2000], Evaluation Loss: 22.9653, Evaluation BLEU: 0.0000\n",
      "Epoch [275/2000], Evaluation Loss: 22.9666, Evaluation BLEU: 0.0000\n",
      "Epoch [276/2000], Evaluation Loss: 22.9680, Evaluation BLEU: 0.0000\n",
      "Epoch [277/2000], Evaluation Loss: 22.9693, Evaluation BLEU: 0.0000\n",
      "Epoch [278/2000], Evaluation Loss: 22.9706, Evaluation BLEU: 0.0000\n",
      "Epoch [279/2000], Evaluation Loss: 22.9719, Evaluation BLEU: 0.0000\n",
      "Epoch [280/2000], Evaluation Loss: 22.9732, Evaluation BLEU: 0.0000\n",
      "Epoch [281/2000], Evaluation Loss: 22.9745, Evaluation BLEU: 0.0000\n",
      "Epoch [282/2000], Evaluation Loss: 22.9757, Evaluation BLEU: 0.0000\n",
      "Epoch [283/2000], Evaluation Loss: 22.9770, Evaluation BLEU: 0.0000\n",
      "Epoch [284/2000], Evaluation Loss: 22.9782, Evaluation BLEU: 0.0000\n",
      "Epoch [285/2000], Evaluation Loss: 22.9795, Evaluation BLEU: 0.0000\n",
      "Epoch [286/2000], Evaluation Loss: 22.9807, Evaluation BLEU: 0.0000\n",
      "Epoch [287/2000], Evaluation Loss: 22.9820, Evaluation BLEU: 0.0000\n",
      "Epoch [288/2000], Evaluation Loss: 22.9832, Evaluation BLEU: 0.0000\n",
      "Epoch [289/2000], Evaluation Loss: 22.9845, Evaluation BLEU: 0.0000\n",
      "Epoch [290/2000], Evaluation Loss: 22.9857, Evaluation BLEU: 0.0000\n",
      "Epoch [291/2000], Evaluation Loss: 22.9869, Evaluation BLEU: 0.0000\n",
      "Epoch [292/2000], Evaluation Loss: 22.9881, Evaluation BLEU: 0.0000\n",
      "Epoch [293/2000], Evaluation Loss: 22.9893, Evaluation BLEU: 0.0000\n",
      "Epoch [294/2000], Evaluation Loss: 22.9905, Evaluation BLEU: 0.0000\n",
      "Epoch [295/2000], Evaluation Loss: 22.9916, Evaluation BLEU: 0.0000\n",
      "Epoch [296/2000], Evaluation Loss: 22.9928, Evaluation BLEU: 0.0000\n",
      "Epoch [297/2000], Evaluation Loss: 22.9940, Evaluation BLEU: 0.0000\n",
      "Epoch [298/2000], Evaluation Loss: 22.9952, Evaluation BLEU: 0.0000\n",
      "Epoch [299/2000], Evaluation Loss: 22.9963, Evaluation BLEU: 0.0000\n",
      "Epoch [300/2000], Evaluation Loss: 22.9975, Evaluation BLEU: 0.0000\n",
      "Epoch [301/2000], Evaluation Loss: 22.9986, Evaluation BLEU: 0.0000\n",
      "Epoch [302/2000], Evaluation Loss: 22.9997, Evaluation BLEU: 0.0000\n",
      "Epoch [303/2000], Evaluation Loss: 23.0009, Evaluation BLEU: 0.0000\n",
      "Epoch [304/2000], Evaluation Loss: 23.0020, Evaluation BLEU: 0.0000\n",
      "Epoch [305/2000], Evaluation Loss: 23.0032, Evaluation BLEU: 0.0000\n",
      "Epoch [306/2000], Evaluation Loss: 23.0043, Evaluation BLEU: 0.0000\n",
      "Epoch [307/2000], Evaluation Loss: 23.0054, Evaluation BLEU: 0.0000\n",
      "Epoch [308/2000], Evaluation Loss: 23.0066, Evaluation BLEU: 0.0000\n",
      "Epoch [309/2000], Evaluation Loss: 23.0077, Evaluation BLEU: 0.0000\n",
      "Epoch [310/2000], Evaluation Loss: 23.0088, Evaluation BLEU: 0.0000\n",
      "Epoch [311/2000], Evaluation Loss: 23.0099, Evaluation BLEU: 0.0000\n",
      "Epoch [312/2000], Evaluation Loss: 23.0110, Evaluation BLEU: 0.0000\n",
      "Epoch [313/2000], Evaluation Loss: 23.0121, Evaluation BLEU: 0.0000\n",
      "Epoch [314/2000], Evaluation Loss: 23.0132, Evaluation BLEU: 0.0000\n",
      "Epoch [315/2000], Evaluation Loss: 23.0142, Evaluation BLEU: 0.0000\n",
      "Epoch [316/2000], Evaluation Loss: 23.0153, Evaluation BLEU: 0.0000\n",
      "Epoch [317/2000], Evaluation Loss: 23.0164, Evaluation BLEU: 0.0000\n",
      "Epoch [318/2000], Evaluation Loss: 23.0175, Evaluation BLEU: 0.0000\n",
      "Epoch [319/2000], Evaluation Loss: 23.0185, Evaluation BLEU: 0.0000\n",
      "Epoch [320/2000], Evaluation Loss: 23.0196, Evaluation BLEU: 0.0000\n",
      "Epoch [321/2000], Evaluation Loss: 23.0206, Evaluation BLEU: 0.0000\n",
      "Epoch [322/2000], Evaluation Loss: 23.0217, Evaluation BLEU: 0.0000\n",
      "Epoch [323/2000], Evaluation Loss: 23.0227, Evaluation BLEU: 0.0000\n",
      "Epoch [324/2000], Evaluation Loss: 23.0237, Evaluation BLEU: 0.0000\n",
      "Epoch [325/2000], Evaluation Loss: 23.0248, Evaluation BLEU: 0.0000\n",
      "Epoch [326/2000], Evaluation Loss: 23.0258, Evaluation BLEU: 0.0000\n",
      "Epoch [327/2000], Evaluation Loss: 23.0268, Evaluation BLEU: 0.0000\n",
      "Epoch [328/2000], Evaluation Loss: 23.0278, Evaluation BLEU: 0.0000\n",
      "Epoch [329/2000], Evaluation Loss: 23.0288, Evaluation BLEU: 0.0000\n",
      "Epoch [330/2000], Evaluation Loss: 23.0298, Evaluation BLEU: 0.0000\n",
      "Epoch [331/2000], Evaluation Loss: 23.0308, Evaluation BLEU: 0.0000\n",
      "Epoch [332/2000], Evaluation Loss: 23.0317, Evaluation BLEU: 0.0000\n",
      "Epoch [333/2000], Evaluation Loss: 23.0327, Evaluation BLEU: 0.0000\n",
      "Epoch [334/2000], Evaluation Loss: 23.0337, Evaluation BLEU: 0.0000\n",
      "Epoch [335/2000], Evaluation Loss: 23.0346, Evaluation BLEU: 0.0000\n",
      "Epoch [336/2000], Evaluation Loss: 23.0355, Evaluation BLEU: 0.0000\n",
      "Epoch [337/2000], Evaluation Loss: 23.0365, Evaluation BLEU: 0.0000\n",
      "Epoch [338/2000], Evaluation Loss: 23.0374, Evaluation BLEU: 0.0000\n",
      "Epoch [339/2000], Evaluation Loss: 23.0383, Evaluation BLEU: 0.0000\n",
      "Epoch [340/2000], Evaluation Loss: 23.0392, Evaluation BLEU: 0.0000\n",
      "Epoch [341/2000], Evaluation Loss: 23.0401, Evaluation BLEU: 0.0000\n",
      "Epoch [342/2000], Evaluation Loss: 23.0410, Evaluation BLEU: 0.0000\n",
      "Epoch [343/2000], Evaluation Loss: 23.0419, Evaluation BLEU: 0.0000\n",
      "Epoch [344/2000], Evaluation Loss: 23.0428, Evaluation BLEU: 0.0000\n",
      "Epoch [345/2000], Evaluation Loss: 23.0436, Evaluation BLEU: 0.0000\n",
      "Epoch [346/2000], Evaluation Loss: 23.0445, Evaluation BLEU: 0.0000\n",
      "Epoch [347/2000], Evaluation Loss: 23.0454, Evaluation BLEU: 0.0000\n",
      "Epoch [348/2000], Evaluation Loss: 23.0463, Evaluation BLEU: 0.0000\n",
      "Epoch [349/2000], Evaluation Loss: 23.0471, Evaluation BLEU: 0.0000\n",
      "Epoch [350/2000], Evaluation Loss: 23.0480, Evaluation BLEU: 0.0000\n",
      "Epoch [351/2000], Evaluation Loss: 23.0489, Evaluation BLEU: 0.0000\n",
      "Epoch [352/2000], Evaluation Loss: 23.0497, Evaluation BLEU: 0.0000\n",
      "Epoch [353/2000], Evaluation Loss: 23.0506, Evaluation BLEU: 0.0000\n",
      "Epoch [354/2000], Evaluation Loss: 23.0514, Evaluation BLEU: 0.0000\n",
      "Epoch [355/2000], Evaluation Loss: 23.0523, Evaluation BLEU: 0.0000\n",
      "Epoch [356/2000], Evaluation Loss: 23.0531, Evaluation BLEU: 0.0000\n",
      "Epoch [357/2000], Evaluation Loss: 23.0540, Evaluation BLEU: 0.0000\n",
      "Epoch [358/2000], Evaluation Loss: 23.0548, Evaluation BLEU: 0.0000\n",
      "Epoch [359/2000], Evaluation Loss: 23.0557, Evaluation BLEU: 0.0000\n",
      "Epoch [360/2000], Evaluation Loss: 23.0565, Evaluation BLEU: 0.0000\n",
      "Epoch [361/2000], Evaluation Loss: 23.0573, Evaluation BLEU: 0.0000\n",
      "Epoch [362/2000], Evaluation Loss: 23.0582, Evaluation BLEU: 0.0000\n",
      "Epoch [363/2000], Evaluation Loss: 23.0590, Evaluation BLEU: 0.0000\n",
      "Epoch [364/2000], Evaluation Loss: 23.0598, Evaluation BLEU: 0.0000\n",
      "Epoch [365/2000], Evaluation Loss: 23.0607, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [366/2000], Evaluation Loss: 23.0615, Evaluation BLEU: 0.0000\n",
      "Epoch [367/2000], Evaluation Loss: 23.0624, Evaluation BLEU: 0.0000\n",
      "Epoch [368/2000], Evaluation Loss: 23.0632, Evaluation BLEU: 0.0000\n",
      "Epoch [369/2000], Evaluation Loss: 23.0640, Evaluation BLEU: 0.0000\n",
      "Epoch [370/2000], Evaluation Loss: 23.0648, Evaluation BLEU: 0.0000\n",
      "Epoch [371/2000], Evaluation Loss: 23.0657, Evaluation BLEU: 0.0000\n",
      "Epoch [372/2000], Evaluation Loss: 23.0665, Evaluation BLEU: 0.0000\n",
      "Epoch [373/2000], Evaluation Loss: 23.0673, Evaluation BLEU: 0.0000\n",
      "Epoch [374/2000], Evaluation Loss: 23.0681, Evaluation BLEU: 0.0000\n",
      "Epoch [375/2000], Evaluation Loss: 23.0689, Evaluation BLEU: 0.0000\n",
      "Epoch [376/2000], Evaluation Loss: 23.0697, Evaluation BLEU: 0.0000\n",
      "Epoch [377/2000], Evaluation Loss: 23.0705, Evaluation BLEU: 0.0000\n",
      "Epoch [378/2000], Evaluation Loss: 23.0713, Evaluation BLEU: 0.0000\n",
      "Epoch [379/2000], Evaluation Loss: 23.0721, Evaluation BLEU: 0.0000\n",
      "Epoch [380/2000], Evaluation Loss: 23.0729, Evaluation BLEU: 0.0000\n",
      "Epoch [381/2000], Evaluation Loss: 23.0737, Evaluation BLEU: 0.0000\n",
      "Epoch [382/2000], Evaluation Loss: 23.0744, Evaluation BLEU: 0.0000\n",
      "Epoch [383/2000], Evaluation Loss: 23.0752, Evaluation BLEU: 0.0000\n",
      "Epoch [384/2000], Evaluation Loss: 23.0760, Evaluation BLEU: 0.0000\n",
      "Epoch [385/2000], Evaluation Loss: 23.0767, Evaluation BLEU: 0.0000\n",
      "Epoch [386/2000], Evaluation Loss: 23.0775, Evaluation BLEU: 0.0000\n",
      "Epoch [387/2000], Evaluation Loss: 23.0783, Evaluation BLEU: 0.0000\n",
      "Epoch [388/2000], Evaluation Loss: 23.0790, Evaluation BLEU: 0.0000\n",
      "Epoch [389/2000], Evaluation Loss: 23.0798, Evaluation BLEU: 0.0000\n",
      "Epoch [390/2000], Evaluation Loss: 23.0805, Evaluation BLEU: 0.0000\n",
      "Epoch [391/2000], Evaluation Loss: 23.0812, Evaluation BLEU: 0.0000\n",
      "Epoch [392/2000], Evaluation Loss: 23.0820, Evaluation BLEU: 0.0000\n",
      "Epoch [393/2000], Evaluation Loss: 23.0828, Evaluation BLEU: 0.0000\n",
      "Epoch [394/2000], Evaluation Loss: 23.0835, Evaluation BLEU: 0.0000\n",
      "Epoch [395/2000], Evaluation Loss: 23.0842, Evaluation BLEU: 0.0000\n",
      "Epoch [396/2000], Evaluation Loss: 23.0850, Evaluation BLEU: 0.0000\n",
      "Epoch [397/2000], Evaluation Loss: 23.0857, Evaluation BLEU: 0.0000\n",
      "Epoch [398/2000], Evaluation Loss: 23.0864, Evaluation BLEU: 0.0000\n",
      "Epoch [399/2000], Evaluation Loss: 23.0872, Evaluation BLEU: 0.0000\n",
      "Epoch [400/2000], Evaluation Loss: 23.0879, Evaluation BLEU: 0.0000\n",
      "Epoch [401/2000], Evaluation Loss: 23.0886, Evaluation BLEU: 0.0000\n",
      "Epoch [402/2000], Evaluation Loss: 23.0893, Evaluation BLEU: 0.0000\n",
      "Epoch [403/2000], Evaluation Loss: 23.0901, Evaluation BLEU: 0.0000\n",
      "Epoch [404/2000], Evaluation Loss: 23.0908, Evaluation BLEU: 0.0000\n",
      "Epoch [405/2000], Evaluation Loss: 23.0916, Evaluation BLEU: 0.0000\n",
      "Epoch [406/2000], Evaluation Loss: 23.0923, Evaluation BLEU: 0.0000\n",
      "Epoch [407/2000], Evaluation Loss: 23.0930, Evaluation BLEU: 0.0000\n",
      "Epoch [408/2000], Evaluation Loss: 23.0938, Evaluation BLEU: 0.0000\n",
      "Epoch [409/2000], Evaluation Loss: 23.0945, Evaluation BLEU: 0.0000\n",
      "Epoch [410/2000], Evaluation Loss: 23.0952, Evaluation BLEU: 0.0000\n",
      "Epoch [411/2000], Evaluation Loss: 23.0960, Evaluation BLEU: 0.0000\n",
      "Epoch [412/2000], Evaluation Loss: 23.0967, Evaluation BLEU: 0.0000\n",
      "Epoch [413/2000], Evaluation Loss: 23.0975, Evaluation BLEU: 0.0000\n",
      "Epoch [414/2000], Evaluation Loss: 23.0982, Evaluation BLEU: 0.0000\n",
      "Epoch [415/2000], Evaluation Loss: 23.0989, Evaluation BLEU: 0.0000\n",
      "Epoch [416/2000], Evaluation Loss: 23.0997, Evaluation BLEU: 0.0000\n",
      "Epoch [417/2000], Evaluation Loss: 23.1004, Evaluation BLEU: 0.0000\n",
      "Epoch [418/2000], Evaluation Loss: 23.1012, Evaluation BLEU: 0.0000\n",
      "Epoch [419/2000], Evaluation Loss: 23.1019, Evaluation BLEU: 0.0000\n",
      "Epoch [420/2000], Evaluation Loss: 23.1026, Evaluation BLEU: 0.0000\n",
      "Epoch [421/2000], Evaluation Loss: 23.1034, Evaluation BLEU: 0.0000\n",
      "Epoch [422/2000], Evaluation Loss: 23.1041, Evaluation BLEU: 0.0000\n",
      "Epoch [423/2000], Evaluation Loss: 23.1049, Evaluation BLEU: 0.0000\n",
      "Epoch [424/2000], Evaluation Loss: 23.1056, Evaluation BLEU: 0.0000\n",
      "Epoch [425/2000], Evaluation Loss: 23.1063, Evaluation BLEU: 0.0000\n",
      "Epoch [426/2000], Evaluation Loss: 23.1070, Evaluation BLEU: 0.0000\n",
      "Epoch [427/2000], Evaluation Loss: 23.1077, Evaluation BLEU: 0.0000\n",
      "Epoch [428/2000], Evaluation Loss: 23.1084, Evaluation BLEU: 0.0000\n",
      "Epoch [429/2000], Evaluation Loss: 23.1091, Evaluation BLEU: 0.0000\n",
      "Epoch [430/2000], Evaluation Loss: 23.1098, Evaluation BLEU: 0.0000\n",
      "Epoch [431/2000], Evaluation Loss: 23.1105, Evaluation BLEU: 0.0000\n",
      "Epoch [432/2000], Evaluation Loss: 23.1112, Evaluation BLEU: 0.0000\n",
      "Epoch [433/2000], Evaluation Loss: 23.1119, Evaluation BLEU: 0.0000\n",
      "Epoch [434/2000], Evaluation Loss: 23.1126, Evaluation BLEU: 0.0000\n",
      "Epoch [435/2000], Evaluation Loss: 23.1133, Evaluation BLEU: 0.0000\n",
      "Epoch [436/2000], Evaluation Loss: 23.1140, Evaluation BLEU: 0.0000\n",
      "Epoch [437/2000], Evaluation Loss: 23.1147, Evaluation BLEU: 0.0000\n",
      "Epoch [438/2000], Evaluation Loss: 23.1154, Evaluation BLEU: 0.0000\n",
      "Epoch [439/2000], Evaluation Loss: 23.1161, Evaluation BLEU: 0.0000\n",
      "Epoch [440/2000], Evaluation Loss: 23.1168, Evaluation BLEU: 0.0000\n",
      "Epoch [441/2000], Evaluation Loss: 23.1175, Evaluation BLEU: 0.0000\n",
      "Epoch [442/2000], Evaluation Loss: 23.1182, Evaluation BLEU: 0.0000\n",
      "Epoch [443/2000], Evaluation Loss: 23.1189, Evaluation BLEU: 0.0000\n",
      "Epoch [444/2000], Evaluation Loss: 23.1195, Evaluation BLEU: 0.0000\n",
      "Epoch [445/2000], Evaluation Loss: 23.1202, Evaluation BLEU: 0.0000\n",
      "Epoch [446/2000], Evaluation Loss: 23.1209, Evaluation BLEU: 0.0000\n",
      "Epoch [447/2000], Evaluation Loss: 23.1216, Evaluation BLEU: 0.0000\n",
      "Epoch [448/2000], Evaluation Loss: 23.1223, Evaluation BLEU: 0.0000\n",
      "Epoch [449/2000], Evaluation Loss: 23.1229, Evaluation BLEU: 0.0000\n",
      "Epoch [450/2000], Evaluation Loss: 23.1236, Evaluation BLEU: 0.0000\n",
      "Epoch [451/2000], Evaluation Loss: 23.1242, Evaluation BLEU: 0.0000\n",
      "Epoch [452/2000], Evaluation Loss: 23.1249, Evaluation BLEU: 0.0000\n",
      "Epoch [453/2000], Evaluation Loss: 23.1256, Evaluation BLEU: 0.0000\n",
      "Epoch [454/2000], Evaluation Loss: 23.1262, Evaluation BLEU: 0.0000\n",
      "Epoch [455/2000], Evaluation Loss: 23.1269, Evaluation BLEU: 0.0000\n",
      "Epoch [456/2000], Evaluation Loss: 23.1275, Evaluation BLEU: 0.0000\n",
      "Epoch [457/2000], Evaluation Loss: 23.1281, Evaluation BLEU: 0.0000\n",
      "Epoch [458/2000], Evaluation Loss: 23.1288, Evaluation BLEU: 0.0000\n",
      "Epoch [459/2000], Evaluation Loss: 23.1294, Evaluation BLEU: 0.0000\n",
      "Epoch [460/2000], Evaluation Loss: 23.1300, Evaluation BLEU: 0.0000\n",
      "Epoch [461/2000], Evaluation Loss: 23.1306, Evaluation BLEU: 0.0000\n",
      "Epoch [462/2000], Evaluation Loss: 23.1313, Evaluation BLEU: 0.0000\n",
      "Epoch [463/2000], Evaluation Loss: 23.1319, Evaluation BLEU: 0.0000\n",
      "Epoch [464/2000], Evaluation Loss: 23.1325, Evaluation BLEU: 0.0000\n",
      "Epoch [465/2000], Evaluation Loss: 23.1331, Evaluation BLEU: 0.0000\n",
      "Epoch [466/2000], Evaluation Loss: 23.1337, Evaluation BLEU: 0.0000\n",
      "Epoch [467/2000], Evaluation Loss: 23.1343, Evaluation BLEU: 0.0000\n",
      "Epoch [468/2000], Evaluation Loss: 23.1349, Evaluation BLEU: 0.0000\n",
      "Epoch [469/2000], Evaluation Loss: 23.1355, Evaluation BLEU: 0.0000\n",
      "Epoch [470/2000], Evaluation Loss: 23.1361, Evaluation BLEU: 0.0000\n",
      "Epoch [471/2000], Evaluation Loss: 23.1367, Evaluation BLEU: 0.0000\n",
      "Epoch [472/2000], Evaluation Loss: 23.1373, Evaluation BLEU: 0.0000\n",
      "Epoch [473/2000], Evaluation Loss: 23.1379, Evaluation BLEU: 0.0000\n",
      "Epoch [474/2000], Evaluation Loss: 23.1385, Evaluation BLEU: 0.0000\n",
      "Epoch [475/2000], Evaluation Loss: 23.1391, Evaluation BLEU: 0.0000\n",
      "Epoch [476/2000], Evaluation Loss: 23.1397, Evaluation BLEU: 0.0000\n",
      "Epoch [477/2000], Evaluation Loss: 23.1403, Evaluation BLEU: 0.0000\n",
      "Epoch [478/2000], Evaluation Loss: 23.1410, Evaluation BLEU: 0.0000\n",
      "Epoch [479/2000], Evaluation Loss: 23.1416, Evaluation BLEU: 0.0000\n",
      "Epoch [480/2000], Evaluation Loss: 23.1422, Evaluation BLEU: 0.0000\n",
      "Epoch [481/2000], Evaluation Loss: 23.1428, Evaluation BLEU: 0.0000\n",
      "Epoch [482/2000], Evaluation Loss: 23.1434, Evaluation BLEU: 0.0000\n",
      "Epoch [483/2000], Evaluation Loss: 23.1440, Evaluation BLEU: 0.0000\n",
      "Epoch [484/2000], Evaluation Loss: 23.1446, Evaluation BLEU: 0.0000\n",
      "Epoch [485/2000], Evaluation Loss: 23.1452, Evaluation BLEU: 0.0000\n",
      "Epoch [486/2000], Evaluation Loss: 23.1459, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [487/2000], Evaluation Loss: 23.1465, Evaluation BLEU: 0.0000\n",
      "Epoch [488/2000], Evaluation Loss: 23.1471, Evaluation BLEU: 0.0000\n",
      "Epoch [489/2000], Evaluation Loss: 23.1477, Evaluation BLEU: 0.0000\n",
      "Epoch [490/2000], Evaluation Loss: 23.1483, Evaluation BLEU: 0.0000\n",
      "Epoch [491/2000], Evaluation Loss: 23.1490, Evaluation BLEU: 0.0000\n",
      "Epoch [492/2000], Evaluation Loss: 23.1496, Evaluation BLEU: 0.0000\n",
      "Epoch [493/2000], Evaluation Loss: 23.1502, Evaluation BLEU: 0.0000\n",
      "Epoch [494/2000], Evaluation Loss: 23.1508, Evaluation BLEU: 0.0000\n",
      "Epoch [495/2000], Evaluation Loss: 23.1514, Evaluation BLEU: 0.0000\n",
      "Epoch [496/2000], Evaluation Loss: 23.1520, Evaluation BLEU: 0.0000\n",
      "Epoch [497/2000], Evaluation Loss: 23.1526, Evaluation BLEU: 0.0000\n",
      "Epoch [498/2000], Evaluation Loss: 23.1532, Evaluation BLEU: 0.0000\n",
      "Epoch [499/2000], Evaluation Loss: 23.1537, Evaluation BLEU: 0.0000\n",
      "Epoch [500/2000], Evaluation Loss: 23.1543, Evaluation BLEU: 0.0000\n",
      "Epoch [501/2000], Evaluation Loss: 23.1549, Evaluation BLEU: 0.0000\n",
      "Epoch [502/2000], Evaluation Loss: 23.1554, Evaluation BLEU: 0.0000\n",
      "Epoch [503/2000], Evaluation Loss: 23.1560, Evaluation BLEU: 0.0000\n",
      "Epoch [504/2000], Evaluation Loss: 23.1566, Evaluation BLEU: 0.0000\n",
      "Epoch [505/2000], Evaluation Loss: 23.1571, Evaluation BLEU: 0.0000\n",
      "Epoch [506/2000], Evaluation Loss: 23.1576, Evaluation BLEU: 0.0000\n",
      "Epoch [507/2000], Evaluation Loss: 23.1581, Evaluation BLEU: 0.0000\n",
      "Epoch [508/2000], Evaluation Loss: 23.1587, Evaluation BLEU: 0.0000\n",
      "Epoch [509/2000], Evaluation Loss: 23.1592, Evaluation BLEU: 0.0000\n",
      "Epoch [510/2000], Evaluation Loss: 23.1597, Evaluation BLEU: 0.0000\n",
      "Epoch [511/2000], Evaluation Loss: 23.1602, Evaluation BLEU: 0.0000\n",
      "Epoch [512/2000], Evaluation Loss: 23.1607, Evaluation BLEU: 0.0000\n",
      "Epoch [513/2000], Evaluation Loss: 23.1611, Evaluation BLEU: 0.0000\n",
      "Epoch [514/2000], Evaluation Loss: 23.1616, Evaluation BLEU: 0.0000\n",
      "Epoch [515/2000], Evaluation Loss: 23.1621, Evaluation BLEU: 0.0000\n",
      "Epoch [516/2000], Evaluation Loss: 23.1626, Evaluation BLEU: 0.0000\n",
      "Epoch [517/2000], Evaluation Loss: 23.1631, Evaluation BLEU: 0.0000\n",
      "Epoch [518/2000], Evaluation Loss: 23.1636, Evaluation BLEU: 0.0000\n",
      "Epoch [519/2000], Evaluation Loss: 23.1640, Evaluation BLEU: 0.0000\n",
      "Epoch [520/2000], Evaluation Loss: 23.1645, Evaluation BLEU: 0.0000\n",
      "Epoch [521/2000], Evaluation Loss: 23.1650, Evaluation BLEU: 0.0000\n",
      "Epoch [522/2000], Evaluation Loss: 23.1655, Evaluation BLEU: 0.0000\n",
      "Epoch [523/2000], Evaluation Loss: 23.1660, Evaluation BLEU: 0.0000\n",
      "Epoch [524/2000], Evaluation Loss: 23.1665, Evaluation BLEU: 0.0000\n",
      "Epoch [525/2000], Evaluation Loss: 23.1670, Evaluation BLEU: 0.0000\n",
      "Epoch [526/2000], Evaluation Loss: 23.1675, Evaluation BLEU: 0.0000\n",
      "Epoch [527/2000], Evaluation Loss: 23.1680, Evaluation BLEU: 0.0000\n",
      "Epoch [528/2000], Evaluation Loss: 23.1685, Evaluation BLEU: 0.0000\n",
      "Epoch [529/2000], Evaluation Loss: 23.1690, Evaluation BLEU: 0.0000\n",
      "Epoch [530/2000], Evaluation Loss: 23.1695, Evaluation BLEU: 0.0000\n",
      "Epoch [531/2000], Evaluation Loss: 23.1700, Evaluation BLEU: 0.0000\n",
      "Epoch [532/2000], Evaluation Loss: 23.1705, Evaluation BLEU: 0.0000\n",
      "Epoch [533/2000], Evaluation Loss: 23.1710, Evaluation BLEU: 0.0000\n",
      "Epoch [534/2000], Evaluation Loss: 23.1714, Evaluation BLEU: 0.0000\n",
      "Epoch [535/2000], Evaluation Loss: 23.1719, Evaluation BLEU: 0.0000\n",
      "Epoch [536/2000], Evaluation Loss: 23.1724, Evaluation BLEU: 0.0000\n",
      "Epoch [537/2000], Evaluation Loss: 23.1729, Evaluation BLEU: 0.0000\n",
      "Epoch [538/2000], Evaluation Loss: 23.1733, Evaluation BLEU: 0.0000\n",
      "Epoch [539/2000], Evaluation Loss: 23.1738, Evaluation BLEU: 0.0000\n",
      "Epoch [540/2000], Evaluation Loss: 23.1743, Evaluation BLEU: 0.0000\n",
      "Epoch [541/2000], Evaluation Loss: 23.1748, Evaluation BLEU: 0.0000\n",
      "Epoch [542/2000], Evaluation Loss: 23.1752, Evaluation BLEU: 0.0000\n",
      "Epoch [543/2000], Evaluation Loss: 23.1757, Evaluation BLEU: 0.0000\n",
      "Epoch [544/2000], Evaluation Loss: 23.1762, Evaluation BLEU: 0.0000\n",
      "Epoch [545/2000], Evaluation Loss: 23.1767, Evaluation BLEU: 0.0000\n",
      "Epoch [546/2000], Evaluation Loss: 23.1772, Evaluation BLEU: 0.0000\n",
      "Epoch [547/2000], Evaluation Loss: 23.1777, Evaluation BLEU: 0.0000\n",
      "Epoch [548/2000], Evaluation Loss: 23.1782, Evaluation BLEU: 0.0000\n",
      "Epoch [549/2000], Evaluation Loss: 23.1787, Evaluation BLEU: 0.0000\n",
      "Epoch [550/2000], Evaluation Loss: 23.1792, Evaluation BLEU: 0.0000\n",
      "Epoch [551/2000], Evaluation Loss: 23.1798, Evaluation BLEU: 0.0000\n",
      "Epoch [552/2000], Evaluation Loss: 23.1803, Evaluation BLEU: 0.0000\n",
      "Epoch [553/2000], Evaluation Loss: 23.1808, Evaluation BLEU: 0.0000\n",
      "Epoch [554/2000], Evaluation Loss: 23.1813, Evaluation BLEU: 0.0000\n",
      "Epoch [555/2000], Evaluation Loss: 23.1819, Evaluation BLEU: 0.0000\n",
      "Epoch [556/2000], Evaluation Loss: 23.1824, Evaluation BLEU: 0.0000\n",
      "Epoch [557/2000], Evaluation Loss: 23.1829, Evaluation BLEU: 0.0000\n",
      "Epoch [558/2000], Evaluation Loss: 23.1834, Evaluation BLEU: 0.0000\n",
      "Epoch [559/2000], Evaluation Loss: 23.1839, Evaluation BLEU: 0.0000\n",
      "Epoch [560/2000], Evaluation Loss: 23.1845, Evaluation BLEU: 0.0000\n",
      "Epoch [561/2000], Evaluation Loss: 23.1850, Evaluation BLEU: 0.0000\n",
      "Epoch [562/2000], Evaluation Loss: 23.1854, Evaluation BLEU: 0.0000\n",
      "Epoch [563/2000], Evaluation Loss: 23.1859, Evaluation BLEU: 0.0000\n",
      "Epoch [564/2000], Evaluation Loss: 23.1864, Evaluation BLEU: 0.0000\n",
      "Epoch [565/2000], Evaluation Loss: 23.1869, Evaluation BLEU: 0.0000\n",
      "Epoch [566/2000], Evaluation Loss: 23.1873, Evaluation BLEU: 0.0000\n",
      "Epoch [567/2000], Evaluation Loss: 23.1878, Evaluation BLEU: 0.0000\n",
      "Epoch [568/2000], Evaluation Loss: 23.1883, Evaluation BLEU: 0.0000\n",
      "Epoch [569/2000], Evaluation Loss: 23.1887, Evaluation BLEU: 0.0000\n",
      "Epoch [570/2000], Evaluation Loss: 23.1892, Evaluation BLEU: 0.0000\n",
      "Epoch [571/2000], Evaluation Loss: 23.1896, Evaluation BLEU: 0.0000\n",
      "Epoch [572/2000], Evaluation Loss: 23.1901, Evaluation BLEU: 0.0000\n",
      "Epoch [573/2000], Evaluation Loss: 23.1905, Evaluation BLEU: 0.0000\n",
      "Epoch [574/2000], Evaluation Loss: 23.1910, Evaluation BLEU: 0.0000\n",
      "Epoch [575/2000], Evaluation Loss: 23.1915, Evaluation BLEU: 0.0000\n",
      "Epoch [576/2000], Evaluation Loss: 23.1919, Evaluation BLEU: 0.0000\n",
      "Epoch [577/2000], Evaluation Loss: 23.1924, Evaluation BLEU: 0.0000\n",
      "Epoch [578/2000], Evaluation Loss: 23.1928, Evaluation BLEU: 0.0000\n",
      "Epoch [579/2000], Evaluation Loss: 23.1933, Evaluation BLEU: 0.0000\n",
      "Epoch [580/2000], Evaluation Loss: 23.1937, Evaluation BLEU: 0.0000\n",
      "Epoch [581/2000], Evaluation Loss: 23.1942, Evaluation BLEU: 0.0000\n",
      "Epoch [582/2000], Evaluation Loss: 23.1946, Evaluation BLEU: 0.0000\n",
      "Epoch [583/2000], Evaluation Loss: 23.1951, Evaluation BLEU: 0.0000\n",
      "Epoch [584/2000], Evaluation Loss: 23.1955, Evaluation BLEU: 0.0000\n",
      "Epoch [585/2000], Evaluation Loss: 23.1960, Evaluation BLEU: 0.0000\n",
      "Epoch [586/2000], Evaluation Loss: 23.1964, Evaluation BLEU: 0.0000\n",
      "Epoch [587/2000], Evaluation Loss: 23.1969, Evaluation BLEU: 0.0000\n",
      "Epoch [588/2000], Evaluation Loss: 23.1973, Evaluation BLEU: 0.0000\n",
      "Epoch [589/2000], Evaluation Loss: 23.1977, Evaluation BLEU: 0.0000\n",
      "Epoch [590/2000], Evaluation Loss: 23.1982, Evaluation BLEU: 0.0000\n",
      "Epoch [591/2000], Evaluation Loss: 23.1987, Evaluation BLEU: 0.0000\n",
      "Epoch [592/2000], Evaluation Loss: 23.1991, Evaluation BLEU: 0.0000\n",
      "Epoch [593/2000], Evaluation Loss: 23.1996, Evaluation BLEU: 0.0000\n",
      "Epoch [594/2000], Evaluation Loss: 23.2000, Evaluation BLEU: 0.0000\n",
      "Epoch [595/2000], Evaluation Loss: 23.2005, Evaluation BLEU: 0.0000\n",
      "Epoch [596/2000], Evaluation Loss: 23.2009, Evaluation BLEU: 0.0000\n",
      "Epoch [597/2000], Evaluation Loss: 23.2014, Evaluation BLEU: 0.0000\n",
      "Epoch [598/2000], Evaluation Loss: 23.2018, Evaluation BLEU: 0.0000\n",
      "Epoch [599/2000], Evaluation Loss: 23.2023, Evaluation BLEU: 0.0000\n",
      "Epoch [600/2000], Evaluation Loss: 23.2027, Evaluation BLEU: 0.0000\n",
      "Epoch [601/2000], Evaluation Loss: 23.2032, Evaluation BLEU: 0.0000\n",
      "Epoch [602/2000], Evaluation Loss: 23.2036, Evaluation BLEU: 0.0000\n",
      "Epoch [603/2000], Evaluation Loss: 23.2041, Evaluation BLEU: 0.0000\n",
      "Epoch [604/2000], Evaluation Loss: 23.2046, Evaluation BLEU: 0.0000\n",
      "Epoch [605/2000], Evaluation Loss: 23.2050, Evaluation BLEU: 0.0000\n",
      "Epoch [606/2000], Evaluation Loss: 23.2055, Evaluation BLEU: 0.0000\n",
      "Epoch [607/2000], Evaluation Loss: 23.2059, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [608/2000], Evaluation Loss: 23.2064, Evaluation BLEU: 0.0000\n",
      "Epoch [609/2000], Evaluation Loss: 23.2068, Evaluation BLEU: 0.0000\n",
      "Epoch [610/2000], Evaluation Loss: 23.2073, Evaluation BLEU: 0.0000\n",
      "Epoch [611/2000], Evaluation Loss: 23.2077, Evaluation BLEU: 0.0000\n",
      "Epoch [612/2000], Evaluation Loss: 23.2082, Evaluation BLEU: 0.0000\n",
      "Epoch [613/2000], Evaluation Loss: 23.2086, Evaluation BLEU: 0.0000\n",
      "Epoch [614/2000], Evaluation Loss: 23.2091, Evaluation BLEU: 0.0000\n",
      "Epoch [615/2000], Evaluation Loss: 23.2095, Evaluation BLEU: 0.0000\n",
      "Epoch [616/2000], Evaluation Loss: 23.2100, Evaluation BLEU: 0.0000\n",
      "Epoch [617/2000], Evaluation Loss: 23.2104, Evaluation BLEU: 0.0000\n",
      "Epoch [618/2000], Evaluation Loss: 23.2109, Evaluation BLEU: 0.0000\n",
      "Epoch [619/2000], Evaluation Loss: 23.2113, Evaluation BLEU: 0.0000\n",
      "Epoch [620/2000], Evaluation Loss: 23.2118, Evaluation BLEU: 0.0000\n",
      "Epoch [621/2000], Evaluation Loss: 23.2122, Evaluation BLEU: 0.0000\n",
      "Epoch [622/2000], Evaluation Loss: 23.2126, Evaluation BLEU: 0.0000\n",
      "Epoch [623/2000], Evaluation Loss: 23.2130, Evaluation BLEU: 0.0000\n",
      "Epoch [624/2000], Evaluation Loss: 23.2134, Evaluation BLEU: 0.0000\n",
      "Epoch [625/2000], Evaluation Loss: 23.2138, Evaluation BLEU: 0.0000\n",
      "Epoch [626/2000], Evaluation Loss: 23.2142, Evaluation BLEU: 0.0000\n",
      "Epoch [627/2000], Evaluation Loss: 23.2146, Evaluation BLEU: 0.0000\n",
      "Epoch [628/2000], Evaluation Loss: 23.2150, Evaluation BLEU: 0.0000\n",
      "Epoch [629/2000], Evaluation Loss: 23.2154, Evaluation BLEU: 0.0000\n",
      "Epoch [630/2000], Evaluation Loss: 23.2158, Evaluation BLEU: 0.0000\n",
      "Epoch [631/2000], Evaluation Loss: 23.2162, Evaluation BLEU: 0.0000\n",
      "Epoch [632/2000], Evaluation Loss: 23.2165, Evaluation BLEU: 0.0000\n",
      "Epoch [633/2000], Evaluation Loss: 23.2169, Evaluation BLEU: 0.0000\n",
      "Epoch [634/2000], Evaluation Loss: 23.2173, Evaluation BLEU: 0.0000\n",
      "Epoch [635/2000], Evaluation Loss: 23.2177, Evaluation BLEU: 0.0000\n",
      "Epoch [636/2000], Evaluation Loss: 23.2180, Evaluation BLEU: 0.0000\n",
      "Epoch [637/2000], Evaluation Loss: 23.2184, Evaluation BLEU: 0.0000\n",
      "Epoch [638/2000], Evaluation Loss: 23.2188, Evaluation BLEU: 0.0000\n",
      "Epoch [639/2000], Evaluation Loss: 23.2192, Evaluation BLEU: 0.0000\n",
      "Epoch [640/2000], Evaluation Loss: 23.2196, Evaluation BLEU: 0.0000\n",
      "Epoch [641/2000], Evaluation Loss: 23.2200, Evaluation BLEU: 0.0000\n",
      "Epoch [642/2000], Evaluation Loss: 23.2204, Evaluation BLEU: 0.0000\n",
      "Epoch [643/2000], Evaluation Loss: 23.2208, Evaluation BLEU: 0.0000\n",
      "Epoch [644/2000], Evaluation Loss: 23.2212, Evaluation BLEU: 0.0000\n",
      "Epoch [645/2000], Evaluation Loss: 23.2217, Evaluation BLEU: 0.0000\n",
      "Epoch [646/2000], Evaluation Loss: 23.2221, Evaluation BLEU: 0.0000\n",
      "Epoch [647/2000], Evaluation Loss: 23.2225, Evaluation BLEU: 0.0000\n",
      "Epoch [648/2000], Evaluation Loss: 23.2230, Evaluation BLEU: 0.0000\n",
      "Epoch [649/2000], Evaluation Loss: 23.2234, Evaluation BLEU: 0.0000\n",
      "Epoch [650/2000], Evaluation Loss: 23.2238, Evaluation BLEU: 0.0000\n",
      "Epoch [651/2000], Evaluation Loss: 23.2242, Evaluation BLEU: 0.0000\n",
      "Epoch [652/2000], Evaluation Loss: 23.2246, Evaluation BLEU: 0.0000\n",
      "Epoch [653/2000], Evaluation Loss: 23.2251, Evaluation BLEU: 0.0000\n",
      "Epoch [654/2000], Evaluation Loss: 23.2255, Evaluation BLEU: 0.0000\n",
      "Epoch [655/2000], Evaluation Loss: 23.2259, Evaluation BLEU: 0.0000\n",
      "Epoch [656/2000], Evaluation Loss: 23.2264, Evaluation BLEU: 0.0000\n",
      "Epoch [657/2000], Evaluation Loss: 23.2268, Evaluation BLEU: 0.0000\n",
      "Epoch [658/2000], Evaluation Loss: 23.2273, Evaluation BLEU: 0.0000\n",
      "Epoch [659/2000], Evaluation Loss: 23.2278, Evaluation BLEU: 0.0000\n",
      "Epoch [660/2000], Evaluation Loss: 23.2282, Evaluation BLEU: 0.0000\n",
      "Epoch [661/2000], Evaluation Loss: 23.2286, Evaluation BLEU: 0.0000\n",
      "Epoch [662/2000], Evaluation Loss: 23.2291, Evaluation BLEU: 0.0000\n",
      "Epoch [663/2000], Evaluation Loss: 23.2295, Evaluation BLEU: 0.0000\n",
      "Epoch [664/2000], Evaluation Loss: 23.2300, Evaluation BLEU: 0.0000\n",
      "Epoch [665/2000], Evaluation Loss: 23.2304, Evaluation BLEU: 0.0000\n",
      "Epoch [666/2000], Evaluation Loss: 23.2309, Evaluation BLEU: 0.0000\n",
      "Epoch [667/2000], Evaluation Loss: 23.2313, Evaluation BLEU: 0.0000\n",
      "Epoch [668/2000], Evaluation Loss: 23.2317, Evaluation BLEU: 0.0000\n",
      "Epoch [669/2000], Evaluation Loss: 23.2322, Evaluation BLEU: 0.0000\n",
      "Epoch [670/2000], Evaluation Loss: 23.2326, Evaluation BLEU: 0.0000\n",
      "Epoch [671/2000], Evaluation Loss: 23.2330, Evaluation BLEU: 0.0000\n",
      "Epoch [672/2000], Evaluation Loss: 23.2334, Evaluation BLEU: 0.0000\n",
      "Epoch [673/2000], Evaluation Loss: 23.2339, Evaluation BLEU: 0.0000\n",
      "Epoch [674/2000], Evaluation Loss: 23.2343, Evaluation BLEU: 0.0000\n",
      "Epoch [675/2000], Evaluation Loss: 23.2348, Evaluation BLEU: 0.0000\n",
      "Epoch [676/2000], Evaluation Loss: 23.2352, Evaluation BLEU: 0.0000\n",
      "Epoch [677/2000], Evaluation Loss: 23.2356, Evaluation BLEU: 0.0000\n",
      "Epoch [678/2000], Evaluation Loss: 23.2361, Evaluation BLEU: 0.0000\n",
      "Epoch [679/2000], Evaluation Loss: 23.2365, Evaluation BLEU: 0.0000\n",
      "Epoch [680/2000], Evaluation Loss: 23.2369, Evaluation BLEU: 0.0000\n",
      "Epoch [681/2000], Evaluation Loss: 23.2374, Evaluation BLEU: 0.0000\n",
      "Epoch [682/2000], Evaluation Loss: 23.2378, Evaluation BLEU: 0.0000\n",
      "Epoch [683/2000], Evaluation Loss: 23.2383, Evaluation BLEU: 0.0000\n",
      "Epoch [684/2000], Evaluation Loss: 23.2388, Evaluation BLEU: 0.0000\n",
      "Epoch [685/2000], Evaluation Loss: 23.2392, Evaluation BLEU: 0.0000\n",
      "Epoch [686/2000], Evaluation Loss: 23.2397, Evaluation BLEU: 0.0000\n",
      "Epoch [687/2000], Evaluation Loss: 23.2401, Evaluation BLEU: 0.0000\n",
      "Epoch [688/2000], Evaluation Loss: 23.2407, Evaluation BLEU: 0.0000\n",
      "Epoch [689/2000], Evaluation Loss: 23.2412, Evaluation BLEU: 0.0000\n",
      "Epoch [690/2000], Evaluation Loss: 23.2417, Evaluation BLEU: 0.0000\n",
      "Epoch [691/2000], Evaluation Loss: 23.2422, Evaluation BLEU: 0.0000\n",
      "Epoch [692/2000], Evaluation Loss: 23.2427, Evaluation BLEU: 0.0000\n",
      "Epoch [693/2000], Evaluation Loss: 23.2432, Evaluation BLEU: 0.0000\n",
      "Epoch [694/2000], Evaluation Loss: 23.2437, Evaluation BLEU: 0.0000\n",
      "Epoch [695/2000], Evaluation Loss: 23.2442, Evaluation BLEU: 0.0000\n",
      "Epoch [696/2000], Evaluation Loss: 23.2447, Evaluation BLEU: 0.0000\n",
      "Epoch [697/2000], Evaluation Loss: 23.2452, Evaluation BLEU: 0.0000\n",
      "Epoch [698/2000], Evaluation Loss: 23.2458, Evaluation BLEU: 0.0000\n",
      "Epoch [699/2000], Evaluation Loss: 23.2463, Evaluation BLEU: 0.0000\n",
      "Epoch [700/2000], Evaluation Loss: 23.2468, Evaluation BLEU: 0.0000\n",
      "Epoch [701/2000], Evaluation Loss: 23.2473, Evaluation BLEU: 0.0000\n",
      "Epoch [702/2000], Evaluation Loss: 23.2478, Evaluation BLEU: 0.0000\n",
      "Epoch [703/2000], Evaluation Loss: 23.2483, Evaluation BLEU: 0.0000\n",
      "Epoch [704/2000], Evaluation Loss: 23.2489, Evaluation BLEU: 0.0000\n",
      "Epoch [705/2000], Evaluation Loss: 23.2494, Evaluation BLEU: 0.0000\n",
      "Epoch [706/2000], Evaluation Loss: 23.2499, Evaluation BLEU: 0.0000\n",
      "Epoch [707/2000], Evaluation Loss: 23.2504, Evaluation BLEU: 0.0000\n",
      "Epoch [708/2000], Evaluation Loss: 23.2509, Evaluation BLEU: 0.0000\n",
      "Epoch [709/2000], Evaluation Loss: 23.2514, Evaluation BLEU: 0.0000\n",
      "Epoch [710/2000], Evaluation Loss: 23.2518, Evaluation BLEU: 0.0000\n",
      "Epoch [711/2000], Evaluation Loss: 23.2523, Evaluation BLEU: 0.0000\n",
      "Epoch [712/2000], Evaluation Loss: 23.2527, Evaluation BLEU: 0.0000\n",
      "Epoch [713/2000], Evaluation Loss: 23.2532, Evaluation BLEU: 0.0000\n",
      "Epoch [714/2000], Evaluation Loss: 23.2536, Evaluation BLEU: 0.0000\n",
      "Epoch [715/2000], Evaluation Loss: 23.2541, Evaluation BLEU: 0.0000\n",
      "Epoch [716/2000], Evaluation Loss: 23.2546, Evaluation BLEU: 0.0000\n",
      "Epoch [717/2000], Evaluation Loss: 23.2550, Evaluation BLEU: 0.0000\n",
      "Epoch [718/2000], Evaluation Loss: 23.2555, Evaluation BLEU: 0.0000\n",
      "Epoch [719/2000], Evaluation Loss: 23.2559, Evaluation BLEU: 0.0000\n",
      "Epoch [720/2000], Evaluation Loss: 23.2563, Evaluation BLEU: 0.0000\n",
      "Epoch [721/2000], Evaluation Loss: 23.2568, Evaluation BLEU: 0.0000\n",
      "Epoch [722/2000], Evaluation Loss: 23.2572, Evaluation BLEU: 0.0000\n",
      "Epoch [723/2000], Evaluation Loss: 23.2575, Evaluation BLEU: 0.0000\n",
      "Epoch [724/2000], Evaluation Loss: 23.2579, Evaluation BLEU: 0.0000\n",
      "Epoch [725/2000], Evaluation Loss: 23.2583, Evaluation BLEU: 0.0000\n",
      "Epoch [726/2000], Evaluation Loss: 23.2586, Evaluation BLEU: 0.0000\n",
      "Epoch [727/2000], Evaluation Loss: 23.2590, Evaluation BLEU: 0.0000\n",
      "Epoch [728/2000], Evaluation Loss: 23.2593, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [729/2000], Evaluation Loss: 23.2597, Evaluation BLEU: 0.0000\n",
      "Epoch [730/2000], Evaluation Loss: 23.2600, Evaluation BLEU: 0.0000\n",
      "Epoch [731/2000], Evaluation Loss: 23.2603, Evaluation BLEU: 0.0000\n",
      "Epoch [732/2000], Evaluation Loss: 23.2607, Evaluation BLEU: 0.0000\n",
      "Epoch [733/2000], Evaluation Loss: 23.2610, Evaluation BLEU: 0.0000\n",
      "Epoch [734/2000], Evaluation Loss: 23.2613, Evaluation BLEU: 0.0000\n",
      "Epoch [735/2000], Evaluation Loss: 23.2616, Evaluation BLEU: 0.0000\n",
      "Epoch [736/2000], Evaluation Loss: 23.2620, Evaluation BLEU: 0.0000\n",
      "Epoch [737/2000], Evaluation Loss: 23.2623, Evaluation BLEU: 0.0000\n",
      "Epoch [738/2000], Evaluation Loss: 23.2627, Evaluation BLEU: 0.0000\n",
      "Epoch [739/2000], Evaluation Loss: 23.2630, Evaluation BLEU: 0.0000\n",
      "Epoch [740/2000], Evaluation Loss: 23.2633, Evaluation BLEU: 0.0000\n",
      "Epoch [741/2000], Evaluation Loss: 23.2637, Evaluation BLEU: 0.0000\n",
      "Epoch [742/2000], Evaluation Loss: 23.2640, Evaluation BLEU: 0.0000\n",
      "Epoch [743/2000], Evaluation Loss: 23.2644, Evaluation BLEU: 0.0000\n",
      "Epoch [744/2000], Evaluation Loss: 23.2648, Evaluation BLEU: 0.0000\n",
      "Epoch [745/2000], Evaluation Loss: 23.2651, Evaluation BLEU: 0.0000\n",
      "Epoch [746/2000], Evaluation Loss: 23.2654, Evaluation BLEU: 0.0000\n",
      "Epoch [747/2000], Evaluation Loss: 23.2657, Evaluation BLEU: 0.0000\n",
      "Epoch [748/2000], Evaluation Loss: 23.2660, Evaluation BLEU: 0.0000\n",
      "Epoch [749/2000], Evaluation Loss: 23.2664, Evaluation BLEU: 0.0000\n",
      "Epoch [750/2000], Evaluation Loss: 23.2667, Evaluation BLEU: 0.0000\n",
      "Epoch [751/2000], Evaluation Loss: 23.2670, Evaluation BLEU: 0.0000\n",
      "Epoch [752/2000], Evaluation Loss: 23.2673, Evaluation BLEU: 0.0000\n",
      "Epoch [753/2000], Evaluation Loss: 23.2677, Evaluation BLEU: 0.0000\n",
      "Epoch [754/2000], Evaluation Loss: 23.2680, Evaluation BLEU: 0.0000\n",
      "Epoch [755/2000], Evaluation Loss: 23.2684, Evaluation BLEU: 0.0000\n",
      "Epoch [756/2000], Evaluation Loss: 23.2688, Evaluation BLEU: 0.0000\n",
      "Epoch [757/2000], Evaluation Loss: 23.2693, Evaluation BLEU: 0.0000\n",
      "Epoch [758/2000], Evaluation Loss: 23.2697, Evaluation BLEU: 0.0000\n",
      "Epoch [759/2000], Evaluation Loss: 23.2701, Evaluation BLEU: 0.0000\n",
      "Epoch [760/2000], Evaluation Loss: 23.2705, Evaluation BLEU: 0.0000\n",
      "Epoch [761/2000], Evaluation Loss: 23.2709, Evaluation BLEU: 0.0000\n",
      "Epoch [762/2000], Evaluation Loss: 23.2714, Evaluation BLEU: 0.0000\n",
      "Epoch [763/2000], Evaluation Loss: 23.2719, Evaluation BLEU: 0.0000\n",
      "Epoch [764/2000], Evaluation Loss: 23.2723, Evaluation BLEU: 0.0000\n",
      "Epoch [765/2000], Evaluation Loss: 23.2728, Evaluation BLEU: 0.0000\n",
      "Epoch [766/2000], Evaluation Loss: 23.2733, Evaluation BLEU: 0.0000\n",
      "Epoch [767/2000], Evaluation Loss: 23.2738, Evaluation BLEU: 0.0000\n",
      "Epoch [768/2000], Evaluation Loss: 23.2743, Evaluation BLEU: 0.0000\n",
      "Epoch [769/2000], Evaluation Loss: 23.2748, Evaluation BLEU: 0.0000\n",
      "Epoch [770/2000], Evaluation Loss: 23.2753, Evaluation BLEU: 0.0000\n",
      "Epoch [771/2000], Evaluation Loss: 23.2759, Evaluation BLEU: 0.0000\n",
      "Epoch [772/2000], Evaluation Loss: 23.2764, Evaluation BLEU: 0.0000\n",
      "Epoch [773/2000], Evaluation Loss: 23.2769, Evaluation BLEU: 0.0000\n",
      "Epoch [774/2000], Evaluation Loss: 23.2774, Evaluation BLEU: 0.0000\n",
      "Epoch [775/2000], Evaluation Loss: 23.2779, Evaluation BLEU: 0.0000\n",
      "Epoch [776/2000], Evaluation Loss: 23.2784, Evaluation BLEU: 0.0000\n",
      "Epoch [777/2000], Evaluation Loss: 23.2789, Evaluation BLEU: 0.0000\n",
      "Epoch [778/2000], Evaluation Loss: 23.2794, Evaluation BLEU: 0.0000\n",
      "Epoch [779/2000], Evaluation Loss: 23.2799, Evaluation BLEU: 0.0000\n",
      "Epoch [780/2000], Evaluation Loss: 23.2804, Evaluation BLEU: 0.0000\n",
      "Epoch [781/2000], Evaluation Loss: 23.2809, Evaluation BLEU: 0.0000\n",
      "Epoch [782/2000], Evaluation Loss: 23.2813, Evaluation BLEU: 0.0000\n",
      "Epoch [783/2000], Evaluation Loss: 23.2818, Evaluation BLEU: 0.0000\n",
      "Epoch [784/2000], Evaluation Loss: 23.2822, Evaluation BLEU: 0.0000\n",
      "Epoch [785/2000], Evaluation Loss: 23.2827, Evaluation BLEU: 0.0000\n",
      "Epoch [786/2000], Evaluation Loss: 23.2831, Evaluation BLEU: 0.0000\n",
      "Epoch [787/2000], Evaluation Loss: 23.2835, Evaluation BLEU: 0.0000\n",
      "Epoch [788/2000], Evaluation Loss: 23.2839, Evaluation BLEU: 0.0000\n",
      "Epoch [789/2000], Evaluation Loss: 23.2843, Evaluation BLEU: 0.0000\n",
      "Epoch [790/2000], Evaluation Loss: 23.2847, Evaluation BLEU: 0.0000\n",
      "Epoch [791/2000], Evaluation Loss: 23.2850, Evaluation BLEU: 0.0000\n",
      "Epoch [792/2000], Evaluation Loss: 23.2854, Evaluation BLEU: 0.0000\n",
      "Epoch [793/2000], Evaluation Loss: 23.2858, Evaluation BLEU: 0.0000\n",
      "Epoch [794/2000], Evaluation Loss: 23.2862, Evaluation BLEU: 0.0000\n",
      "Epoch [795/2000], Evaluation Loss: 23.2865, Evaluation BLEU: 0.0000\n",
      "Epoch [796/2000], Evaluation Loss: 23.2869, Evaluation BLEU: 0.0000\n",
      "Epoch [797/2000], Evaluation Loss: 23.2873, Evaluation BLEU: 0.0000\n",
      "Epoch [798/2000], Evaluation Loss: 23.2876, Evaluation BLEU: 0.0000\n",
      "Epoch [799/2000], Evaluation Loss: 23.2879, Evaluation BLEU: 0.0000\n",
      "Epoch [800/2000], Evaluation Loss: 23.2882, Evaluation BLEU: 0.0000\n",
      "Epoch [801/2000], Evaluation Loss: 23.2885, Evaluation BLEU: 0.0000\n",
      "Epoch [802/2000], Evaluation Loss: 23.2888, Evaluation BLEU: 0.0000\n",
      "Epoch [803/2000], Evaluation Loss: 23.2891, Evaluation BLEU: 0.0000\n",
      "Epoch [804/2000], Evaluation Loss: 23.2894, Evaluation BLEU: 0.0000\n",
      "Epoch [805/2000], Evaluation Loss: 23.2897, Evaluation BLEU: 0.0000\n",
      "Epoch [806/2000], Evaluation Loss: 23.2900, Evaluation BLEU: 0.0000\n",
      "Epoch [807/2000], Evaluation Loss: 23.2902, Evaluation BLEU: 0.0000\n",
      "Epoch [808/2000], Evaluation Loss: 23.2905, Evaluation BLEU: 0.0000\n",
      "Epoch [809/2000], Evaluation Loss: 23.2908, Evaluation BLEU: 0.0000\n",
      "Epoch [810/2000], Evaluation Loss: 23.2911, Evaluation BLEU: 0.0000\n",
      "Epoch [811/2000], Evaluation Loss: 23.2914, Evaluation BLEU: 0.0000\n",
      "Epoch [812/2000], Evaluation Loss: 23.2917, Evaluation BLEU: 0.0000\n",
      "Epoch [813/2000], Evaluation Loss: 23.2920, Evaluation BLEU: 0.0000\n",
      "Epoch [814/2000], Evaluation Loss: 23.2922, Evaluation BLEU: 0.0000\n",
      "Epoch [815/2000], Evaluation Loss: 23.2925, Evaluation BLEU: 0.0000\n",
      "Epoch [816/2000], Evaluation Loss: 23.2927, Evaluation BLEU: 0.0000\n",
      "Epoch [817/2000], Evaluation Loss: 23.2930, Evaluation BLEU: 0.0000\n",
      "Epoch [818/2000], Evaluation Loss: 23.2932, Evaluation BLEU: 0.0000\n",
      "Epoch [819/2000], Evaluation Loss: 23.2935, Evaluation BLEU: 0.0000\n",
      "Epoch [820/2000], Evaluation Loss: 23.2938, Evaluation BLEU: 0.0000\n",
      "Epoch [821/2000], Evaluation Loss: 23.2940, Evaluation BLEU: 0.0000\n",
      "Epoch [822/2000], Evaluation Loss: 23.2943, Evaluation BLEU: 0.0000\n",
      "Epoch [823/2000], Evaluation Loss: 23.2946, Evaluation BLEU: 0.0000\n",
      "Epoch [824/2000], Evaluation Loss: 23.2948, Evaluation BLEU: 0.0000\n",
      "Epoch [825/2000], Evaluation Loss: 23.2951, Evaluation BLEU: 0.0000\n",
      "Epoch [826/2000], Evaluation Loss: 23.2953, Evaluation BLEU: 0.0000\n",
      "Epoch [827/2000], Evaluation Loss: 23.2955, Evaluation BLEU: 0.0000\n",
      "Epoch [828/2000], Evaluation Loss: 23.2956, Evaluation BLEU: 0.0000\n",
      "Epoch [829/2000], Evaluation Loss: 23.2958, Evaluation BLEU: 0.0000\n",
      "Epoch [830/2000], Evaluation Loss: 23.2960, Evaluation BLEU: 0.0000\n",
      "Epoch [831/2000], Evaluation Loss: 23.2962, Evaluation BLEU: 0.0000\n",
      "Epoch [832/2000], Evaluation Loss: 23.2964, Evaluation BLEU: 0.0000\n",
      "Epoch [833/2000], Evaluation Loss: 23.2966, Evaluation BLEU: 0.0000\n",
      "Epoch [834/2000], Evaluation Loss: 23.2967, Evaluation BLEU: 0.0000\n",
      "Epoch [835/2000], Evaluation Loss: 23.2969, Evaluation BLEU: 0.0000\n",
      "Epoch [836/2000], Evaluation Loss: 23.2971, Evaluation BLEU: 0.0000\n",
      "Epoch [837/2000], Evaluation Loss: 23.2972, Evaluation BLEU: 0.0000\n",
      "Epoch [838/2000], Evaluation Loss: 23.2974, Evaluation BLEU: 0.0000\n",
      "Epoch [839/2000], Evaluation Loss: 23.2975, Evaluation BLEU: 0.0000\n",
      "Epoch [840/2000], Evaluation Loss: 23.2977, Evaluation BLEU: 0.0000\n",
      "Epoch [841/2000], Evaluation Loss: 23.2979, Evaluation BLEU: 0.0000\n",
      "Epoch [842/2000], Evaluation Loss: 23.2981, Evaluation BLEU: 0.0000\n",
      "Epoch [843/2000], Evaluation Loss: 23.2982, Evaluation BLEU: 0.0000\n",
      "Epoch [844/2000], Evaluation Loss: 23.2984, Evaluation BLEU: 0.0000\n",
      "Epoch [845/2000], Evaluation Loss: 23.2986, Evaluation BLEU: 0.0000\n",
      "Epoch [846/2000], Evaluation Loss: 23.2988, Evaluation BLEU: 0.0000\n",
      "Epoch [847/2000], Evaluation Loss: 23.2990, Evaluation BLEU: 0.0000\n",
      "Epoch [848/2000], Evaluation Loss: 23.2992, Evaluation BLEU: 0.0000\n",
      "Epoch [849/2000], Evaluation Loss: 23.2994, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [850/2000], Evaluation Loss: 23.2996, Evaluation BLEU: 0.0000\n",
      "Epoch [851/2000], Evaluation Loss: 23.2999, Evaluation BLEU: 0.0000\n",
      "Epoch [852/2000], Evaluation Loss: 23.3001, Evaluation BLEU: 0.0000\n",
      "Epoch [853/2000], Evaluation Loss: 23.3003, Evaluation BLEU: 0.0000\n",
      "Epoch [854/2000], Evaluation Loss: 23.3005, Evaluation BLEU: 0.0000\n",
      "Epoch [855/2000], Evaluation Loss: 23.3006, Evaluation BLEU: 0.0000\n",
      "Epoch [856/2000], Evaluation Loss: 23.3008, Evaluation BLEU: 0.0000\n",
      "Epoch [857/2000], Evaluation Loss: 23.3010, Evaluation BLEU: 0.0000\n",
      "Epoch [858/2000], Evaluation Loss: 23.3012, Evaluation BLEU: 0.0000\n",
      "Epoch [859/2000], Evaluation Loss: 23.3013, Evaluation BLEU: 0.0000\n",
      "Epoch [860/2000], Evaluation Loss: 23.3014, Evaluation BLEU: 0.0000\n",
      "Epoch [861/2000], Evaluation Loss: 23.3016, Evaluation BLEU: 0.0000\n",
      "Epoch [862/2000], Evaluation Loss: 23.3017, Evaluation BLEU: 0.0000\n",
      "Epoch [863/2000], Evaluation Loss: 23.3019, Evaluation BLEU: 0.0000\n",
      "Epoch [864/2000], Evaluation Loss: 23.3020, Evaluation BLEU: 0.0000\n",
      "Epoch [865/2000], Evaluation Loss: 23.3022, Evaluation BLEU: 0.0000\n",
      "Epoch [866/2000], Evaluation Loss: 23.3024, Evaluation BLEU: 0.0000\n",
      "Epoch [867/2000], Evaluation Loss: 23.3026, Evaluation BLEU: 0.0000\n",
      "Epoch [868/2000], Evaluation Loss: 23.3028, Evaluation BLEU: 0.0000\n",
      "Epoch [869/2000], Evaluation Loss: 23.3030, Evaluation BLEU: 0.0000\n",
      "Epoch [870/2000], Evaluation Loss: 23.3032, Evaluation BLEU: 0.0000\n",
      "Epoch [871/2000], Evaluation Loss: 23.3034, Evaluation BLEU: 0.0000\n",
      "Epoch [872/2000], Evaluation Loss: 23.3035, Evaluation BLEU: 0.0000\n",
      "Epoch [873/2000], Evaluation Loss: 23.3037, Evaluation BLEU: 0.0000\n",
      "Epoch [874/2000], Evaluation Loss: 23.3039, Evaluation BLEU: 0.0000\n",
      "Epoch [875/2000], Evaluation Loss: 23.3041, Evaluation BLEU: 0.0000\n",
      "Epoch [876/2000], Evaluation Loss: 23.3043, Evaluation BLEU: 0.0000\n",
      "Epoch [877/2000], Evaluation Loss: 23.3045, Evaluation BLEU: 0.0000\n",
      "Epoch [878/2000], Evaluation Loss: 23.3048, Evaluation BLEU: 0.0000\n",
      "Epoch [879/2000], Evaluation Loss: 23.3050, Evaluation BLEU: 0.0000\n",
      "Epoch [880/2000], Evaluation Loss: 23.3052, Evaluation BLEU: 0.0000\n",
      "Epoch [881/2000], Evaluation Loss: 23.3055, Evaluation BLEU: 0.0000\n",
      "Epoch [882/2000], Evaluation Loss: 23.3057, Evaluation BLEU: 0.0000\n",
      "Epoch [883/2000], Evaluation Loss: 23.3059, Evaluation BLEU: 0.0000\n",
      "Epoch [884/2000], Evaluation Loss: 23.3062, Evaluation BLEU: 0.0000\n",
      "Epoch [885/2000], Evaluation Loss: 23.3064, Evaluation BLEU: 0.0000\n",
      "Epoch [886/2000], Evaluation Loss: 23.3066, Evaluation BLEU: 0.0000\n",
      "Epoch [887/2000], Evaluation Loss: 23.3068, Evaluation BLEU: 0.0000\n",
      "Epoch [888/2000], Evaluation Loss: 23.3070, Evaluation BLEU: 0.0000\n",
      "Epoch [889/2000], Evaluation Loss: 23.3073, Evaluation BLEU: 0.0000\n",
      "Epoch [890/2000], Evaluation Loss: 23.3075, Evaluation BLEU: 0.0000\n",
      "Epoch [891/2000], Evaluation Loss: 23.3077, Evaluation BLEU: 0.0000\n",
      "Epoch [892/2000], Evaluation Loss: 23.3080, Evaluation BLEU: 0.0000\n",
      "Epoch [893/2000], Evaluation Loss: 23.3082, Evaluation BLEU: 0.0000\n",
      "Epoch [894/2000], Evaluation Loss: 23.3085, Evaluation BLEU: 0.0000\n",
      "Epoch [895/2000], Evaluation Loss: 23.3087, Evaluation BLEU: 0.0000\n",
      "Epoch [896/2000], Evaluation Loss: 23.3090, Evaluation BLEU: 0.0000\n",
      "Epoch [897/2000], Evaluation Loss: 23.3092, Evaluation BLEU: 0.0000\n",
      "Epoch [898/2000], Evaluation Loss: 23.3095, Evaluation BLEU: 0.0000\n",
      "Epoch [899/2000], Evaluation Loss: 23.3098, Evaluation BLEU: 0.0000\n",
      "Epoch [900/2000], Evaluation Loss: 23.3101, Evaluation BLEU: 0.0000\n",
      "Epoch [901/2000], Evaluation Loss: 23.3104, Evaluation BLEU: 0.0000\n",
      "Epoch [902/2000], Evaluation Loss: 23.3107, Evaluation BLEU: 0.0000\n",
      "Epoch [903/2000], Evaluation Loss: 23.3110, Evaluation BLEU: 0.0000\n",
      "Epoch [904/2000], Evaluation Loss: 23.3113, Evaluation BLEU: 0.0000\n",
      "Epoch [905/2000], Evaluation Loss: 23.3116, Evaluation BLEU: 0.0000\n",
      "Epoch [906/2000], Evaluation Loss: 23.3119, Evaluation BLEU: 0.0000\n",
      "Epoch [907/2000], Evaluation Loss: 23.3123, Evaluation BLEU: 0.0000\n",
      "Epoch [908/2000], Evaluation Loss: 23.3126, Evaluation BLEU: 0.0000\n",
      "Epoch [909/2000], Evaluation Loss: 23.3129, Evaluation BLEU: 0.0000\n",
      "Epoch [910/2000], Evaluation Loss: 23.3132, Evaluation BLEU: 0.0000\n",
      "Epoch [911/2000], Evaluation Loss: 23.3135, Evaluation BLEU: 0.0000\n",
      "Epoch [912/2000], Evaluation Loss: 23.3138, Evaluation BLEU: 0.0000\n",
      "Epoch [913/2000], Evaluation Loss: 23.3141, Evaluation BLEU: 0.0000\n",
      "Epoch [914/2000], Evaluation Loss: 23.3144, Evaluation BLEU: 0.0000\n",
      "Epoch [915/2000], Evaluation Loss: 23.3147, Evaluation BLEU: 0.0000\n",
      "Epoch [916/2000], Evaluation Loss: 23.3150, Evaluation BLEU: 0.0000\n",
      "Epoch [917/2000], Evaluation Loss: 23.3153, Evaluation BLEU: 0.0000\n",
      "Epoch [918/2000], Evaluation Loss: 23.3157, Evaluation BLEU: 0.0000\n",
      "Epoch [919/2000], Evaluation Loss: 23.3160, Evaluation BLEU: 0.0000\n",
      "Epoch [920/2000], Evaluation Loss: 23.3163, Evaluation BLEU: 0.0000\n",
      "Epoch [921/2000], Evaluation Loss: 23.3167, Evaluation BLEU: 0.0000\n",
      "Epoch [922/2000], Evaluation Loss: 23.3170, Evaluation BLEU: 0.0000\n",
      "Epoch [923/2000], Evaluation Loss: 23.3174, Evaluation BLEU: 0.0000\n",
      "Epoch [924/2000], Evaluation Loss: 23.3178, Evaluation BLEU: 0.0000\n",
      "Epoch [925/2000], Evaluation Loss: 23.3181, Evaluation BLEU: 0.0000\n",
      "Epoch [926/2000], Evaluation Loss: 23.3185, Evaluation BLEU: 0.0000\n",
      "Epoch [927/2000], Evaluation Loss: 23.3189, Evaluation BLEU: 0.0000\n",
      "Epoch [928/2000], Evaluation Loss: 23.3193, Evaluation BLEU: 0.0000\n",
      "Epoch [929/2000], Evaluation Loss: 23.3196, Evaluation BLEU: 0.0000\n",
      "Epoch [930/2000], Evaluation Loss: 23.3200, Evaluation BLEU: 0.0000\n",
      "Epoch [931/2000], Evaluation Loss: 23.3204, Evaluation BLEU: 0.0000\n",
      "Epoch [932/2000], Evaluation Loss: 23.3208, Evaluation BLEU: 0.0000\n",
      "Epoch [933/2000], Evaluation Loss: 23.3211, Evaluation BLEU: 0.0000\n",
      "Epoch [934/2000], Evaluation Loss: 23.3214, Evaluation BLEU: 0.0000\n",
      "Epoch [935/2000], Evaluation Loss: 23.3217, Evaluation BLEU: 0.0000\n",
      "Epoch [936/2000], Evaluation Loss: 23.3221, Evaluation BLEU: 0.0000\n",
      "Epoch [937/2000], Evaluation Loss: 23.3224, Evaluation BLEU: 0.0000\n",
      "Epoch [938/2000], Evaluation Loss: 23.3227, Evaluation BLEU: 0.0000\n",
      "Epoch [939/2000], Evaluation Loss: 23.3230, Evaluation BLEU: 0.0000\n",
      "Epoch [940/2000], Evaluation Loss: 23.3233, Evaluation BLEU: 0.0000\n",
      "Epoch [941/2000], Evaluation Loss: 23.3236, Evaluation BLEU: 0.0000\n",
      "Epoch [942/2000], Evaluation Loss: 23.3240, Evaluation BLEU: 0.0000\n",
      "Epoch [943/2000], Evaluation Loss: 23.3243, Evaluation BLEU: 0.0000\n",
      "Epoch [944/2000], Evaluation Loss: 23.3246, Evaluation BLEU: 0.0000\n",
      "Epoch [945/2000], Evaluation Loss: 23.3248, Evaluation BLEU: 0.0000\n",
      "Epoch [946/2000], Evaluation Loss: 23.3251, Evaluation BLEU: 0.0000\n",
      "Epoch [947/2000], Evaluation Loss: 23.3255, Evaluation BLEU: 0.0000\n",
      "Epoch [948/2000], Evaluation Loss: 23.3258, Evaluation BLEU: 0.0000\n",
      "Epoch [949/2000], Evaluation Loss: 23.3261, Evaluation BLEU: 0.0000\n",
      "Epoch [950/2000], Evaluation Loss: 23.3264, Evaluation BLEU: 0.0000\n",
      "Epoch [951/2000], Evaluation Loss: 23.3267, Evaluation BLEU: 0.0000\n",
      "Epoch [952/2000], Evaluation Loss: 23.3271, Evaluation BLEU: 0.0000\n",
      "Epoch [953/2000], Evaluation Loss: 23.3274, Evaluation BLEU: 0.0000\n",
      "Epoch [954/2000], Evaluation Loss: 23.3277, Evaluation BLEU: 0.0000\n",
      "Epoch [955/2000], Evaluation Loss: 23.3280, Evaluation BLEU: 0.0000\n",
      "Epoch [956/2000], Evaluation Loss: 23.3283, Evaluation BLEU: 0.0000\n",
      "Epoch [957/2000], Evaluation Loss: 23.3286, Evaluation BLEU: 0.0000\n",
      "Epoch [958/2000], Evaluation Loss: 23.3289, Evaluation BLEU: 0.0000\n",
      "Epoch [959/2000], Evaluation Loss: 23.3292, Evaluation BLEU: 0.0000\n",
      "Epoch [960/2000], Evaluation Loss: 23.3295, Evaluation BLEU: 0.0000\n",
      "Epoch [961/2000], Evaluation Loss: 23.3298, Evaluation BLEU: 0.0000\n",
      "Epoch [962/2000], Evaluation Loss: 23.3300, Evaluation BLEU: 0.0000\n",
      "Epoch [963/2000], Evaluation Loss: 23.3302, Evaluation BLEU: 0.0000\n",
      "Epoch [964/2000], Evaluation Loss: 23.3304, Evaluation BLEU: 0.0000\n",
      "Epoch [965/2000], Evaluation Loss: 23.3307, Evaluation BLEU: 0.0000\n",
      "Epoch [966/2000], Evaluation Loss: 23.3309, Evaluation BLEU: 0.0000\n",
      "Epoch [967/2000], Evaluation Loss: 23.3311, Evaluation BLEU: 0.0000\n",
      "Epoch [968/2000], Evaluation Loss: 23.3312, Evaluation BLEU: 0.0000\n",
      "Epoch [969/2000], Evaluation Loss: 23.3314, Evaluation BLEU: 0.0000\n",
      "Epoch [970/2000], Evaluation Loss: 23.3315, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [971/2000], Evaluation Loss: 23.3317, Evaluation BLEU: 0.0000\n",
      "Epoch [972/2000], Evaluation Loss: 23.3319, Evaluation BLEU: 0.0000\n",
      "Epoch [973/2000], Evaluation Loss: 23.3320, Evaluation BLEU: 0.0000\n",
      "Epoch [974/2000], Evaluation Loss: 23.3321, Evaluation BLEU: 0.0000\n",
      "Epoch [975/2000], Evaluation Loss: 23.3323, Evaluation BLEU: 0.0000\n",
      "Epoch [976/2000], Evaluation Loss: 23.3324, Evaluation BLEU: 0.0000\n",
      "Epoch [977/2000], Evaluation Loss: 23.3326, Evaluation BLEU: 0.0000\n",
      "Epoch [978/2000], Evaluation Loss: 23.3327, Evaluation BLEU: 0.0000\n",
      "Epoch [979/2000], Evaluation Loss: 23.3328, Evaluation BLEU: 0.0000\n",
      "Epoch [980/2000], Evaluation Loss: 23.3330, Evaluation BLEU: 0.0000\n",
      "Epoch [981/2000], Evaluation Loss: 23.3331, Evaluation BLEU: 0.0000\n",
      "Epoch [982/2000], Evaluation Loss: 23.3332, Evaluation BLEU: 0.0000\n",
      "Epoch [983/2000], Evaluation Loss: 23.3334, Evaluation BLEU: 0.0000\n",
      "Epoch [984/2000], Evaluation Loss: 23.3335, Evaluation BLEU: 0.0000\n",
      "Epoch [985/2000], Evaluation Loss: 23.3337, Evaluation BLEU: 0.0000\n",
      "Epoch [986/2000], Evaluation Loss: 23.3339, Evaluation BLEU: 0.0000\n",
      "Epoch [987/2000], Evaluation Loss: 23.3341, Evaluation BLEU: 0.0000\n",
      "Epoch [988/2000], Evaluation Loss: 23.3343, Evaluation BLEU: 0.0000\n",
      "Epoch [989/2000], Evaluation Loss: 23.3346, Evaluation BLEU: 0.0000\n",
      "Epoch [990/2000], Evaluation Loss: 23.3349, Evaluation BLEU: 0.0000\n",
      "Epoch [991/2000], Evaluation Loss: 23.3351, Evaluation BLEU: 0.0000\n",
      "Epoch [992/2000], Evaluation Loss: 23.3354, Evaluation BLEU: 0.0000\n",
      "Epoch [993/2000], Evaluation Loss: 23.3357, Evaluation BLEU: 0.0000\n",
      "Epoch [994/2000], Evaluation Loss: 23.3361, Evaluation BLEU: 0.0000\n",
      "Epoch [995/2000], Evaluation Loss: 23.3365, Evaluation BLEU: 0.0000\n",
      "Epoch [996/2000], Evaluation Loss: 23.3368, Evaluation BLEU: 0.0000\n",
      "Epoch [997/2000], Evaluation Loss: 23.3372, Evaluation BLEU: 0.0000\n",
      "Epoch [998/2000], Evaluation Loss: 23.3375, Evaluation BLEU: 0.0000\n",
      "Epoch [999/2000], Evaluation Loss: 23.3378, Evaluation BLEU: 0.0000\n",
      "Epoch [1000/2000], Evaluation Loss: 23.3381, Evaluation BLEU: 0.0000\n",
      "Epoch [1001/2000], Evaluation Loss: 23.3384, Evaluation BLEU: 0.0000\n",
      "Epoch [1002/2000], Evaluation Loss: 23.3388, Evaluation BLEU: 0.0000\n",
      "Epoch [1003/2000], Evaluation Loss: 23.3391, Evaluation BLEU: 0.0000\n",
      "Epoch [1004/2000], Evaluation Loss: 23.3395, Evaluation BLEU: 0.0000\n",
      "Epoch [1005/2000], Evaluation Loss: 23.3398, Evaluation BLEU: 0.0000\n",
      "Epoch [1006/2000], Evaluation Loss: 23.3402, Evaluation BLEU: 0.0000\n",
      "Epoch [1007/2000], Evaluation Loss: 23.3405, Evaluation BLEU: 0.0000\n",
      "Epoch [1008/2000], Evaluation Loss: 23.3409, Evaluation BLEU: 0.0000\n",
      "Epoch [1009/2000], Evaluation Loss: 23.3412, Evaluation BLEU: 0.0000\n",
      "Epoch [1010/2000], Evaluation Loss: 23.3416, Evaluation BLEU: 0.0000\n",
      "Epoch [1011/2000], Evaluation Loss: 23.3420, Evaluation BLEU: 0.0000\n",
      "Epoch [1012/2000], Evaluation Loss: 23.3424, Evaluation BLEU: 0.0000\n",
      "Epoch [1013/2000], Evaluation Loss: 23.3427, Evaluation BLEU: 0.0000\n",
      "Epoch [1014/2000], Evaluation Loss: 23.3431, Evaluation BLEU: 0.0000\n",
      "Epoch [1015/2000], Evaluation Loss: 23.3435, Evaluation BLEU: 0.0000\n",
      "Epoch [1016/2000], Evaluation Loss: 23.3439, Evaluation BLEU: 0.0000\n",
      "Epoch [1017/2000], Evaluation Loss: 23.3443, Evaluation BLEU: 0.0000\n",
      "Epoch [1018/2000], Evaluation Loss: 23.3447, Evaluation BLEU: 0.0000\n",
      "Epoch [1019/2000], Evaluation Loss: 23.3451, Evaluation BLEU: 0.0000\n",
      "Epoch [1020/2000], Evaluation Loss: 23.3455, Evaluation BLEU: 0.0000\n",
      "Epoch [1021/2000], Evaluation Loss: 23.3458, Evaluation BLEU: 0.0000\n",
      "Epoch [1022/2000], Evaluation Loss: 23.3462, Evaluation BLEU: 0.0000\n",
      "Epoch [1023/2000], Evaluation Loss: 23.3466, Evaluation BLEU: 0.0000\n",
      "Epoch [1024/2000], Evaluation Loss: 23.3469, Evaluation BLEU: 0.0000\n",
      "Epoch [1025/2000], Evaluation Loss: 23.3472, Evaluation BLEU: 0.0000\n",
      "Epoch [1026/2000], Evaluation Loss: 23.3476, Evaluation BLEU: 0.0000\n",
      "Epoch [1027/2000], Evaluation Loss: 23.3479, Evaluation BLEU: 0.0000\n",
      "Epoch [1028/2000], Evaluation Loss: 23.3482, Evaluation BLEU: 0.0000\n",
      "Epoch [1029/2000], Evaluation Loss: 23.3485, Evaluation BLEU: 0.0000\n",
      "Epoch [1030/2000], Evaluation Loss: 23.3488, Evaluation BLEU: 0.0000\n",
      "Epoch [1031/2000], Evaluation Loss: 23.3491, Evaluation BLEU: 0.0000\n",
      "Epoch [1032/2000], Evaluation Loss: 23.3494, Evaluation BLEU: 0.0000\n",
      "Epoch [1033/2000], Evaluation Loss: 23.3497, Evaluation BLEU: 0.0000\n",
      "Epoch [1034/2000], Evaluation Loss: 23.3501, Evaluation BLEU: 0.0000\n",
      "Epoch [1035/2000], Evaluation Loss: 23.3504, Evaluation BLEU: 0.0000\n",
      "Epoch [1036/2000], Evaluation Loss: 23.3507, Evaluation BLEU: 0.0000\n",
      "Epoch [1037/2000], Evaluation Loss: 23.3510, Evaluation BLEU: 0.0000\n",
      "Epoch [1038/2000], Evaluation Loss: 23.3514, Evaluation BLEU: 0.0000\n",
      "Epoch [1039/2000], Evaluation Loss: 23.3517, Evaluation BLEU: 0.0000\n",
      "Epoch [1040/2000], Evaluation Loss: 23.3521, Evaluation BLEU: 0.0000\n",
      "Epoch [1041/2000], Evaluation Loss: 23.3524, Evaluation BLEU: 0.0000\n",
      "Epoch [1042/2000], Evaluation Loss: 23.3528, Evaluation BLEU: 0.0000\n",
      "Epoch [1043/2000], Evaluation Loss: 23.3531, Evaluation BLEU: 0.0000\n",
      "Epoch [1044/2000], Evaluation Loss: 23.3535, Evaluation BLEU: 0.0000\n",
      "Epoch [1045/2000], Evaluation Loss: 23.3539, Evaluation BLEU: 0.0000\n",
      "Epoch [1046/2000], Evaluation Loss: 23.3542, Evaluation BLEU: 0.0000\n",
      "Epoch [1047/2000], Evaluation Loss: 23.3545, Evaluation BLEU: 0.0000\n",
      "Epoch [1048/2000], Evaluation Loss: 23.3549, Evaluation BLEU: 0.0000\n",
      "Epoch [1049/2000], Evaluation Loss: 23.3553, Evaluation BLEU: 0.0000\n",
      "Epoch [1050/2000], Evaluation Loss: 23.3556, Evaluation BLEU: 0.0000\n",
      "Epoch [1051/2000], Evaluation Loss: 23.3560, Evaluation BLEU: 0.0000\n",
      "Epoch [1052/2000], Evaluation Loss: 23.3564, Evaluation BLEU: 0.0000\n",
      "Epoch [1053/2000], Evaluation Loss: 23.3567, Evaluation BLEU: 0.0000\n",
      "Epoch [1054/2000], Evaluation Loss: 23.3571, Evaluation BLEU: 0.0000\n",
      "Epoch [1055/2000], Evaluation Loss: 23.3575, Evaluation BLEU: 0.0000\n",
      "Epoch [1056/2000], Evaluation Loss: 23.3578, Evaluation BLEU: 0.0000\n",
      "Epoch [1057/2000], Evaluation Loss: 23.3582, Evaluation BLEU: 0.0000\n",
      "Epoch [1058/2000], Evaluation Loss: 23.3586, Evaluation BLEU: 0.0000\n",
      "Epoch [1059/2000], Evaluation Loss: 23.3589, Evaluation BLEU: 0.0000\n",
      "Epoch [1060/2000], Evaluation Loss: 23.3593, Evaluation BLEU: 0.0000\n",
      "Epoch [1061/2000], Evaluation Loss: 23.3597, Evaluation BLEU: 0.0000\n",
      "Epoch [1062/2000], Evaluation Loss: 23.3600, Evaluation BLEU: 0.0000\n",
      "Epoch [1063/2000], Evaluation Loss: 23.3604, Evaluation BLEU: 0.0000\n",
      "Epoch [1064/2000], Evaluation Loss: 23.3607, Evaluation BLEU: 0.0000\n",
      "Epoch [1065/2000], Evaluation Loss: 23.3611, Evaluation BLEU: 0.0000\n",
      "Epoch [1066/2000], Evaluation Loss: 23.3614, Evaluation BLEU: 0.0000\n",
      "Epoch [1067/2000], Evaluation Loss: 23.3618, Evaluation BLEU: 0.0000\n",
      "Epoch [1068/2000], Evaluation Loss: 23.3621, Evaluation BLEU: 0.0000\n",
      "Epoch [1069/2000], Evaluation Loss: 23.3625, Evaluation BLEU: 0.0000\n",
      "Epoch [1070/2000], Evaluation Loss: 23.3629, Evaluation BLEU: 0.0000\n",
      "Epoch [1071/2000], Evaluation Loss: 23.3632, Evaluation BLEU: 0.0000\n",
      "Epoch [1072/2000], Evaluation Loss: 23.3636, Evaluation BLEU: 0.0000\n",
      "Epoch [1073/2000], Evaluation Loss: 23.3640, Evaluation BLEU: 0.0000\n",
      "Epoch [1074/2000], Evaluation Loss: 23.3644, Evaluation BLEU: 0.0000\n",
      "Epoch [1075/2000], Evaluation Loss: 23.3648, Evaluation BLEU: 0.0000\n",
      "Epoch [1076/2000], Evaluation Loss: 23.3652, Evaluation BLEU: 0.0000\n",
      "Epoch [1077/2000], Evaluation Loss: 23.3655, Evaluation BLEU: 0.0000\n",
      "Epoch [1078/2000], Evaluation Loss: 23.3659, Evaluation BLEU: 0.0000\n",
      "Epoch [1079/2000], Evaluation Loss: 23.3663, Evaluation BLEU: 0.0000\n",
      "Epoch [1080/2000], Evaluation Loss: 23.3667, Evaluation BLEU: 0.0000\n",
      "Epoch [1081/2000], Evaluation Loss: 23.3671, Evaluation BLEU: 0.0000\n",
      "Epoch [1082/2000], Evaluation Loss: 23.3674, Evaluation BLEU: 0.0000\n",
      "Epoch [1083/2000], Evaluation Loss: 23.3678, Evaluation BLEU: 0.0000\n",
      "Epoch [1084/2000], Evaluation Loss: 23.3681, Evaluation BLEU: 0.0000\n",
      "Epoch [1085/2000], Evaluation Loss: 23.3685, Evaluation BLEU: 0.0000\n",
      "Epoch [1086/2000], Evaluation Loss: 23.3688, Evaluation BLEU: 0.0000\n",
      "Epoch [1087/2000], Evaluation Loss: 23.3692, Evaluation BLEU: 0.0000\n",
      "Epoch [1088/2000], Evaluation Loss: 23.3695, Evaluation BLEU: 0.0000\n",
      "Epoch [1089/2000], Evaluation Loss: 23.3698, Evaluation BLEU: 0.0000\n",
      "Epoch [1090/2000], Evaluation Loss: 23.3701, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1091/2000], Evaluation Loss: 23.3704, Evaluation BLEU: 0.0000\n",
      "Epoch [1092/2000], Evaluation Loss: 23.3708, Evaluation BLEU: 0.0000\n",
      "Epoch [1093/2000], Evaluation Loss: 23.3711, Evaluation BLEU: 0.0000\n",
      "Epoch [1094/2000], Evaluation Loss: 23.3714, Evaluation BLEU: 0.0000\n",
      "Epoch [1095/2000], Evaluation Loss: 23.3717, Evaluation BLEU: 0.0000\n",
      "Epoch [1096/2000], Evaluation Loss: 23.3721, Evaluation BLEU: 0.0000\n",
      "Epoch [1097/2000], Evaluation Loss: 23.3725, Evaluation BLEU: 0.0000\n",
      "Epoch [1098/2000], Evaluation Loss: 23.3728, Evaluation BLEU: 0.0000\n",
      "Epoch [1099/2000], Evaluation Loss: 23.3733, Evaluation BLEU: 0.0000\n",
      "Epoch [1100/2000], Evaluation Loss: 23.3736, Evaluation BLEU: 0.0000\n",
      "Epoch [1101/2000], Evaluation Loss: 23.3740, Evaluation BLEU: 0.0000\n",
      "Epoch [1102/2000], Evaluation Loss: 23.3743, Evaluation BLEU: 0.0000\n",
      "Epoch [1103/2000], Evaluation Loss: 23.3747, Evaluation BLEU: 0.0000\n",
      "Epoch [1104/2000], Evaluation Loss: 23.3750, Evaluation BLEU: 0.0000\n",
      "Epoch [1105/2000], Evaluation Loss: 23.3754, Evaluation BLEU: 0.0000\n",
      "Epoch [1106/2000], Evaluation Loss: 23.3757, Evaluation BLEU: 0.0000\n",
      "Epoch [1107/2000], Evaluation Loss: 23.3760, Evaluation BLEU: 0.0000\n",
      "Epoch [1108/2000], Evaluation Loss: 23.3764, Evaluation BLEU: 0.0000\n",
      "Epoch [1109/2000], Evaluation Loss: 23.3767, Evaluation BLEU: 0.0000\n",
      "Epoch [1110/2000], Evaluation Loss: 23.3770, Evaluation BLEU: 0.0000\n",
      "Epoch [1111/2000], Evaluation Loss: 23.3773, Evaluation BLEU: 0.0000\n",
      "Epoch [1112/2000], Evaluation Loss: 23.3776, Evaluation BLEU: 0.0000\n",
      "Epoch [1113/2000], Evaluation Loss: 23.3780, Evaluation BLEU: 0.0000\n",
      "Epoch [1114/2000], Evaluation Loss: 23.3783, Evaluation BLEU: 0.0000\n",
      "Epoch [1115/2000], Evaluation Loss: 23.3786, Evaluation BLEU: 0.0000\n",
      "Epoch [1116/2000], Evaluation Loss: 23.3789, Evaluation BLEU: 0.0000\n",
      "Epoch [1117/2000], Evaluation Loss: 23.3792, Evaluation BLEU: 0.0000\n",
      "Epoch [1118/2000], Evaluation Loss: 23.3795, Evaluation BLEU: 0.0000\n",
      "Epoch [1119/2000], Evaluation Loss: 23.3798, Evaluation BLEU: 0.0000\n",
      "Epoch [1120/2000], Evaluation Loss: 23.3801, Evaluation BLEU: 0.0000\n",
      "Epoch [1121/2000], Evaluation Loss: 23.3803, Evaluation BLEU: 0.0000\n",
      "Epoch [1122/2000], Evaluation Loss: 23.3806, Evaluation BLEU: 0.0000\n",
      "Epoch [1123/2000], Evaluation Loss: 23.3808, Evaluation BLEU: 0.0000\n",
      "Epoch [1124/2000], Evaluation Loss: 23.3811, Evaluation BLEU: 0.0000\n",
      "Epoch [1125/2000], Evaluation Loss: 23.3813, Evaluation BLEU: 0.0000\n",
      "Epoch [1126/2000], Evaluation Loss: 23.3816, Evaluation BLEU: 0.0000\n",
      "Epoch [1127/2000], Evaluation Loss: 23.3818, Evaluation BLEU: 0.0000\n",
      "Epoch [1128/2000], Evaluation Loss: 23.3821, Evaluation BLEU: 0.0000\n",
      "Epoch [1129/2000], Evaluation Loss: 23.3824, Evaluation BLEU: 0.0000\n",
      "Epoch [1130/2000], Evaluation Loss: 23.3826, Evaluation BLEU: 0.0000\n",
      "Epoch [1131/2000], Evaluation Loss: 23.3829, Evaluation BLEU: 0.0000\n",
      "Epoch [1132/2000], Evaluation Loss: 23.3831, Evaluation BLEU: 0.0000\n",
      "Epoch [1133/2000], Evaluation Loss: 23.3834, Evaluation BLEU: 0.0000\n",
      "Epoch [1134/2000], Evaluation Loss: 23.3837, Evaluation BLEU: 0.0000\n",
      "Epoch [1135/2000], Evaluation Loss: 23.3840, Evaluation BLEU: 0.0000\n",
      "Epoch [1136/2000], Evaluation Loss: 23.3843, Evaluation BLEU: 0.0000\n",
      "Epoch [1137/2000], Evaluation Loss: 23.3846, Evaluation BLEU: 0.0000\n",
      "Epoch [1138/2000], Evaluation Loss: 23.3848, Evaluation BLEU: 0.0000\n",
      "Epoch [1139/2000], Evaluation Loss: 23.3851, Evaluation BLEU: 0.0000\n",
      "Epoch [1140/2000], Evaluation Loss: 23.3854, Evaluation BLEU: 0.0000\n",
      "Epoch [1141/2000], Evaluation Loss: 23.3856, Evaluation BLEU: 0.0000\n",
      "Epoch [1142/2000], Evaluation Loss: 23.3859, Evaluation BLEU: 0.0000\n",
      "Epoch [1143/2000], Evaluation Loss: 23.3861, Evaluation BLEU: 0.0000\n",
      "Epoch [1144/2000], Evaluation Loss: 23.3864, Evaluation BLEU: 0.0000\n",
      "Epoch [1145/2000], Evaluation Loss: 23.3866, Evaluation BLEU: 0.0000\n",
      "Epoch [1146/2000], Evaluation Loss: 23.3869, Evaluation BLEU: 0.0000\n",
      "Epoch [1147/2000], Evaluation Loss: 23.3872, Evaluation BLEU: 0.0000\n",
      "Epoch [1148/2000], Evaluation Loss: 23.3875, Evaluation BLEU: 0.0000\n",
      "Epoch [1149/2000], Evaluation Loss: 23.3878, Evaluation BLEU: 0.0000\n",
      "Epoch [1150/2000], Evaluation Loss: 23.3881, Evaluation BLEU: 0.0000\n",
      "Epoch [1151/2000], Evaluation Loss: 23.3885, Evaluation BLEU: 0.0000\n",
      "Epoch [1152/2000], Evaluation Loss: 23.3888, Evaluation BLEU: 0.0000\n",
      "Epoch [1153/2000], Evaluation Loss: 23.3891, Evaluation BLEU: 0.0000\n",
      "Epoch [1154/2000], Evaluation Loss: 23.3895, Evaluation BLEU: 0.0000\n",
      "Epoch [1155/2000], Evaluation Loss: 23.3898, Evaluation BLEU: 0.0000\n",
      "Epoch [1156/2000], Evaluation Loss: 23.3902, Evaluation BLEU: 0.0000\n",
      "Epoch [1157/2000], Evaluation Loss: 23.3905, Evaluation BLEU: 0.0000\n",
      "Epoch [1158/2000], Evaluation Loss: 23.3908, Evaluation BLEU: 0.0000\n",
      "Epoch [1159/2000], Evaluation Loss: 23.3911, Evaluation BLEU: 0.0000\n",
      "Epoch [1160/2000], Evaluation Loss: 23.3914, Evaluation BLEU: 0.0000\n",
      "Epoch [1161/2000], Evaluation Loss: 23.3917, Evaluation BLEU: 0.0000\n",
      "Epoch [1162/2000], Evaluation Loss: 23.3919, Evaluation BLEU: 0.0000\n",
      "Epoch [1163/2000], Evaluation Loss: 23.3922, Evaluation BLEU: 0.0000\n",
      "Epoch [1164/2000], Evaluation Loss: 23.3924, Evaluation BLEU: 0.0000\n",
      "Epoch [1165/2000], Evaluation Loss: 23.3926, Evaluation BLEU: 0.0000\n",
      "Epoch [1166/2000], Evaluation Loss: 23.3928, Evaluation BLEU: 0.0000\n",
      "Epoch [1167/2000], Evaluation Loss: 23.3930, Evaluation BLEU: 0.0000\n",
      "Epoch [1168/2000], Evaluation Loss: 23.3932, Evaluation BLEU: 0.0000\n",
      "Epoch [1169/2000], Evaluation Loss: 23.3934, Evaluation BLEU: 0.0000\n",
      "Epoch [1170/2000], Evaluation Loss: 23.3936, Evaluation BLEU: 0.0000\n",
      "Epoch [1171/2000], Evaluation Loss: 23.3938, Evaluation BLEU: 0.0000\n",
      "Epoch [1172/2000], Evaluation Loss: 23.3940, Evaluation BLEU: 0.0000\n",
      "Epoch [1173/2000], Evaluation Loss: 23.3942, Evaluation BLEU: 0.0000\n",
      "Epoch [1174/2000], Evaluation Loss: 23.3944, Evaluation BLEU: 0.0000\n",
      "Epoch [1175/2000], Evaluation Loss: 23.3946, Evaluation BLEU: 0.0000\n",
      "Epoch [1176/2000], Evaluation Loss: 23.3949, Evaluation BLEU: 0.0000\n",
      "Epoch [1177/2000], Evaluation Loss: 23.3951, Evaluation BLEU: 0.0000\n",
      "Epoch [1178/2000], Evaluation Loss: 23.3953, Evaluation BLEU: 0.0000\n",
      "Epoch [1179/2000], Evaluation Loss: 23.3956, Evaluation BLEU: 0.0000\n",
      "Epoch [1180/2000], Evaluation Loss: 23.3958, Evaluation BLEU: 0.0000\n",
      "Epoch [1181/2000], Evaluation Loss: 23.3961, Evaluation BLEU: 0.0000\n",
      "Epoch [1182/2000], Evaluation Loss: 23.3964, Evaluation BLEU: 0.0000\n",
      "Epoch [1183/2000], Evaluation Loss: 23.3966, Evaluation BLEU: 0.0000\n",
      "Epoch [1184/2000], Evaluation Loss: 23.3969, Evaluation BLEU: 0.0000\n",
      "Epoch [1185/2000], Evaluation Loss: 23.3972, Evaluation BLEU: 0.0000\n",
      "Epoch [1186/2000], Evaluation Loss: 23.3975, Evaluation BLEU: 0.0000\n",
      "Epoch [1187/2000], Evaluation Loss: 23.3978, Evaluation BLEU: 0.0000\n",
      "Epoch [1188/2000], Evaluation Loss: 23.3981, Evaluation BLEU: 0.0000\n",
      "Epoch [1189/2000], Evaluation Loss: 23.3984, Evaluation BLEU: 0.0000\n",
      "Epoch [1190/2000], Evaluation Loss: 23.3987, Evaluation BLEU: 0.0000\n",
      "Epoch [1191/2000], Evaluation Loss: 23.3990, Evaluation BLEU: 0.0000\n",
      "Epoch [1192/2000], Evaluation Loss: 23.3992, Evaluation BLEU: 0.0000\n",
      "Epoch [1193/2000], Evaluation Loss: 23.3994, Evaluation BLEU: 0.0000\n",
      "Epoch [1194/2000], Evaluation Loss: 23.3996, Evaluation BLEU: 0.0000\n",
      "Epoch [1195/2000], Evaluation Loss: 23.3998, Evaluation BLEU: 0.0000\n",
      "Epoch [1196/2000], Evaluation Loss: 23.3999, Evaluation BLEU: 0.0000\n",
      "Epoch [1197/2000], Evaluation Loss: 23.4001, Evaluation BLEU: 0.0000\n",
      "Epoch [1198/2000], Evaluation Loss: 23.4001, Evaluation BLEU: 0.0000\n",
      "Epoch [1199/2000], Evaluation Loss: 23.4002, Evaluation BLEU: 0.0000\n",
      "Epoch [1200/2000], Evaluation Loss: 23.4003, Evaluation BLEU: 0.0000\n",
      "Epoch [1201/2000], Evaluation Loss: 23.4004, Evaluation BLEU: 0.0000\n",
      "Epoch [1202/2000], Evaluation Loss: 23.4004, Evaluation BLEU: 0.0000\n",
      "Epoch [1203/2000], Evaluation Loss: 23.4005, Evaluation BLEU: 0.0000\n",
      "Epoch [1204/2000], Evaluation Loss: 23.4005, Evaluation BLEU: 0.0000\n",
      "Epoch [1205/2000], Evaluation Loss: 23.4006, Evaluation BLEU: 0.0000\n",
      "Epoch [1206/2000], Evaluation Loss: 23.4007, Evaluation BLEU: 0.0000\n",
      "Epoch [1207/2000], Evaluation Loss: 23.4007, Evaluation BLEU: 0.0000\n",
      "Epoch [1208/2000], Evaluation Loss: 23.4008, Evaluation BLEU: 0.0000\n",
      "Epoch [1209/2000], Evaluation Loss: 23.4010, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1210/2000], Evaluation Loss: 23.4011, Evaluation BLEU: 0.0000\n",
      "Epoch [1211/2000], Evaluation Loss: 23.4012, Evaluation BLEU: 0.0000\n",
      "Epoch [1212/2000], Evaluation Loss: 23.4013, Evaluation BLEU: 0.0000\n",
      "Epoch [1213/2000], Evaluation Loss: 23.4014, Evaluation BLEU: 0.0000\n",
      "Epoch [1214/2000], Evaluation Loss: 23.4016, Evaluation BLEU: 0.0000\n",
      "Epoch [1215/2000], Evaluation Loss: 23.4018, Evaluation BLEU: 0.0000\n",
      "Epoch [1216/2000], Evaluation Loss: 23.4020, Evaluation BLEU: 0.0000\n",
      "Epoch [1217/2000], Evaluation Loss: 23.4022, Evaluation BLEU: 0.0000\n",
      "Epoch [1218/2000], Evaluation Loss: 23.4023, Evaluation BLEU: 0.0000\n",
      "Epoch [1219/2000], Evaluation Loss: 23.4025, Evaluation BLEU: 0.0000\n",
      "Epoch [1220/2000], Evaluation Loss: 23.4027, Evaluation BLEU: 0.0000\n",
      "Epoch [1221/2000], Evaluation Loss: 23.4029, Evaluation BLEU: 0.0000\n",
      "Epoch [1222/2000], Evaluation Loss: 23.4031, Evaluation BLEU: 0.0000\n",
      "Epoch [1223/2000], Evaluation Loss: 23.4033, Evaluation BLEU: 0.0000\n",
      "Epoch [1224/2000], Evaluation Loss: 23.4036, Evaluation BLEU: 0.0000\n",
      "Epoch [1225/2000], Evaluation Loss: 23.4038, Evaluation BLEU: 0.0000\n",
      "Epoch [1226/2000], Evaluation Loss: 23.4041, Evaluation BLEU: 0.0000\n",
      "Epoch [1227/2000], Evaluation Loss: 23.4043, Evaluation BLEU: 0.0000\n",
      "Epoch [1228/2000], Evaluation Loss: 23.4045, Evaluation BLEU: 0.0000\n",
      "Epoch [1229/2000], Evaluation Loss: 23.4048, Evaluation BLEU: 0.0000\n",
      "Epoch [1230/2000], Evaluation Loss: 23.4050, Evaluation BLEU: 0.0000\n",
      "Epoch [1231/2000], Evaluation Loss: 23.4052, Evaluation BLEU: 0.0000\n",
      "Epoch [1232/2000], Evaluation Loss: 23.4054, Evaluation BLEU: 0.0000\n",
      "Epoch [1233/2000], Evaluation Loss: 23.4055, Evaluation BLEU: 0.0000\n",
      "Epoch [1234/2000], Evaluation Loss: 23.4057, Evaluation BLEU: 0.0000\n",
      "Epoch [1235/2000], Evaluation Loss: 23.4059, Evaluation BLEU: 0.0000\n",
      "Epoch [1236/2000], Evaluation Loss: 23.4060, Evaluation BLEU: 0.0000\n",
      "Epoch [1237/2000], Evaluation Loss: 23.4062, Evaluation BLEU: 0.0000\n",
      "Epoch [1238/2000], Evaluation Loss: 23.4063, Evaluation BLEU: 0.0000\n",
      "Epoch [1239/2000], Evaluation Loss: 23.4064, Evaluation BLEU: 0.0000\n",
      "Epoch [1240/2000], Evaluation Loss: 23.4065, Evaluation BLEU: 0.0000\n",
      "Epoch [1241/2000], Evaluation Loss: 23.4067, Evaluation BLEU: 0.0000\n",
      "Epoch [1242/2000], Evaluation Loss: 23.4068, Evaluation BLEU: 0.0000\n",
      "Epoch [1243/2000], Evaluation Loss: 23.4069, Evaluation BLEU: 0.0000\n",
      "Epoch [1244/2000], Evaluation Loss: 23.4070, Evaluation BLEU: 0.0000\n",
      "Epoch [1245/2000], Evaluation Loss: 23.4071, Evaluation BLEU: 0.0000\n",
      "Epoch [1246/2000], Evaluation Loss: 23.4072, Evaluation BLEU: 0.0000\n",
      "Epoch [1247/2000], Evaluation Loss: 23.4074, Evaluation BLEU: 0.0000\n",
      "Epoch [1248/2000], Evaluation Loss: 23.4075, Evaluation BLEU: 0.0000\n",
      "Epoch [1249/2000], Evaluation Loss: 23.4077, Evaluation BLEU: 0.0000\n",
      "Epoch [1250/2000], Evaluation Loss: 23.4078, Evaluation BLEU: 0.0000\n",
      "Epoch [1251/2000], Evaluation Loss: 23.4079, Evaluation BLEU: 0.0000\n",
      "Epoch [1252/2000], Evaluation Loss: 23.4081, Evaluation BLEU: 0.0000\n",
      "Epoch [1253/2000], Evaluation Loss: 23.4083, Evaluation BLEU: 0.0000\n",
      "Epoch [1254/2000], Evaluation Loss: 23.4085, Evaluation BLEU: 0.0000\n",
      "Epoch [1255/2000], Evaluation Loss: 23.4087, Evaluation BLEU: 0.0000\n",
      "Epoch [1256/2000], Evaluation Loss: 23.4089, Evaluation BLEU: 0.0000\n",
      "Epoch [1257/2000], Evaluation Loss: 23.4090, Evaluation BLEU: 0.0000\n",
      "Epoch [1258/2000], Evaluation Loss: 23.4092, Evaluation BLEU: 0.0000\n",
      "Epoch [1259/2000], Evaluation Loss: 23.4093, Evaluation BLEU: 0.0000\n",
      "Epoch [1260/2000], Evaluation Loss: 23.4094, Evaluation BLEU: 0.0000\n",
      "Epoch [1261/2000], Evaluation Loss: 23.4096, Evaluation BLEU: 0.0000\n",
      "Epoch [1262/2000], Evaluation Loss: 23.4098, Evaluation BLEU: 0.0000\n",
      "Epoch [1263/2000], Evaluation Loss: 23.4099, Evaluation BLEU: 0.0000\n",
      "Epoch [1264/2000], Evaluation Loss: 23.4101, Evaluation BLEU: 0.0000\n",
      "Epoch [1265/2000], Evaluation Loss: 23.4103, Evaluation BLEU: 0.0000\n",
      "Epoch [1266/2000], Evaluation Loss: 23.4104, Evaluation BLEU: 0.0000\n",
      "Epoch [1267/2000], Evaluation Loss: 23.4106, Evaluation BLEU: 0.0000\n",
      "Epoch [1268/2000], Evaluation Loss: 23.4109, Evaluation BLEU: 0.0000\n",
      "Epoch [1269/2000], Evaluation Loss: 23.4111, Evaluation BLEU: 0.0000\n",
      "Epoch [1270/2000], Evaluation Loss: 23.4113, Evaluation BLEU: 0.0000\n",
      "Epoch [1271/2000], Evaluation Loss: 23.4116, Evaluation BLEU: 0.0000\n",
      "Epoch [1272/2000], Evaluation Loss: 23.4118, Evaluation BLEU: 0.0000\n",
      "Epoch [1273/2000], Evaluation Loss: 23.4121, Evaluation BLEU: 0.0000\n",
      "Epoch [1274/2000], Evaluation Loss: 23.4123, Evaluation BLEU: 0.0000\n",
      "Epoch [1275/2000], Evaluation Loss: 23.4126, Evaluation BLEU: 0.0000\n",
      "Epoch [1276/2000], Evaluation Loss: 23.4129, Evaluation BLEU: 0.0000\n",
      "Epoch [1277/2000], Evaluation Loss: 23.4132, Evaluation BLEU: 0.0000\n",
      "Epoch [1278/2000], Evaluation Loss: 23.4135, Evaluation BLEU: 0.0000\n",
      "Epoch [1279/2000], Evaluation Loss: 23.4137, Evaluation BLEU: 0.0000\n",
      "Epoch [1280/2000], Evaluation Loss: 23.4140, Evaluation BLEU: 0.0000\n",
      "Epoch [1281/2000], Evaluation Loss: 23.4142, Evaluation BLEU: 0.0000\n",
      "Epoch [1282/2000], Evaluation Loss: 23.4145, Evaluation BLEU: 0.0000\n",
      "Epoch [1283/2000], Evaluation Loss: 23.4147, Evaluation BLEU: 0.0000\n",
      "Epoch [1284/2000], Evaluation Loss: 23.4150, Evaluation BLEU: 0.0000\n",
      "Epoch [1285/2000], Evaluation Loss: 23.4152, Evaluation BLEU: 0.0000\n",
      "Epoch [1286/2000], Evaluation Loss: 23.4155, Evaluation BLEU: 0.0000\n",
      "Epoch [1287/2000], Evaluation Loss: 23.4157, Evaluation BLEU: 0.0000\n",
      "Epoch [1288/2000], Evaluation Loss: 23.4159, Evaluation BLEU: 0.0000\n",
      "Epoch [1289/2000], Evaluation Loss: 23.4160, Evaluation BLEU: 0.0000\n",
      "Epoch [1290/2000], Evaluation Loss: 23.4162, Evaluation BLEU: 0.0000\n",
      "Epoch [1291/2000], Evaluation Loss: 23.4164, Evaluation BLEU: 0.0000\n",
      "Epoch [1292/2000], Evaluation Loss: 23.4166, Evaluation BLEU: 0.0000\n",
      "Epoch [1293/2000], Evaluation Loss: 23.4168, Evaluation BLEU: 0.0000\n",
      "Epoch [1294/2000], Evaluation Loss: 23.4170, Evaluation BLEU: 0.0000\n",
      "Epoch [1295/2000], Evaluation Loss: 23.4172, Evaluation BLEU: 0.0000\n",
      "Epoch [1296/2000], Evaluation Loss: 23.4174, Evaluation BLEU: 0.0000\n",
      "Epoch [1297/2000], Evaluation Loss: 23.4176, Evaluation BLEU: 0.0000\n",
      "Epoch [1298/2000], Evaluation Loss: 23.4179, Evaluation BLEU: 0.0000\n",
      "Epoch [1299/2000], Evaluation Loss: 23.4181, Evaluation BLEU: 0.0000\n",
      "Epoch [1300/2000], Evaluation Loss: 23.4184, Evaluation BLEU: 0.0000\n",
      "Epoch [1301/2000], Evaluation Loss: 23.4187, Evaluation BLEU: 0.0000\n",
      "Epoch [1302/2000], Evaluation Loss: 23.4189, Evaluation BLEU: 0.0000\n",
      "Epoch [1303/2000], Evaluation Loss: 23.4193, Evaluation BLEU: 0.0000\n",
      "Epoch [1304/2000], Evaluation Loss: 23.4196, Evaluation BLEU: 0.0000\n",
      "Epoch [1305/2000], Evaluation Loss: 23.4200, Evaluation BLEU: 0.0000\n",
      "Epoch [1306/2000], Evaluation Loss: 23.4203, Evaluation BLEU: 0.0000\n",
      "Epoch [1307/2000], Evaluation Loss: 23.4207, Evaluation BLEU: 0.0000\n",
      "Epoch [1308/2000], Evaluation Loss: 23.4210, Evaluation BLEU: 0.0000\n",
      "Epoch [1309/2000], Evaluation Loss: 23.4212, Evaluation BLEU: 0.0000\n",
      "Epoch [1310/2000], Evaluation Loss: 23.4215, Evaluation BLEU: 0.0000\n",
      "Epoch [1311/2000], Evaluation Loss: 23.4218, Evaluation BLEU: 0.0000\n",
      "Epoch [1312/2000], Evaluation Loss: 23.4220, Evaluation BLEU: 0.0000\n",
      "Epoch [1313/2000], Evaluation Loss: 23.4223, Evaluation BLEU: 0.0000\n",
      "Epoch [1314/2000], Evaluation Loss: 23.4226, Evaluation BLEU: 0.0000\n",
      "Epoch [1315/2000], Evaluation Loss: 23.4229, Evaluation BLEU: 0.0000\n",
      "Epoch [1316/2000], Evaluation Loss: 23.4232, Evaluation BLEU: 0.0000\n",
      "Epoch [1317/2000], Evaluation Loss: 23.4235, Evaluation BLEU: 0.0000\n",
      "Epoch [1318/2000], Evaluation Loss: 23.4238, Evaluation BLEU: 0.0000\n",
      "Epoch [1319/2000], Evaluation Loss: 23.4240, Evaluation BLEU: 0.0000\n",
      "Epoch [1320/2000], Evaluation Loss: 23.4243, Evaluation BLEU: 0.0000\n",
      "Epoch [1321/2000], Evaluation Loss: 23.4245, Evaluation BLEU: 0.0000\n",
      "Epoch [1322/2000], Evaluation Loss: 23.4248, Evaluation BLEU: 0.0000\n",
      "Epoch [1323/2000], Evaluation Loss: 23.4251, Evaluation BLEU: 0.0000\n",
      "Epoch [1324/2000], Evaluation Loss: 23.4254, Evaluation BLEU: 0.0000\n",
      "Epoch [1325/2000], Evaluation Loss: 23.4256, Evaluation BLEU: 0.0000\n",
      "Epoch [1326/2000], Evaluation Loss: 23.4259, Evaluation BLEU: 0.0000\n",
      "Epoch [1327/2000], Evaluation Loss: 23.4262, Evaluation BLEU: 0.0000\n",
      "Epoch [1328/2000], Evaluation Loss: 23.4265, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1329/2000], Evaluation Loss: 23.4268, Evaluation BLEU: 0.0000\n",
      "Epoch [1330/2000], Evaluation Loss: 23.4272, Evaluation BLEU: 0.0000\n",
      "Epoch [1331/2000], Evaluation Loss: 23.4275, Evaluation BLEU: 0.0000\n",
      "Epoch [1332/2000], Evaluation Loss: 23.4278, Evaluation BLEU: 0.0000\n",
      "Epoch [1333/2000], Evaluation Loss: 23.4282, Evaluation BLEU: 0.0000\n",
      "Epoch [1334/2000], Evaluation Loss: 23.4284, Evaluation BLEU: 0.0000\n",
      "Epoch [1335/2000], Evaluation Loss: 23.4287, Evaluation BLEU: 0.0000\n",
      "Epoch [1336/2000], Evaluation Loss: 23.4290, Evaluation BLEU: 0.0000\n",
      "Epoch [1337/2000], Evaluation Loss: 23.4293, Evaluation BLEU: 0.0000\n",
      "Epoch [1338/2000], Evaluation Loss: 23.4296, Evaluation BLEU: 0.0000\n",
      "Epoch [1339/2000], Evaluation Loss: 23.4299, Evaluation BLEU: 0.0000\n",
      "Epoch [1340/2000], Evaluation Loss: 23.4301, Evaluation BLEU: 0.0000\n",
      "Epoch [1341/2000], Evaluation Loss: 23.4304, Evaluation BLEU: 0.0000\n",
      "Epoch [1342/2000], Evaluation Loss: 23.4307, Evaluation BLEU: 0.0000\n",
      "Epoch [1343/2000], Evaluation Loss: 23.4310, Evaluation BLEU: 0.0000\n",
      "Epoch [1344/2000], Evaluation Loss: 23.4313, Evaluation BLEU: 0.0000\n",
      "Epoch [1345/2000], Evaluation Loss: 23.4316, Evaluation BLEU: 0.0000\n",
      "Epoch [1346/2000], Evaluation Loss: 23.4319, Evaluation BLEU: 0.0000\n",
      "Epoch [1347/2000], Evaluation Loss: 23.4322, Evaluation BLEU: 0.0000\n",
      "Epoch [1348/2000], Evaluation Loss: 23.4325, Evaluation BLEU: 0.0000\n",
      "Epoch [1349/2000], Evaluation Loss: 23.4328, Evaluation BLEU: 0.0000\n",
      "Epoch [1350/2000], Evaluation Loss: 23.4331, Evaluation BLEU: 0.0000\n",
      "Epoch [1351/2000], Evaluation Loss: 23.4334, Evaluation BLEU: 0.0000\n",
      "Epoch [1352/2000], Evaluation Loss: 23.4336, Evaluation BLEU: 0.0000\n",
      "Epoch [1353/2000], Evaluation Loss: 23.4339, Evaluation BLEU: 0.0000\n",
      "Epoch [1354/2000], Evaluation Loss: 23.4341, Evaluation BLEU: 0.0000\n",
      "Epoch [1355/2000], Evaluation Loss: 23.4343, Evaluation BLEU: 0.0000\n",
      "Epoch [1356/2000], Evaluation Loss: 23.4346, Evaluation BLEU: 0.0000\n",
      "Epoch [1357/2000], Evaluation Loss: 23.4348, Evaluation BLEU: 0.0000\n",
      "Epoch [1358/2000], Evaluation Loss: 23.4350, Evaluation BLEU: 0.0000\n",
      "Epoch [1359/2000], Evaluation Loss: 23.4352, Evaluation BLEU: 0.0000\n",
      "Epoch [1360/2000], Evaluation Loss: 23.4354, Evaluation BLEU: 0.0000\n",
      "Epoch [1361/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1362/2000], Evaluation Loss: 23.4357, Evaluation BLEU: 0.0000\n",
      "Epoch [1363/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1364/2000], Evaluation Loss: 23.4359, Evaluation BLEU: 0.0000\n",
      "Epoch [1365/2000], Evaluation Loss: 23.4359, Evaluation BLEU: 0.0000\n",
      "Epoch [1366/2000], Evaluation Loss: 23.4360, Evaluation BLEU: 0.0000\n",
      "Epoch [1367/2000], Evaluation Loss: 23.4360, Evaluation BLEU: 0.0000\n",
      "Epoch [1368/2000], Evaluation Loss: 23.4360, Evaluation BLEU: 0.0000\n",
      "Epoch [1369/2000], Evaluation Loss: 23.4360, Evaluation BLEU: 0.0000\n",
      "Epoch [1370/2000], Evaluation Loss: 23.4360, Evaluation BLEU: 0.0000\n",
      "Epoch [1371/2000], Evaluation Loss: 23.4360, Evaluation BLEU: 0.0000\n",
      "Epoch [1372/2000], Evaluation Loss: 23.4360, Evaluation BLEU: 0.0000\n",
      "Epoch [1373/2000], Evaluation Loss: 23.4359, Evaluation BLEU: 0.0000\n",
      "Epoch [1374/2000], Evaluation Loss: 23.4359, Evaluation BLEU: 0.0000\n",
      "Epoch [1375/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1376/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1377/2000], Evaluation Loss: 23.4357, Evaluation BLEU: 0.0000\n",
      "Epoch [1378/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1379/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1380/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1381/2000], Evaluation Loss: 23.4355, Evaluation BLEU: 0.0000\n",
      "Epoch [1382/2000], Evaluation Loss: 23.4355, Evaluation BLEU: 0.0000\n",
      "Epoch [1383/2000], Evaluation Loss: 23.4355, Evaluation BLEU: 0.0000\n",
      "Epoch [1384/2000], Evaluation Loss: 23.4355, Evaluation BLEU: 0.0000\n",
      "Epoch [1385/2000], Evaluation Loss: 23.4355, Evaluation BLEU: 0.0000\n",
      "Epoch [1386/2000], Evaluation Loss: 23.4355, Evaluation BLEU: 0.0000\n",
      "Epoch [1387/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1388/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1389/2000], Evaluation Loss: 23.4357, Evaluation BLEU: 0.0000\n",
      "Epoch [1390/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1391/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1392/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1393/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1394/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1395/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1396/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1397/2000], Evaluation Loss: 23.4357, Evaluation BLEU: 0.0000\n",
      "Epoch [1398/2000], Evaluation Loss: 23.4357, Evaluation BLEU: 0.0000\n",
      "Epoch [1399/2000], Evaluation Loss: 23.4357, Evaluation BLEU: 0.0000\n",
      "Epoch [1400/2000], Evaluation Loss: 23.4357, Evaluation BLEU: 0.0000\n",
      "Epoch [1401/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1402/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1403/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1404/2000], Evaluation Loss: 23.4355, Evaluation BLEU: 0.0000\n",
      "Epoch [1405/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1406/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1407/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1408/2000], Evaluation Loss: 23.4357, Evaluation BLEU: 0.0000\n",
      "Epoch [1409/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1410/2000], Evaluation Loss: 23.4359, Evaluation BLEU: 0.0000\n",
      "Epoch [1411/2000], Evaluation Loss: 23.4360, Evaluation BLEU: 0.0000\n",
      "Epoch [1412/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1413/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1414/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1415/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1416/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1417/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1418/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1419/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1420/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1421/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1422/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1423/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1424/2000], Evaluation Loss: 23.4362, Evaluation BLEU: 0.0000\n",
      "Epoch [1425/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1426/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1427/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1428/2000], Evaluation Loss: 23.4362, Evaluation BLEU: 0.0000\n",
      "Epoch [1429/2000], Evaluation Loss: 23.4362, Evaluation BLEU: 0.0000\n",
      "Epoch [1430/2000], Evaluation Loss: 23.4363, Evaluation BLEU: 0.0000\n",
      "Epoch [1431/2000], Evaluation Loss: 23.4363, Evaluation BLEU: 0.0000\n",
      "Epoch [1432/2000], Evaluation Loss: 23.4364, Evaluation BLEU: 0.0000\n",
      "Epoch [1433/2000], Evaluation Loss: 23.4365, Evaluation BLEU: 0.0000\n",
      "Epoch [1434/2000], Evaluation Loss: 23.4366, Evaluation BLEU: 0.0000\n",
      "Epoch [1435/2000], Evaluation Loss: 23.4367, Evaluation BLEU: 0.0000\n",
      "Epoch [1436/2000], Evaluation Loss: 23.4368, Evaluation BLEU: 0.0000\n",
      "Epoch [1437/2000], Evaluation Loss: 23.4369, Evaluation BLEU: 0.0000\n",
      "Epoch [1438/2000], Evaluation Loss: 23.4369, Evaluation BLEU: 0.0000\n",
      "Epoch [1439/2000], Evaluation Loss: 23.4369, Evaluation BLEU: 0.0000\n",
      "Epoch [1440/2000], Evaluation Loss: 23.4369, Evaluation BLEU: 0.0000\n",
      "Epoch [1441/2000], Evaluation Loss: 23.4369, Evaluation BLEU: 0.0000\n",
      "Epoch [1442/2000], Evaluation Loss: 23.4370, Evaluation BLEU: 0.0000\n",
      "Epoch [1443/2000], Evaluation Loss: 23.4370, Evaluation BLEU: 0.0000\n",
      "Epoch [1444/2000], Evaluation Loss: 23.4371, Evaluation BLEU: 0.0000\n",
      "Epoch [1445/2000], Evaluation Loss: 23.4373, Evaluation BLEU: 0.0000\n",
      "Epoch [1446/2000], Evaluation Loss: 23.4374, Evaluation BLEU: 0.0000\n",
      "Epoch [1447/2000], Evaluation Loss: 23.4376, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1448/2000], Evaluation Loss: 23.4377, Evaluation BLEU: 0.0000\n",
      "Epoch [1449/2000], Evaluation Loss: 23.4379, Evaluation BLEU: 0.0000\n",
      "Epoch [1450/2000], Evaluation Loss: 23.4380, Evaluation BLEU: 0.0000\n",
      "Epoch [1451/2000], Evaluation Loss: 23.4382, Evaluation BLEU: 0.0000\n",
      "Epoch [1452/2000], Evaluation Loss: 23.4384, Evaluation BLEU: 0.0000\n",
      "Epoch [1453/2000], Evaluation Loss: 23.4385, Evaluation BLEU: 0.0000\n",
      "Epoch [1454/2000], Evaluation Loss: 23.4387, Evaluation BLEU: 0.0000\n",
      "Epoch [1455/2000], Evaluation Loss: 23.4389, Evaluation BLEU: 0.0000\n",
      "Epoch [1456/2000], Evaluation Loss: 23.4391, Evaluation BLEU: 0.0000\n",
      "Epoch [1457/2000], Evaluation Loss: 23.4393, Evaluation BLEU: 0.0000\n",
      "Epoch [1458/2000], Evaluation Loss: 23.4395, Evaluation BLEU: 0.0000\n",
      "Epoch [1459/2000], Evaluation Loss: 23.4397, Evaluation BLEU: 0.0000\n",
      "Epoch [1460/2000], Evaluation Loss: 23.4398, Evaluation BLEU: 0.0000\n",
      "Epoch [1461/2000], Evaluation Loss: 23.4400, Evaluation BLEU: 0.0000\n",
      "Epoch [1462/2000], Evaluation Loss: 23.4401, Evaluation BLEU: 0.0000\n",
      "Epoch [1463/2000], Evaluation Loss: 23.4402, Evaluation BLEU: 0.0000\n",
      "Epoch [1464/2000], Evaluation Loss: 23.4403, Evaluation BLEU: 0.0000\n",
      "Epoch [1465/2000], Evaluation Loss: 23.4404, Evaluation BLEU: 0.0000\n",
      "Epoch [1466/2000], Evaluation Loss: 23.4406, Evaluation BLEU: 0.0000\n",
      "Epoch [1467/2000], Evaluation Loss: 23.4407, Evaluation BLEU: 0.0000\n",
      "Epoch [1468/2000], Evaluation Loss: 23.4409, Evaluation BLEU: 0.0000\n",
      "Epoch [1469/2000], Evaluation Loss: 23.4410, Evaluation BLEU: 0.0000\n",
      "Epoch [1470/2000], Evaluation Loss: 23.4412, Evaluation BLEU: 0.0000\n",
      "Epoch [1471/2000], Evaluation Loss: 23.4413, Evaluation BLEU: 0.0000\n",
      "Epoch [1472/2000], Evaluation Loss: 23.4414, Evaluation BLEU: 0.0000\n",
      "Epoch [1473/2000], Evaluation Loss: 23.4415, Evaluation BLEU: 0.0000\n",
      "Epoch [1474/2000], Evaluation Loss: 23.4415, Evaluation BLEU: 0.0000\n",
      "Epoch [1475/2000], Evaluation Loss: 23.4415, Evaluation BLEU: 0.0000\n",
      "Epoch [1476/2000], Evaluation Loss: 23.4415, Evaluation BLEU: 0.0000\n",
      "Epoch [1477/2000], Evaluation Loss: 23.4414, Evaluation BLEU: 0.0000\n",
      "Epoch [1478/2000], Evaluation Loss: 23.4414, Evaluation BLEU: 0.0000\n",
      "Epoch [1479/2000], Evaluation Loss: 23.4413, Evaluation BLEU: 0.0000\n",
      "Epoch [1480/2000], Evaluation Loss: 23.4413, Evaluation BLEU: 0.0000\n",
      "Epoch [1481/2000], Evaluation Loss: 23.4412, Evaluation BLEU: 0.0000\n",
      "Epoch [1482/2000], Evaluation Loss: 23.4412, Evaluation BLEU: 0.0000\n",
      "Epoch [1483/2000], Evaluation Loss: 23.4412, Evaluation BLEU: 0.0000\n",
      "Epoch [1484/2000], Evaluation Loss: 23.4413, Evaluation BLEU: 0.0000\n",
      "Epoch [1485/2000], Evaluation Loss: 23.4413, Evaluation BLEU: 0.0000\n",
      "Epoch [1486/2000], Evaluation Loss: 23.4414, Evaluation BLEU: 0.0000\n",
      "Epoch [1487/2000], Evaluation Loss: 23.4414, Evaluation BLEU: 0.0000\n",
      "Epoch [1488/2000], Evaluation Loss: 23.4414, Evaluation BLEU: 0.0000\n",
      "Epoch [1489/2000], Evaluation Loss: 23.4414, Evaluation BLEU: 0.0000\n",
      "Epoch [1490/2000], Evaluation Loss: 23.4415, Evaluation BLEU: 0.0000\n",
      "Epoch [1491/2000], Evaluation Loss: 23.4415, Evaluation BLEU: 0.0000\n",
      "Epoch [1492/2000], Evaluation Loss: 23.4416, Evaluation BLEU: 0.0000\n",
      "Epoch [1493/2000], Evaluation Loss: 23.4416, Evaluation BLEU: 0.0000\n",
      "Epoch [1494/2000], Evaluation Loss: 23.4417, Evaluation BLEU: 0.0000\n",
      "Epoch [1495/2000], Evaluation Loss: 23.4419, Evaluation BLEU: 0.0000\n",
      "Epoch [1496/2000], Evaluation Loss: 23.4420, Evaluation BLEU: 0.0000\n",
      "Epoch [1497/2000], Evaluation Loss: 23.4422, Evaluation BLEU: 0.0000\n",
      "Epoch [1498/2000], Evaluation Loss: 23.4423, Evaluation BLEU: 0.0000\n",
      "Epoch [1499/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1500/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1501/2000], Evaluation Loss: 23.4427, Evaluation BLEU: 0.0000\n",
      "Epoch [1502/2000], Evaluation Loss: 23.4429, Evaluation BLEU: 0.0000\n",
      "Epoch [1503/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1504/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1505/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1506/2000], Evaluation Loss: 23.4432, Evaluation BLEU: 0.0000\n",
      "Epoch [1507/2000], Evaluation Loss: 23.4432, Evaluation BLEU: 0.0000\n",
      "Epoch [1508/2000], Evaluation Loss: 23.4432, Evaluation BLEU: 0.0000\n",
      "Epoch [1509/2000], Evaluation Loss: 23.4432, Evaluation BLEU: 0.0000\n",
      "Epoch [1510/2000], Evaluation Loss: 23.4432, Evaluation BLEU: 0.0000\n",
      "Epoch [1511/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1512/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1513/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1514/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1515/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1516/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1517/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1518/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1519/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1520/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1521/2000], Evaluation Loss: 23.4429, Evaluation BLEU: 0.0000\n",
      "Epoch [1522/2000], Evaluation Loss: 23.4429, Evaluation BLEU: 0.0000\n",
      "Epoch [1523/2000], Evaluation Loss: 23.4428, Evaluation BLEU: 0.0000\n",
      "Epoch [1524/2000], Evaluation Loss: 23.4428, Evaluation BLEU: 0.0000\n",
      "Epoch [1525/2000], Evaluation Loss: 23.4427, Evaluation BLEU: 0.0000\n",
      "Epoch [1526/2000], Evaluation Loss: 23.4427, Evaluation BLEU: 0.0000\n",
      "Epoch [1527/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1528/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1529/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1530/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1531/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1532/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1533/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1534/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1535/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1536/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1537/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1538/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1539/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1540/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1541/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1542/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1543/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1544/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1545/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1546/2000], Evaluation Loss: 23.4427, Evaluation BLEU: 0.0000\n",
      "Epoch [1547/2000], Evaluation Loss: 23.4427, Evaluation BLEU: 0.0000\n",
      "Epoch [1548/2000], Evaluation Loss: 23.4428, Evaluation BLEU: 0.0000\n",
      "Epoch [1549/2000], Evaluation Loss: 23.4428, Evaluation BLEU: 0.0000\n",
      "Epoch [1550/2000], Evaluation Loss: 23.4429, Evaluation BLEU: 0.0000\n",
      "Epoch [1551/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1552/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1553/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1554/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1555/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1556/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1557/2000], Evaluation Loss: 23.4432, Evaluation BLEU: 0.0000\n",
      "Epoch [1558/2000], Evaluation Loss: 23.4434, Evaluation BLEU: 0.0000\n",
      "Epoch [1559/2000], Evaluation Loss: 23.4435, Evaluation BLEU: 0.0000\n",
      "Epoch [1560/2000], Evaluation Loss: 23.4437, Evaluation BLEU: 0.0000\n",
      "Epoch [1561/2000], Evaluation Loss: 23.4439, Evaluation BLEU: 0.0000\n",
      "Epoch [1562/2000], Evaluation Loss: 23.4441, Evaluation BLEU: 0.0000\n",
      "Epoch [1563/2000], Evaluation Loss: 23.4444, Evaluation BLEU: 0.0000\n",
      "Epoch [1564/2000], Evaluation Loss: 23.4447, Evaluation BLEU: 0.0000\n",
      "Epoch [1565/2000], Evaluation Loss: 23.4449, Evaluation BLEU: 0.0000\n",
      "Epoch [1566/2000], Evaluation Loss: 23.4452, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1567/2000], Evaluation Loss: 23.4455, Evaluation BLEU: 0.0000\n",
      "Epoch [1568/2000], Evaluation Loss: 23.4458, Evaluation BLEU: 0.0000\n",
      "Epoch [1569/2000], Evaluation Loss: 23.4461, Evaluation BLEU: 0.0000\n",
      "Epoch [1570/2000], Evaluation Loss: 23.4463, Evaluation BLEU: 0.0000\n",
      "Epoch [1571/2000], Evaluation Loss: 23.4465, Evaluation BLEU: 0.0000\n",
      "Epoch [1572/2000], Evaluation Loss: 23.4467, Evaluation BLEU: 0.0000\n",
      "Epoch [1573/2000], Evaluation Loss: 23.4468, Evaluation BLEU: 0.0000\n",
      "Epoch [1574/2000], Evaluation Loss: 23.4470, Evaluation BLEU: 0.0000\n",
      "Epoch [1575/2000], Evaluation Loss: 23.4471, Evaluation BLEU: 0.0000\n",
      "Epoch [1576/2000], Evaluation Loss: 23.4473, Evaluation BLEU: 0.0000\n",
      "Epoch [1577/2000], Evaluation Loss: 23.4476, Evaluation BLEU: 0.0000\n",
      "Epoch [1578/2000], Evaluation Loss: 23.4478, Evaluation BLEU: 0.0000\n",
      "Epoch [1579/2000], Evaluation Loss: 23.4481, Evaluation BLEU: 0.0000\n",
      "Epoch [1580/2000], Evaluation Loss: 23.4483, Evaluation BLEU: 0.0000\n",
      "Epoch [1581/2000], Evaluation Loss: 23.4486, Evaluation BLEU: 0.0000\n",
      "Epoch [1582/2000], Evaluation Loss: 23.4489, Evaluation BLEU: 0.0000\n",
      "Epoch [1583/2000], Evaluation Loss: 23.4491, Evaluation BLEU: 0.0000\n",
      "Epoch [1584/2000], Evaluation Loss: 23.4493, Evaluation BLEU: 0.0000\n",
      "Epoch [1585/2000], Evaluation Loss: 23.4495, Evaluation BLEU: 0.0000\n",
      "Epoch [1586/2000], Evaluation Loss: 23.4496, Evaluation BLEU: 0.0000\n",
      "Epoch [1587/2000], Evaluation Loss: 23.4497, Evaluation BLEU: 0.0000\n",
      "Epoch [1588/2000], Evaluation Loss: 23.4498, Evaluation BLEU: 0.0000\n",
      "Epoch [1589/2000], Evaluation Loss: 23.4500, Evaluation BLEU: 0.0000\n",
      "Epoch [1590/2000], Evaluation Loss: 23.4502, Evaluation BLEU: 0.0000\n",
      "Epoch [1591/2000], Evaluation Loss: 23.4503, Evaluation BLEU: 0.0000\n",
      "Epoch [1592/2000], Evaluation Loss: 23.4505, Evaluation BLEU: 0.0000\n",
      "Epoch [1593/2000], Evaluation Loss: 23.4507, Evaluation BLEU: 0.0000\n",
      "Epoch [1594/2000], Evaluation Loss: 23.4510, Evaluation BLEU: 0.0000\n",
      "Epoch [1595/2000], Evaluation Loss: 23.4512, Evaluation BLEU: 0.0000\n",
      "Epoch [1596/2000], Evaluation Loss: 23.4514, Evaluation BLEU: 0.0000\n",
      "Epoch [1597/2000], Evaluation Loss: 23.4516, Evaluation BLEU: 0.0000\n",
      "Epoch [1598/2000], Evaluation Loss: 23.4518, Evaluation BLEU: 0.0000\n",
      "Epoch [1599/2000], Evaluation Loss: 23.4519, Evaluation BLEU: 0.0000\n",
      "Epoch [1600/2000], Evaluation Loss: 23.4520, Evaluation BLEU: 0.0000\n",
      "Epoch [1601/2000], Evaluation Loss: 23.4521, Evaluation BLEU: 0.0000\n",
      "Epoch [1602/2000], Evaluation Loss: 23.4522, Evaluation BLEU: 0.0000\n",
      "Epoch [1603/2000], Evaluation Loss: 23.4523, Evaluation BLEU: 0.0000\n",
      "Epoch [1604/2000], Evaluation Loss: 23.4524, Evaluation BLEU: 0.0000\n",
      "Epoch [1605/2000], Evaluation Loss: 23.4524, Evaluation BLEU: 0.0000\n",
      "Epoch [1606/2000], Evaluation Loss: 23.4524, Evaluation BLEU: 0.0000\n",
      "Epoch [1607/2000], Evaluation Loss: 23.4525, Evaluation BLEU: 0.0000\n",
      "Epoch [1608/2000], Evaluation Loss: 23.4526, Evaluation BLEU: 0.0000\n",
      "Epoch [1609/2000], Evaluation Loss: 23.4527, Evaluation BLEU: 0.0000\n",
      "Epoch [1610/2000], Evaluation Loss: 23.4528, Evaluation BLEU: 0.0000\n",
      "Epoch [1611/2000], Evaluation Loss: 23.4530, Evaluation BLEU: 0.0000\n",
      "Epoch [1612/2000], Evaluation Loss: 23.4532, Evaluation BLEU: 0.0000\n",
      "Epoch [1613/2000], Evaluation Loss: 23.4534, Evaluation BLEU: 0.0000\n",
      "Epoch [1614/2000], Evaluation Loss: 23.4537, Evaluation BLEU: 0.0000\n",
      "Epoch [1615/2000], Evaluation Loss: 23.4540, Evaluation BLEU: 0.0000\n",
      "Epoch [1616/2000], Evaluation Loss: 23.4542, Evaluation BLEU: 0.0000\n",
      "Epoch [1617/2000], Evaluation Loss: 23.4545, Evaluation BLEU: 0.0000\n",
      "Epoch [1618/2000], Evaluation Loss: 23.4547, Evaluation BLEU: 0.0000\n",
      "Epoch [1619/2000], Evaluation Loss: 23.4548, Evaluation BLEU: 0.0000\n",
      "Epoch [1620/2000], Evaluation Loss: 23.4549, Evaluation BLEU: 0.0000\n",
      "Epoch [1621/2000], Evaluation Loss: 23.4550, Evaluation BLEU: 0.0000\n",
      "Epoch [1622/2000], Evaluation Loss: 23.4550, Evaluation BLEU: 0.0000\n",
      "Epoch [1623/2000], Evaluation Loss: 23.4551, Evaluation BLEU: 0.0000\n",
      "Epoch [1624/2000], Evaluation Loss: 23.4551, Evaluation BLEU: 0.0000\n",
      "Epoch [1625/2000], Evaluation Loss: 23.4551, Evaluation BLEU: 0.0000\n",
      "Epoch [1626/2000], Evaluation Loss: 23.4552, Evaluation BLEU: 0.0000\n",
      "Epoch [1627/2000], Evaluation Loss: 23.4552, Evaluation BLEU: 0.0000\n",
      "Epoch [1628/2000], Evaluation Loss: 23.4553, Evaluation BLEU: 0.0000\n",
      "Epoch [1629/2000], Evaluation Loss: 23.4554, Evaluation BLEU: 0.0000\n",
      "Epoch [1630/2000], Evaluation Loss: 23.4555, Evaluation BLEU: 0.0000\n",
      "Epoch [1631/2000], Evaluation Loss: 23.4556, Evaluation BLEU: 0.0000\n",
      "Epoch [1632/2000], Evaluation Loss: 23.4558, Evaluation BLEU: 0.0000\n",
      "Epoch [1633/2000], Evaluation Loss: 23.4560, Evaluation BLEU: 0.0000\n",
      "Epoch [1634/2000], Evaluation Loss: 23.4561, Evaluation BLEU: 0.0000\n",
      "Epoch [1635/2000], Evaluation Loss: 23.4563, Evaluation BLEU: 0.0000\n",
      "Epoch [1636/2000], Evaluation Loss: 23.4564, Evaluation BLEU: 0.0000\n",
      "Epoch [1637/2000], Evaluation Loss: 23.4566, Evaluation BLEU: 0.0000\n",
      "Epoch [1638/2000], Evaluation Loss: 23.4567, Evaluation BLEU: 0.0000\n",
      "Epoch [1639/2000], Evaluation Loss: 23.4568, Evaluation BLEU: 0.0000\n",
      "Epoch [1640/2000], Evaluation Loss: 23.4569, Evaluation BLEU: 0.0000\n",
      "Epoch [1641/2000], Evaluation Loss: 23.4570, Evaluation BLEU: 0.0000\n",
      "Epoch [1642/2000], Evaluation Loss: 23.4570, Evaluation BLEU: 0.0000\n",
      "Epoch [1643/2000], Evaluation Loss: 23.4571, Evaluation BLEU: 0.0000\n",
      "Epoch [1644/2000], Evaluation Loss: 23.4571, Evaluation BLEU: 0.0000\n",
      "Epoch [1645/2000], Evaluation Loss: 23.4572, Evaluation BLEU: 0.0000\n",
      "Epoch [1646/2000], Evaluation Loss: 23.4572, Evaluation BLEU: 0.0000\n",
      "Epoch [1647/2000], Evaluation Loss: 23.4572, Evaluation BLEU: 0.0000\n",
      "Epoch [1648/2000], Evaluation Loss: 23.4573, Evaluation BLEU: 0.0000\n",
      "Epoch [1649/2000], Evaluation Loss: 23.4574, Evaluation BLEU: 0.0000\n",
      "Epoch [1650/2000], Evaluation Loss: 23.4575, Evaluation BLEU: 0.0000\n",
      "Epoch [1651/2000], Evaluation Loss: 23.4577, Evaluation BLEU: 0.0000\n",
      "Epoch [1652/2000], Evaluation Loss: 23.4578, Evaluation BLEU: 0.0000\n",
      "Epoch [1653/2000], Evaluation Loss: 23.4580, Evaluation BLEU: 0.0000\n",
      "Epoch [1654/2000], Evaluation Loss: 23.4582, Evaluation BLEU: 0.0000\n",
      "Epoch [1655/2000], Evaluation Loss: 23.4583, Evaluation BLEU: 0.0000\n",
      "Epoch [1656/2000], Evaluation Loss: 23.4584, Evaluation BLEU: 0.0000\n",
      "Epoch [1657/2000], Evaluation Loss: 23.4584, Evaluation BLEU: 0.0000\n",
      "Epoch [1658/2000], Evaluation Loss: 23.4585, Evaluation BLEU: 0.0000\n",
      "Epoch [1659/2000], Evaluation Loss: 23.4586, Evaluation BLEU: 0.0000\n",
      "Epoch [1660/2000], Evaluation Loss: 23.4587, Evaluation BLEU: 0.0000\n",
      "Epoch [1661/2000], Evaluation Loss: 23.4587, Evaluation BLEU: 0.0000\n",
      "Epoch [1662/2000], Evaluation Loss: 23.4588, Evaluation BLEU: 0.0000\n",
      "Epoch [1663/2000], Evaluation Loss: 23.4590, Evaluation BLEU: 0.0000\n",
      "Epoch [1664/2000], Evaluation Loss: 23.4590, Evaluation BLEU: 0.0000\n",
      "Epoch [1665/2000], Evaluation Loss: 23.4591, Evaluation BLEU: 0.0000\n",
      "Epoch [1666/2000], Evaluation Loss: 23.4592, Evaluation BLEU: 0.0000\n",
      "Epoch [1667/2000], Evaluation Loss: 23.4592, Evaluation BLEU: 0.0000\n",
      "Epoch [1668/2000], Evaluation Loss: 23.4593, Evaluation BLEU: 0.0000\n",
      "Epoch [1669/2000], Evaluation Loss: 23.4594, Evaluation BLEU: 0.0000\n",
      "Epoch [1670/2000], Evaluation Loss: 23.4594, Evaluation BLEU: 0.0000\n",
      "Epoch [1671/2000], Evaluation Loss: 23.4595, Evaluation BLEU: 0.0000\n",
      "Epoch [1672/2000], Evaluation Loss: 23.4596, Evaluation BLEU: 0.0000\n",
      "Epoch [1673/2000], Evaluation Loss: 23.4597, Evaluation BLEU: 0.0000\n",
      "Epoch [1674/2000], Evaluation Loss: 23.4598, Evaluation BLEU: 0.0000\n",
      "Epoch [1675/2000], Evaluation Loss: 23.4600, Evaluation BLEU: 0.0000\n",
      "Epoch [1676/2000], Evaluation Loss: 23.4602, Evaluation BLEU: 0.0000\n",
      "Epoch [1677/2000], Evaluation Loss: 23.4603, Evaluation BLEU: 0.0000\n",
      "Epoch [1678/2000], Evaluation Loss: 23.4604, Evaluation BLEU: 0.0000\n",
      "Epoch [1679/2000], Evaluation Loss: 23.4605, Evaluation BLEU: 0.0000\n",
      "Epoch [1680/2000], Evaluation Loss: 23.4605, Evaluation BLEU: 0.0000\n",
      "Epoch [1681/2000], Evaluation Loss: 23.4605, Evaluation BLEU: 0.0000\n",
      "Epoch [1682/2000], Evaluation Loss: 23.4604, Evaluation BLEU: 0.0000\n",
      "Epoch [1683/2000], Evaluation Loss: 23.4604, Evaluation BLEU: 0.0000\n",
      "Epoch [1684/2000], Evaluation Loss: 23.4604, Evaluation BLEU: 0.0000\n",
      "Epoch [1685/2000], Evaluation Loss: 23.4604, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1686/2000], Evaluation Loss: 23.4604, Evaluation BLEU: 0.0000\n",
      "Epoch [1687/2000], Evaluation Loss: 23.4604, Evaluation BLEU: 0.0000\n",
      "Epoch [1688/2000], Evaluation Loss: 23.4604, Evaluation BLEU: 0.0000\n",
      "Epoch [1689/2000], Evaluation Loss: 23.4605, Evaluation BLEU: 0.0000\n",
      "Epoch [1690/2000], Evaluation Loss: 23.4606, Evaluation BLEU: 0.0000\n",
      "Epoch [1691/2000], Evaluation Loss: 23.4607, Evaluation BLEU: 0.0000\n",
      "Epoch [1692/2000], Evaluation Loss: 23.4608, Evaluation BLEU: 0.0000\n",
      "Epoch [1693/2000], Evaluation Loss: 23.4609, Evaluation BLEU: 0.0000\n",
      "Epoch [1694/2000], Evaluation Loss: 23.4611, Evaluation BLEU: 0.0000\n",
      "Epoch [1695/2000], Evaluation Loss: 23.4612, Evaluation BLEU: 0.0000\n",
      "Epoch [1696/2000], Evaluation Loss: 23.4614, Evaluation BLEU: 0.0000\n",
      "Epoch [1697/2000], Evaluation Loss: 23.4617, Evaluation BLEU: 0.0000\n",
      "Epoch [1698/2000], Evaluation Loss: 23.4619, Evaluation BLEU: 0.0000\n",
      "Epoch [1699/2000], Evaluation Loss: 23.4621, Evaluation BLEU: 0.0000\n",
      "Epoch [1700/2000], Evaluation Loss: 23.4623, Evaluation BLEU: 0.0000\n",
      "Epoch [1701/2000], Evaluation Loss: 23.4626, Evaluation BLEU: 0.0000\n",
      "Epoch [1702/2000], Evaluation Loss: 23.4628, Evaluation BLEU: 0.0000\n",
      "Epoch [1703/2000], Evaluation Loss: 23.4630, Evaluation BLEU: 0.0000\n",
      "Epoch [1704/2000], Evaluation Loss: 23.4631, Evaluation BLEU: 0.0000\n",
      "Epoch [1705/2000], Evaluation Loss: 23.4633, Evaluation BLEU: 0.0000\n",
      "Epoch [1706/2000], Evaluation Loss: 23.4635, Evaluation BLEU: 0.0000\n",
      "Epoch [1707/2000], Evaluation Loss: 23.4637, Evaluation BLEU: 0.0000\n",
      "Epoch [1708/2000], Evaluation Loss: 23.4639, Evaluation BLEU: 0.0000\n",
      "Epoch [1709/2000], Evaluation Loss: 23.4641, Evaluation BLEU: 0.0000\n",
      "Epoch [1710/2000], Evaluation Loss: 23.4643, Evaluation BLEU: 0.0000\n",
      "Epoch [1711/2000], Evaluation Loss: 23.4644, Evaluation BLEU: 0.0000\n",
      "Epoch [1712/2000], Evaluation Loss: 23.4645, Evaluation BLEU: 0.0000\n",
      "Epoch [1713/2000], Evaluation Loss: 23.4647, Evaluation BLEU: 0.0000\n",
      "Epoch [1714/2000], Evaluation Loss: 23.4649, Evaluation BLEU: 0.0000\n",
      "Epoch [1715/2000], Evaluation Loss: 23.4650, Evaluation BLEU: 0.0000\n",
      "Epoch [1716/2000], Evaluation Loss: 23.4651, Evaluation BLEU: 0.0000\n",
      "Epoch [1717/2000], Evaluation Loss: 23.4652, Evaluation BLEU: 0.0000\n",
      "Epoch [1718/2000], Evaluation Loss: 23.4654, Evaluation BLEU: 0.0000\n",
      "Epoch [1719/2000], Evaluation Loss: 23.4655, Evaluation BLEU: 0.0000\n",
      "Epoch [1720/2000], Evaluation Loss: 23.4656, Evaluation BLEU: 0.0000\n",
      "Epoch [1721/2000], Evaluation Loss: 23.4657, Evaluation BLEU: 0.0000\n",
      "Epoch [1722/2000], Evaluation Loss: 23.4659, Evaluation BLEU: 0.0000\n",
      "Epoch [1723/2000], Evaluation Loss: 23.4660, Evaluation BLEU: 0.0000\n",
      "Epoch [1724/2000], Evaluation Loss: 23.4661, Evaluation BLEU: 0.0000\n",
      "Epoch [1725/2000], Evaluation Loss: 23.4662, Evaluation BLEU: 0.0000\n",
      "Epoch [1726/2000], Evaluation Loss: 23.4663, Evaluation BLEU: 0.0000\n",
      "Epoch [1727/2000], Evaluation Loss: 23.4663, Evaluation BLEU: 0.0000\n",
      "Epoch [1728/2000], Evaluation Loss: 23.4662, Evaluation BLEU: 0.0000\n",
      "Epoch [1729/2000], Evaluation Loss: 23.4661, Evaluation BLEU: 0.0000\n",
      "Epoch [1730/2000], Evaluation Loss: 23.4659, Evaluation BLEU: 0.0000\n",
      "Epoch [1731/2000], Evaluation Loss: 23.4657, Evaluation BLEU: 0.0000\n",
      "Epoch [1732/2000], Evaluation Loss: 23.4655, Evaluation BLEU: 0.0000\n",
      "Epoch [1733/2000], Evaluation Loss: 23.4653, Evaluation BLEU: 0.0000\n",
      "Epoch [1734/2000], Evaluation Loss: 23.4651, Evaluation BLEU: 0.0000\n",
      "Epoch [1735/2000], Evaluation Loss: 23.4649, Evaluation BLEU: 0.0000\n",
      "Epoch [1736/2000], Evaluation Loss: 23.4648, Evaluation BLEU: 0.0000\n",
      "Epoch [1737/2000], Evaluation Loss: 23.4647, Evaluation BLEU: 0.0000\n",
      "Epoch [1738/2000], Evaluation Loss: 23.4647, Evaluation BLEU: 0.0000\n",
      "Epoch [1739/2000], Evaluation Loss: 23.4646, Evaluation BLEU: 0.0000\n",
      "Epoch [1740/2000], Evaluation Loss: 23.4646, Evaluation BLEU: 0.0000\n",
      "Epoch [1741/2000], Evaluation Loss: 23.4646, Evaluation BLEU: 0.0000\n",
      "Epoch [1742/2000], Evaluation Loss: 23.4646, Evaluation BLEU: 0.0000\n",
      "Epoch [1743/2000], Evaluation Loss: 23.4647, Evaluation BLEU: 0.0000\n",
      "Epoch [1744/2000], Evaluation Loss: 23.4648, Evaluation BLEU: 0.0000\n",
      "Epoch [1745/2000], Evaluation Loss: 23.4650, Evaluation BLEU: 0.0000\n",
      "Epoch [1746/2000], Evaluation Loss: 23.4652, Evaluation BLEU: 0.0000\n",
      "Epoch [1747/2000], Evaluation Loss: 23.4654, Evaluation BLEU: 0.0000\n",
      "Epoch [1748/2000], Evaluation Loss: 23.4656, Evaluation BLEU: 0.0000\n",
      "Epoch [1749/2000], Evaluation Loss: 23.4658, Evaluation BLEU: 0.0000\n",
      "Epoch [1750/2000], Evaluation Loss: 23.4661, Evaluation BLEU: 0.0000\n",
      "Epoch [1751/2000], Evaluation Loss: 23.4663, Evaluation BLEU: 0.0000\n",
      "Epoch [1752/2000], Evaluation Loss: 23.4666, Evaluation BLEU: 0.0000\n",
      "Epoch [1753/2000], Evaluation Loss: 23.4669, Evaluation BLEU: 0.0000\n",
      "Epoch [1754/2000], Evaluation Loss: 23.4673, Evaluation BLEU: 0.0000\n",
      "Epoch [1755/2000], Evaluation Loss: 23.4676, Evaluation BLEU: 0.0000\n",
      "Epoch [1756/2000], Evaluation Loss: 23.4680, Evaluation BLEU: 0.0000\n",
      "Epoch [1757/2000], Evaluation Loss: 23.4684, Evaluation BLEU: 0.0000\n",
      "Epoch [1758/2000], Evaluation Loss: 23.4687, Evaluation BLEU: 0.0000\n",
      "Epoch [1759/2000], Evaluation Loss: 23.4691, Evaluation BLEU: 0.0000\n",
      "Epoch [1760/2000], Evaluation Loss: 23.4695, Evaluation BLEU: 0.0000\n",
      "Epoch [1761/2000], Evaluation Loss: 23.4700, Evaluation BLEU: 0.0000\n",
      "Epoch [1762/2000], Evaluation Loss: 23.4704, Evaluation BLEU: 0.0000\n",
      "Epoch [1763/2000], Evaluation Loss: 23.4709, Evaluation BLEU: 0.0000\n",
      "Epoch [1764/2000], Evaluation Loss: 23.4714, Evaluation BLEU: 0.0000\n",
      "Epoch [1765/2000], Evaluation Loss: 23.4719, Evaluation BLEU: 0.0000\n",
      "Epoch [1766/2000], Evaluation Loss: 23.4724, Evaluation BLEU: 0.0000\n",
      "Epoch [1767/2000], Evaluation Loss: 23.4730, Evaluation BLEU: 0.0000\n",
      "Epoch [1768/2000], Evaluation Loss: 23.4736, Evaluation BLEU: 0.0000\n",
      "Epoch [1769/2000], Evaluation Loss: 23.4742, Evaluation BLEU: 0.0000\n",
      "Epoch [1770/2000], Evaluation Loss: 23.4748, Evaluation BLEU: 0.0000\n",
      "Epoch [1771/2000], Evaluation Loss: 23.4755, Evaluation BLEU: 0.0000\n",
      "Epoch [1772/2000], Evaluation Loss: 23.4762, Evaluation BLEU: 0.0000\n",
      "Epoch [1773/2000], Evaluation Loss: 23.4769, Evaluation BLEU: 0.0000\n",
      "Epoch [1774/2000], Evaluation Loss: 23.4776, Evaluation BLEU: 0.0000\n",
      "Epoch [1775/2000], Evaluation Loss: 23.4782, Evaluation BLEU: 0.0000\n",
      "Epoch [1776/2000], Evaluation Loss: 23.4789, Evaluation BLEU: 0.0000\n",
      "Epoch [1777/2000], Evaluation Loss: 23.4795, Evaluation BLEU: 0.0000\n",
      "Epoch [1778/2000], Evaluation Loss: 23.4802, Evaluation BLEU: 0.0000\n",
      "Epoch [1779/2000], Evaluation Loss: 23.4807, Evaluation BLEU: 0.0000\n",
      "Epoch [1780/2000], Evaluation Loss: 23.4813, Evaluation BLEU: 0.0000\n",
      "Epoch [1781/2000], Evaluation Loss: 23.4817, Evaluation BLEU: 0.0000\n",
      "Epoch [1782/2000], Evaluation Loss: 23.4822, Evaluation BLEU: 0.0000\n",
      "Epoch [1783/2000], Evaluation Loss: 23.4826, Evaluation BLEU: 0.0000\n",
      "Epoch [1784/2000], Evaluation Loss: 23.4829, Evaluation BLEU: 0.0000\n",
      "Epoch [1785/2000], Evaluation Loss: 23.4832, Evaluation BLEU: 0.0000\n",
      "Epoch [1786/2000], Evaluation Loss: 23.4835, Evaluation BLEU: 0.0000\n",
      "Epoch [1787/2000], Evaluation Loss: 23.4837, Evaluation BLEU: 0.0000\n",
      "Epoch [1788/2000], Evaluation Loss: 23.4839, Evaluation BLEU: 0.0000\n",
      "Epoch [1789/2000], Evaluation Loss: 23.4841, Evaluation BLEU: 0.0000\n",
      "Epoch [1790/2000], Evaluation Loss: 23.4843, Evaluation BLEU: 0.0000\n",
      "Epoch [1791/2000], Evaluation Loss: 23.4844, Evaluation BLEU: 0.0000\n",
      "Epoch [1792/2000], Evaluation Loss: 23.4845, Evaluation BLEU: 0.0000\n",
      "Epoch [1793/2000], Evaluation Loss: 23.4847, Evaluation BLEU: 0.0000\n",
      "Epoch [1794/2000], Evaluation Loss: 23.4848, Evaluation BLEU: 0.0000\n",
      "Epoch [1795/2000], Evaluation Loss: 23.4849, Evaluation BLEU: 0.0000\n",
      "Epoch [1796/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1797/2000], Evaluation Loss: 23.4851, Evaluation BLEU: 0.0000\n",
      "Epoch [1798/2000], Evaluation Loss: 23.4852, Evaluation BLEU: 0.0000\n",
      "Epoch [1799/2000], Evaluation Loss: 23.4853, Evaluation BLEU: 0.0000\n",
      "Epoch [1800/2000], Evaluation Loss: 23.4855, Evaluation BLEU: 0.0000\n",
      "Epoch [1801/2000], Evaluation Loss: 23.4856, Evaluation BLEU: 0.0000\n",
      "Epoch [1802/2000], Evaluation Loss: 23.4858, Evaluation BLEU: 0.0000\n",
      "Epoch [1803/2000], Evaluation Loss: 23.4860, Evaluation BLEU: 0.0000\n",
      "Epoch [1804/2000], Evaluation Loss: 23.4861, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1805/2000], Evaluation Loss: 23.4862, Evaluation BLEU: 0.0000\n",
      "Epoch [1806/2000], Evaluation Loss: 23.4863, Evaluation BLEU: 0.0000\n",
      "Epoch [1807/2000], Evaluation Loss: 23.4864, Evaluation BLEU: 0.0000\n",
      "Epoch [1808/2000], Evaluation Loss: 23.4864, Evaluation BLEU: 0.0000\n",
      "Epoch [1809/2000], Evaluation Loss: 23.4864, Evaluation BLEU: 0.0000\n",
      "Epoch [1810/2000], Evaluation Loss: 23.4863, Evaluation BLEU: 0.0000\n",
      "Epoch [1811/2000], Evaluation Loss: 23.4862, Evaluation BLEU: 0.0000\n",
      "Epoch [1812/2000], Evaluation Loss: 23.4861, Evaluation BLEU: 0.0000\n",
      "Epoch [1813/2000], Evaluation Loss: 23.4860, Evaluation BLEU: 0.0000\n",
      "Epoch [1814/2000], Evaluation Loss: 23.4859, Evaluation BLEU: 0.0000\n",
      "Epoch [1815/2000], Evaluation Loss: 23.4857, Evaluation BLEU: 0.0000\n",
      "Epoch [1816/2000], Evaluation Loss: 23.4855, Evaluation BLEU: 0.0000\n",
      "Epoch [1817/2000], Evaluation Loss: 23.4854, Evaluation BLEU: 0.0000\n",
      "Epoch [1818/2000], Evaluation Loss: 23.4852, Evaluation BLEU: 0.0000\n",
      "Epoch [1819/2000], Evaluation Loss: 23.4851, Evaluation BLEU: 0.0000\n",
      "Epoch [1820/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1821/2000], Evaluation Loss: 23.4848, Evaluation BLEU: 0.0000\n",
      "Epoch [1822/2000], Evaluation Loss: 23.4848, Evaluation BLEU: 0.0000\n",
      "Epoch [1823/2000], Evaluation Loss: 23.4847, Evaluation BLEU: 0.0000\n",
      "Epoch [1824/2000], Evaluation Loss: 23.4847, Evaluation BLEU: 0.0000\n",
      "Epoch [1825/2000], Evaluation Loss: 23.4846, Evaluation BLEU: 0.0000\n",
      "Epoch [1826/2000], Evaluation Loss: 23.4846, Evaluation BLEU: 0.0000\n",
      "Epoch [1827/2000], Evaluation Loss: 23.4845, Evaluation BLEU: 0.0000\n",
      "Epoch [1828/2000], Evaluation Loss: 23.4845, Evaluation BLEU: 0.0000\n",
      "Epoch [1829/2000], Evaluation Loss: 23.4844, Evaluation BLEU: 0.0000\n",
      "Epoch [1830/2000], Evaluation Loss: 23.4843, Evaluation BLEU: 0.0000\n",
      "Epoch [1831/2000], Evaluation Loss: 23.4843, Evaluation BLEU: 0.0000\n",
      "Epoch [1832/2000], Evaluation Loss: 23.4842, Evaluation BLEU: 0.0000\n",
      "Epoch [1833/2000], Evaluation Loss: 23.4842, Evaluation BLEU: 0.0000\n",
      "Epoch [1834/2000], Evaluation Loss: 23.4842, Evaluation BLEU: 0.0000\n",
      "Epoch [1835/2000], Evaluation Loss: 23.4842, Evaluation BLEU: 0.0000\n",
      "Epoch [1836/2000], Evaluation Loss: 23.4843, Evaluation BLEU: 0.0000\n",
      "Epoch [1837/2000], Evaluation Loss: 23.4844, Evaluation BLEU: 0.0000\n",
      "Epoch [1838/2000], Evaluation Loss: 23.4844, Evaluation BLEU: 0.0000\n",
      "Epoch [1839/2000], Evaluation Loss: 23.4845, Evaluation BLEU: 0.0000\n",
      "Epoch [1840/2000], Evaluation Loss: 23.4847, Evaluation BLEU: 0.0000\n",
      "Epoch [1841/2000], Evaluation Loss: 23.4848, Evaluation BLEU: 0.0000\n",
      "Epoch [1842/2000], Evaluation Loss: 23.4848, Evaluation BLEU: 0.0000\n",
      "Epoch [1843/2000], Evaluation Loss: 23.4849, Evaluation BLEU: 0.0000\n",
      "Epoch [1844/2000], Evaluation Loss: 23.4849, Evaluation BLEU: 0.0000\n",
      "Epoch [1845/2000], Evaluation Loss: 23.4849, Evaluation BLEU: 0.0000\n",
      "Epoch [1846/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1847/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1848/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1849/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1850/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1851/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1852/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1853/2000], Evaluation Loss: 23.4849, Evaluation BLEU: 0.0000\n",
      "Epoch [1854/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1855/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1856/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1857/2000], Evaluation Loss: 23.4851, Evaluation BLEU: 0.0000\n",
      "Epoch [1858/2000], Evaluation Loss: 23.4851, Evaluation BLEU: 0.0000\n",
      "Epoch [1859/2000], Evaluation Loss: 23.4852, Evaluation BLEU: 0.0000\n",
      "Epoch [1860/2000], Evaluation Loss: 23.4853, Evaluation BLEU: 0.0000\n",
      "Epoch [1861/2000], Evaluation Loss: 23.4854, Evaluation BLEU: 0.0000\n",
      "Epoch [1862/2000], Evaluation Loss: 23.4856, Evaluation BLEU: 0.0000\n",
      "Epoch [1863/2000], Evaluation Loss: 23.4857, Evaluation BLEU: 0.0000\n",
      "Epoch [1864/2000], Evaluation Loss: 23.4859, Evaluation BLEU: 0.0000\n",
      "Epoch [1865/2000], Evaluation Loss: 23.4861, Evaluation BLEU: 0.0000\n",
      "Epoch [1866/2000], Evaluation Loss: 23.4863, Evaluation BLEU: 0.0000\n",
      "Epoch [1867/2000], Evaluation Loss: 23.4865, Evaluation BLEU: 0.0000\n",
      "Epoch [1868/2000], Evaluation Loss: 23.4867, Evaluation BLEU: 0.0000\n",
      "Epoch [1869/2000], Evaluation Loss: 23.4869, Evaluation BLEU: 0.0000\n",
      "Epoch [1870/2000], Evaluation Loss: 23.4871, Evaluation BLEU: 0.0000\n",
      "Epoch [1871/2000], Evaluation Loss: 23.4873, Evaluation BLEU: 0.0000\n",
      "Epoch [1872/2000], Evaluation Loss: 23.4875, Evaluation BLEU: 0.0000\n",
      "Epoch [1873/2000], Evaluation Loss: 23.4876, Evaluation BLEU: 0.0000\n",
      "Epoch [1874/2000], Evaluation Loss: 23.4878, Evaluation BLEU: 0.0000\n",
      "Epoch [1875/2000], Evaluation Loss: 23.4880, Evaluation BLEU: 0.0000\n",
      "Epoch [1876/2000], Evaluation Loss: 23.4881, Evaluation BLEU: 0.0000\n",
      "Epoch [1877/2000], Evaluation Loss: 23.4883, Evaluation BLEU: 0.0000\n",
      "Epoch [1878/2000], Evaluation Loss: 23.4884, Evaluation BLEU: 0.0000\n",
      "Epoch [1879/2000], Evaluation Loss: 23.4886, Evaluation BLEU: 0.0000\n",
      "Epoch [1880/2000], Evaluation Loss: 23.4887, Evaluation BLEU: 0.0000\n",
      "Epoch [1881/2000], Evaluation Loss: 23.4889, Evaluation BLEU: 0.0000\n",
      "Epoch [1882/2000], Evaluation Loss: 23.4890, Evaluation BLEU: 0.0000\n",
      "Epoch [1883/2000], Evaluation Loss: 23.4892, Evaluation BLEU: 0.0000\n",
      "Epoch [1884/2000], Evaluation Loss: 23.4893, Evaluation BLEU: 0.0000\n",
      "Epoch [1885/2000], Evaluation Loss: 23.4895, Evaluation BLEU: 0.0000\n",
      "Epoch [1886/2000], Evaluation Loss: 23.4897, Evaluation BLEU: 0.0000\n",
      "Epoch [1887/2000], Evaluation Loss: 23.4899, Evaluation BLEU: 0.0000\n",
      "Epoch [1888/2000], Evaluation Loss: 23.4901, Evaluation BLEU: 0.0000\n",
      "Epoch [1889/2000], Evaluation Loss: 23.4903, Evaluation BLEU: 0.0000\n",
      "Epoch [1890/2000], Evaluation Loss: 23.4905, Evaluation BLEU: 0.0000\n",
      "Epoch [1891/2000], Evaluation Loss: 23.4907, Evaluation BLEU: 0.0000\n",
      "Epoch [1892/2000], Evaluation Loss: 23.4910, Evaluation BLEU: 0.0000\n",
      "Epoch [1893/2000], Evaluation Loss: 23.4912, Evaluation BLEU: 0.0000\n",
      "Epoch [1894/2000], Evaluation Loss: 23.4915, Evaluation BLEU: 0.0000\n",
      "Epoch [1895/2000], Evaluation Loss: 23.4918, Evaluation BLEU: 0.0000\n",
      "Epoch [1896/2000], Evaluation Loss: 23.4921, Evaluation BLEU: 0.0000\n",
      "Epoch [1897/2000], Evaluation Loss: 23.4924, Evaluation BLEU: 0.0000\n",
      "Epoch [1898/2000], Evaluation Loss: 23.4927, Evaluation BLEU: 0.0000\n",
      "Epoch [1899/2000], Evaluation Loss: 23.4930, Evaluation BLEU: 0.0000\n",
      "Epoch [1900/2000], Evaluation Loss: 23.4932, Evaluation BLEU: 0.0000\n",
      "Epoch [1901/2000], Evaluation Loss: 23.4935, Evaluation BLEU: 0.0000\n",
      "Epoch [1902/2000], Evaluation Loss: 23.4937, Evaluation BLEU: 0.0000\n",
      "Epoch [1903/2000], Evaluation Loss: 23.4940, Evaluation BLEU: 0.0000\n",
      "Epoch [1904/2000], Evaluation Loss: 23.4942, Evaluation BLEU: 0.0000\n",
      "Epoch [1905/2000], Evaluation Loss: 23.4944, Evaluation BLEU: 0.0000\n",
      "Epoch [1906/2000], Evaluation Loss: 23.4946, Evaluation BLEU: 0.0000\n",
      "Epoch [1907/2000], Evaluation Loss: 23.4949, Evaluation BLEU: 0.0000\n",
      "Epoch [1908/2000], Evaluation Loss: 23.4952, Evaluation BLEU: 0.0000\n",
      "Epoch [1909/2000], Evaluation Loss: 23.4954, Evaluation BLEU: 0.0000\n",
      "Epoch [1910/2000], Evaluation Loss: 23.4957, Evaluation BLEU: 0.0000\n",
      "Epoch [1911/2000], Evaluation Loss: 23.4960, Evaluation BLEU: 0.0000\n",
      "Epoch [1912/2000], Evaluation Loss: 23.4963, Evaluation BLEU: 0.0000\n",
      "Epoch [1913/2000], Evaluation Loss: 23.4966, Evaluation BLEU: 0.0000\n",
      "Epoch [1914/2000], Evaluation Loss: 23.4967, Evaluation BLEU: 0.0000\n",
      "Epoch [1915/2000], Evaluation Loss: 23.4969, Evaluation BLEU: 0.0000\n",
      "Epoch [1916/2000], Evaluation Loss: 23.4970, Evaluation BLEU: 0.0000\n",
      "Epoch [1917/2000], Evaluation Loss: 23.4971, Evaluation BLEU: 0.0000\n",
      "Epoch [1918/2000], Evaluation Loss: 23.4972, Evaluation BLEU: 0.0000\n",
      "Epoch [1919/2000], Evaluation Loss: 23.4973, Evaluation BLEU: 0.0000\n",
      "Epoch [1920/2000], Evaluation Loss: 23.4973, Evaluation BLEU: 0.0000\n",
      "Epoch [1921/2000], Evaluation Loss: 23.4974, Evaluation BLEU: 0.0000\n",
      "Epoch [1922/2000], Evaluation Loss: 23.4974, Evaluation BLEU: 0.0000\n",
      "Epoch [1923/2000], Evaluation Loss: 23.4974, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1924/2000], Evaluation Loss: 23.4975, Evaluation BLEU: 0.0000\n",
      "Epoch [1925/2000], Evaluation Loss: 23.4975, Evaluation BLEU: 0.0000\n",
      "Epoch [1926/2000], Evaluation Loss: 23.4975, Evaluation BLEU: 0.0000\n",
      "Epoch [1927/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1928/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1929/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1930/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1931/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1932/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1933/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1934/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1935/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1936/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1937/2000], Evaluation Loss: 23.4977, Evaluation BLEU: 0.0000\n",
      "Epoch [1938/2000], Evaluation Loss: 23.4977, Evaluation BLEU: 0.0000\n",
      "Epoch [1939/2000], Evaluation Loss: 23.4978, Evaluation BLEU: 0.0000\n",
      "Epoch [1940/2000], Evaluation Loss: 23.4978, Evaluation BLEU: 0.0000\n",
      "Epoch [1941/2000], Evaluation Loss: 23.4979, Evaluation BLEU: 0.0000\n",
      "Epoch [1942/2000], Evaluation Loss: 23.4980, Evaluation BLEU: 0.0000\n",
      "Epoch [1943/2000], Evaluation Loss: 23.4981, Evaluation BLEU: 0.0000\n",
      "Epoch [1944/2000], Evaluation Loss: 23.4983, Evaluation BLEU: 0.0000\n",
      "Epoch [1945/2000], Evaluation Loss: 23.4984, Evaluation BLEU: 0.0000\n",
      "Epoch [1946/2000], Evaluation Loss: 23.4986, Evaluation BLEU: 0.0000\n",
      "Epoch [1947/2000], Evaluation Loss: 23.4988, Evaluation BLEU: 0.0000\n",
      "Epoch [1948/2000], Evaluation Loss: 23.4990, Evaluation BLEU: 0.0000\n",
      "Epoch [1949/2000], Evaluation Loss: 23.4992, Evaluation BLEU: 0.0000\n",
      "Epoch [1950/2000], Evaluation Loss: 23.4994, Evaluation BLEU: 0.0000\n",
      "Epoch [1951/2000], Evaluation Loss: 23.4996, Evaluation BLEU: 0.0000\n",
      "Epoch [1952/2000], Evaluation Loss: 23.4997, Evaluation BLEU: 0.0000\n",
      "Epoch [1953/2000], Evaluation Loss: 23.4998, Evaluation BLEU: 0.0000\n",
      "Epoch [1954/2000], Evaluation Loss: 23.4999, Evaluation BLEU: 0.0000\n",
      "Epoch [1955/2000], Evaluation Loss: 23.5000, Evaluation BLEU: 0.0000\n",
      "Epoch [1956/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1957/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1958/2000], Evaluation Loss: 23.5003, Evaluation BLEU: 0.0000\n",
      "Epoch [1959/2000], Evaluation Loss: 23.5003, Evaluation BLEU: 0.0000\n",
      "Epoch [1960/2000], Evaluation Loss: 23.5003, Evaluation BLEU: 0.0000\n",
      "Epoch [1961/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1962/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1963/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1964/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1965/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1966/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1967/2000], Evaluation Loss: 23.5001, Evaluation BLEU: 0.0000\n",
      "Epoch [1968/2000], Evaluation Loss: 23.5001, Evaluation BLEU: 0.0000\n",
      "Epoch [1969/2000], Evaluation Loss: 23.5000, Evaluation BLEU: 0.0000\n",
      "Epoch [1970/2000], Evaluation Loss: 23.5000, Evaluation BLEU: 0.0000\n",
      "Epoch [1971/2000], Evaluation Loss: 23.5000, Evaluation BLEU: 0.0000\n",
      "Epoch [1972/2000], Evaluation Loss: 23.5000, Evaluation BLEU: 0.0000\n",
      "Epoch [1973/2000], Evaluation Loss: 23.5001, Evaluation BLEU: 0.0000\n",
      "Epoch [1974/2000], Evaluation Loss: 23.5001, Evaluation BLEU: 0.0000\n",
      "Epoch [1975/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1976/2000], Evaluation Loss: 23.5003, Evaluation BLEU: 0.0000\n",
      "Epoch [1977/2000], Evaluation Loss: 23.5004, Evaluation BLEU: 0.0000\n",
      "Epoch [1978/2000], Evaluation Loss: 23.5005, Evaluation BLEU: 0.0000\n",
      "Epoch [1979/2000], Evaluation Loss: 23.5007, Evaluation BLEU: 0.0000\n",
      "Epoch [1980/2000], Evaluation Loss: 23.5009, Evaluation BLEU: 0.0000\n",
      "Epoch [1981/2000], Evaluation Loss: 23.5011, Evaluation BLEU: 0.0000\n",
      "Epoch [1982/2000], Evaluation Loss: 23.5013, Evaluation BLEU: 0.0000\n",
      "Epoch [1983/2000], Evaluation Loss: 23.5016, Evaluation BLEU: 0.0000\n",
      "Epoch [1984/2000], Evaluation Loss: 23.5019, Evaluation BLEU: 0.0000\n",
      "Epoch [1985/2000], Evaluation Loss: 23.5022, Evaluation BLEU: 0.0000\n",
      "Epoch [1986/2000], Evaluation Loss: 23.5025, Evaluation BLEU: 0.0000\n",
      "Epoch [1987/2000], Evaluation Loss: 23.5028, Evaluation BLEU: 0.0000\n",
      "Epoch [1988/2000], Evaluation Loss: 23.5032, Evaluation BLEU: 0.0000\n",
      "Epoch [1989/2000], Evaluation Loss: 23.5035, Evaluation BLEU: 0.0000\n",
      "Epoch [1990/2000], Evaluation Loss: 23.5039, Evaluation BLEU: 0.0000\n",
      "Epoch [1991/2000], Evaluation Loss: 23.5043, Evaluation BLEU: 0.0000\n",
      "Epoch [1992/2000], Evaluation Loss: 23.5046, Evaluation BLEU: 0.0000\n",
      "Epoch [1993/2000], Evaluation Loss: 23.5050, Evaluation BLEU: 0.0000\n",
      "Epoch [1994/2000], Evaluation Loss: 23.5054, Evaluation BLEU: 0.0000\n",
      "Epoch [1995/2000], Evaluation Loss: 23.5058, Evaluation BLEU: 0.0000\n",
      "Epoch [1996/2000], Evaluation Loss: 23.5062, Evaluation BLEU: 0.0000\n",
      "Epoch [1997/2000], Evaluation Loss: 23.5066, Evaluation BLEU: 0.0000\n",
      "Epoch [1998/2000], Evaluation Loss: 23.5071, Evaluation BLEU: 0.0000\n",
      "Epoch [1999/2000], Evaluation Loss: 23.5075, Evaluation BLEU: 0.0000\n",
      "Epoch [2000/2000], Evaluation Loss: 23.5079, Evaluation BLEU: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "\n",
    "# constants\n",
    "TRAIN_BATCH_SIZE = 3\n",
    "N_EPOCHS = 2000\n",
    "max_token_len = 80\n",
    "LOG_EVERY = 10000\n",
    "\n",
    "# encode source and target data using encode_batch function\n",
    "source_train_data_encoded = encode_batch((src_train))\n",
    "target_train_data_encoded = encode_batch((tgt_train))\n",
    "\n",
    "# Initialize models\n",
    "bert_model = BertModel.from_pretrained('bert-base-cased')\n",
    "gpt2_model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Configuration\n",
    "max_token_len = 80\n",
    "start_token_id = bert_tokenizer.cls_token_id\n",
    "end_token_id = gpt2_tokenizer.eos_token_id\n",
    "\n",
    "# Initialize the Encoder-Decoder model with cross-attention enabled\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')\n",
    "\n",
    "# Update configuration for the model\n",
    "model.config.decoder_start_token_id = start_token_id\n",
    "model.config.eos_token_id = end_token_id\n",
    "model.config.max_length = max_token_len\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.add_cross_attention = True  # Enable cross-attention\n",
    "\n",
    "# Loss function and optimizer setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, len(source_train_data_encoded[0]), TRAIN_BATCH_SIZE):\n",
    "        batch_source = source_train_data_encoded[0][i:i+TRAIN_BATCH_SIZE]\n",
    "        batch_target = target_train_data_encoded[0][i:i+TRAIN_BATCH_SIZE]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids=batch_source, decoder_input_ids=batch_target)\n",
    "\n",
    "        logits_flat = outputs.logits.view(-1, outputs.logits.size(-1))\n",
    "        target_flat = batch_target.view(-1)\n",
    "\n",
    "        loss = criterion(logits_flat, target_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % LOG_EVERY == 0 and i > 0:\n",
    "            print(f'Epoch [{epoch + 1}/{N_EPOCHS}], Batch [{i + 1}/{len(source_train_data_encoded[0])}], Loss: {running_loss / LOG_EVERY:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Evaluate\n",
    "    val_loss, val_bleu = evaluate_model(model, eval_data_loader, criterion, gpt2_tokenizer)\n",
    "    print(f'Epoch [{epoch + 1}/{N_EPOCHS}], Evaluation Loss: {val_loss:.4f}, Evaluation BLEU: {val_bleu:.4f}')\n",
    "    #val_loss, val_bleu, val_sari = evaluate_model(model, eval_data_loader, criterion, gpt2_tokenizer, ref_sentences[0])\n",
    "    #print(f'Epoch [{epoch + 1}/{N_EPOCHS}], Evaluation Loss: {val_loss:.4f}, Evaluation BLEU: {val_bleu:.4f}, Evaluation SARI: {val_sari:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARIngram(sgrams, cgrams, rgramslist, numref):\n",
    "    rgramsall = [rgram for rgrams in rgramslist for rgram in rgrams]\n",
    "    rgramcounter = Counter(rgramsall)\n",
    "\t\n",
    "    sgramcounter = Counter(sgrams)\n",
    "    sgramcounter_rep = Counter()\n",
    "    for sgram, scount in sgramcounter.items():\n",
    "        sgramcounter_rep[sgram] = scount * numref\n",
    "        \n",
    "    cgramcounter = Counter(cgrams)\n",
    "    cgramcounter_rep = Counter()\n",
    "    for cgram, ccount in cgramcounter.items():\n",
    "        cgramcounter_rep[cgram] = ccount * numref\n",
    "\t\n",
    "    \n",
    "    # KEEP\n",
    "    keepgramcounter_rep = sgramcounter_rep & cgramcounter_rep\n",
    "    keepgramcountergood_rep = keepgramcounter_rep & rgramcounter\n",
    "    keepgramcounterall_rep = sgramcounter_rep & rgramcounter\n",
    "\n",
    "    keeptmpscore1 = 0\n",
    "    keeptmpscore2 = 0\n",
    "    for keepgram in keepgramcountergood_rep:\n",
    "        keeptmpscore1 += keepgramcountergood_rep[keepgram] / keepgramcounter_rep[keepgram]\n",
    "        keeptmpscore2 += keepgramcountergood_rep[keepgram] / keepgramcounterall_rep[keepgram]\n",
    "        #print \"KEEP\", keepgram, keepscore, cgramcounter[keepgram], sgramcounter[keepgram], rgramcounter[keepgram]\n",
    "    keepscore_precision = 0\n",
    "    if len(keepgramcounter_rep) > 0:\n",
    "    \tkeepscore_precision = keeptmpscore1 / len(keepgramcounter_rep)\n",
    "    keepscore_recall = 0\n",
    "    if len(keepgramcounterall_rep) > 0:\n",
    "    \tkeepscore_recall = keeptmpscore2 / len(keepgramcounterall_rep)\n",
    "    keepscore = 0\n",
    "    if keepscore_precision > 0 or keepscore_recall > 0:\n",
    "        keepscore = 2 * keepscore_precision * keepscore_recall / (keepscore_precision + keepscore_recall)\n",
    "\n",
    "\n",
    "    # DELETION\n",
    "    delgramcounter_rep = sgramcounter_rep - cgramcounter_rep\n",
    "    delgramcountergood_rep = delgramcounter_rep - rgramcounter\n",
    "    delgramcounterall_rep = sgramcounter_rep - rgramcounter\n",
    "    deltmpscore1 = 0\n",
    "    deltmpscore2 = 0\n",
    "    for delgram in delgramcountergood_rep:\n",
    "        deltmpscore1 += delgramcountergood_rep[delgram] / delgramcounter_rep[delgram]\n",
    "        deltmpscore2 += delgramcountergood_rep[delgram] / delgramcounterall_rep[delgram]\n",
    "    delscore_precision = 0\n",
    "    if len(delgramcounter_rep) > 0:\n",
    "    \tdelscore_precision = deltmpscore1 / len(delgramcounter_rep)\n",
    "    delscore_recall = 0\n",
    "    if len(delgramcounterall_rep) > 0:\n",
    "    \tdelscore_recall = deltmpscore1 / len(delgramcounterall_rep)\n",
    "    delscore = 0\n",
    "    if delscore_precision > 0 or delscore_recall > 0:\n",
    "        delscore = 2 * delscore_precision * delscore_recall / (delscore_precision + delscore_recall)\n",
    "\n",
    "\n",
    "    # ADDITION\n",
    "    addgramcounter = set(cgramcounter) - set(sgramcounter)\n",
    "    addgramcountergood = set(addgramcounter) & set(rgramcounter)\n",
    "    addgramcounterall = set(rgramcounter) - set(sgramcounter)\n",
    "\n",
    "    addtmpscore = 0\n",
    "    for addgram in addgramcountergood:\n",
    "        addtmpscore += 1\n",
    "\n",
    "    addscore_precision = 0\n",
    "    addscore_recall = 0\n",
    "    if len(addgramcounter) > 0:\n",
    "    \taddscore_precision = addtmpscore / len(addgramcounter)\n",
    "    if len(addgramcounterall) > 0:\n",
    "    \taddscore_recall = addtmpscore / len(addgramcounterall)\n",
    "    addscore = 0\n",
    "    if addscore_precision > 0 or addscore_recall > 0:\n",
    "        addscore = 2 * addscore_precision * addscore_recall / (addscore_precision + addscore_recall)\n",
    "    \n",
    "    return (keepscore, delscore_precision, addscore)\n",
    "    \n",
    "\n",
    "def SARIsent (ssent, csent, rsents) :\n",
    "    numref = len(rsents)\t\n",
    "\n",
    "    s1grams = ssent.lower().split(\" \")\n",
    "    c1grams = csent.lower().split(\" \")\n",
    "    s2grams = []\n",
    "    c2grams = []\n",
    "    s3grams = []\n",
    "    c3grams = []\n",
    "    s4grams = []\n",
    "    c4grams = []\n",
    " \n",
    "    r1gramslist = []\n",
    "    r2gramslist = []\n",
    "    r3gramslist = []\n",
    "    r4gramslist = []\n",
    "    for rsent in rsents:\n",
    "        r1grams = rsent.lower().split(\" \")    \n",
    "        r2grams = []\n",
    "        r3grams = []\n",
    "        r4grams = []\n",
    "        r1gramslist.append(r1grams)\n",
    "        for i in range(0, len(r1grams)-1) :\n",
    "            if i < len(r1grams) - 1:\n",
    "                r2gram = r1grams[i] + \" \" + r1grams[i+1]\n",
    "                r2grams.append(r2gram)\n",
    "            if i < len(r1grams)-2:\n",
    "                r3gram = r1grams[i] + \" \" + r1grams[i+1] + \" \" + r1grams[i+2]\n",
    "                r3grams.append(r3gram)\n",
    "            if i < len(r1grams)-3:\n",
    "                r4gram = r1grams[i] + \" \" + r1grams[i+1] + \" \" + r1grams[i+2] + \" \" + r1grams[i+3]\n",
    "                r4grams.append(r4gram)        \n",
    "        r2gramslist.append(r2grams)\n",
    "        r3gramslist.append(r3grams)\n",
    "        r4gramslist.append(r4grams)\n",
    "       \n",
    "    for i in range(0, len(s1grams)-1) :\n",
    "        if i < len(s1grams) - 1:\n",
    "            s2gram = s1grams[i] + \" \" + s1grams[i+1]\n",
    "            s2grams.append(s2gram)\n",
    "        if i < len(s1grams)-2:\n",
    "            s3gram = s1grams[i] + \" \" + s1grams[i+1] + \" \" + s1grams[i+2]\n",
    "            s3grams.append(s3gram)\n",
    "        if i < len(s1grams)-3:\n",
    "            s4gram = s1grams[i] + \" \" + s1grams[i+1] + \" \" + s1grams[i+2] + \" \" + s1grams[i+3]\n",
    "            s4grams.append(s4gram)\n",
    "            \n",
    "    for i in range(0, len(c1grams)-1) :\n",
    "        if i < len(c1grams) - 1:\n",
    "            c2gram = c1grams[i] + \" \" + c1grams[i+1]\n",
    "            c2grams.append(c2gram)\n",
    "        if i < len(c1grams)-2:\n",
    "            c3gram = c1grams[i] + \" \" + c1grams[i+1] + \" \" + c1grams[i+2]\n",
    "            c3grams.append(c3gram)\n",
    "        if i < len(c1grams)-3:\n",
    "            c4gram = c1grams[i] + \" \" + c1grams[i+1] + \" \" + c1grams[i+2] + \" \" + c1grams[i+3]\n",
    "            c4grams.append(c4gram)\n",
    "\n",
    "\n",
    "    (keep1score, del1score, add1score) = SARIngram(s1grams, c1grams, r1gramslist, numref)\n",
    "    (keep2score, del2score, add2score) = SARIngram(s2grams, c2grams, r2gramslist, numref)\n",
    "    (keep3score, del3score, add3score) = SARIngram(s3grams, c3grams, r3gramslist, numref)\n",
    "    (keep4score, del4score, add4score) = SARIngram(s4grams, c4grams, r4gramslist, numref)\n",
    "    avgkeepscore = sum([keep1score,keep2score,keep3score,keep4score])/4\n",
    "    avgdelscore = sum([del1score,del2score,del3score,del4score])/4\n",
    "    avgaddscore = sum([add1score,add2score,add3score,add4score])/4\n",
    "    finalscore = (avgkeepscore + avgdelscore + avgaddscore ) / 3\n",
    "\n",
    "    return finalscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from collections import Counter\n",
    "\n",
    "# BLEU score fore evaluation\n",
    "def compute_bleu_score(logits, labels):\n",
    "    refs = get_sent_tokens(labels)\n",
    "    weights = (1.0/2.0, 1.0/2.0, )\n",
    "    score = corpus_bleu(refs, logits.tolist(), smoothing_function=SmoothingFunction(epsilon=1e-10).method1, weights=weights)\n",
    "    return score\n",
    "\n",
    "def compute_sari(norm, pred_tensor, ref):\n",
    "    pred = decode_sent_tokens(pred_tensor)\n",
    "    score = 0\n",
    "    for step, item in enumerate(ref):\n",
    "        score += sari.SARIsent(norm[step], pred[step], item)\n",
    "    return score/TRAIN_BATCH_SIZE\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, target_tokenizer, ref_sentences):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch_source = batch[0]\n",
    "            batch_target = batch[1]\n",
    "\n",
    "            outputs = model(input_ids=batch_source, decoder_input_ids=batch_target)\n",
    "\n",
    "            logits_flat = outputs.logits.view(-1, outputs.logits.size(-1))\n",
    "            target_flat = batch_target.view(-1)\n",
    "\n",
    "            loss = criterion(logits_flat, target_flat)\n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "\n",
    "            predicted_ids = outputs.logits.argmax(-1)\n",
    "            predicted_sentences = [target_tokenizer.decode(ids, skip_special_tokens=True) for ids in predicted_ids]\n",
    "            target_sentences = [target_tokenizer.decode(ids, skip_special_tokens=True) for ids in batch_target]\n",
    "\n",
    "            references.extend([sent.split() for sent in target_sentences])\n",
    "            hypotheses.extend([sent.split() for sent in predicted_sentences])\n",
    "\n",
    "    avg_loss = total_loss / total_batches\n",
    "\n",
    "    # Calculate BLEU score using your function\n",
    "    #bleu_score = compute_bleu_score(torch.argmax(outputs, dim=-1), batch[1])\n",
    "    logits = outputs.logits  # Extract the logits from Seq2SeqLMOutput\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)  # Get the predicted IDs\n",
    "    bleu_score = compute_bleu_score(predicted_ids, batch[1])  # Compute BLEU score\n",
    "    \n",
    "    # Calculate SARI score\n",
    "    sari_scores = []\n",
    "    for idx, gen_sent in enumerate(hypotheses):\n",
    "        sari_score = compute_sari(ref_sentences[idx], gen_sent)\n",
    "        sari_scores.append(sari_score)\n",
    "    \n",
    "    avg_sari_score = sum(sari_scores) / len(sari_scores)\n",
    "\n",
    "    return avg_loss, bleu_score, avg_sari_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.3.crossattention.c_attn.weight', 'h.5.ln_cross_attn.weight', 'h.6.ln_cross_attn.weight', 'h.9.crossattention.q_attn.bias', 'h.9.ln_cross_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.6.crossattention.q_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.3.ln_cross_attn.bias', 'h.7.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.2.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.bias', 'h.11.crossattention.q_attn.bias', 'h.4.ln_cross_attn.weight', 'h.0.ln_cross_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.7.crossattention.q_attn.bias', 'h.5.crossattention.c_proj.bias', 'h.1.ln_cross_attn.weight', 'h.4.crossattention.q_attn.bias', 'h.1.crossattention.c_proj.weight', 'h.10.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.4.ln_cross_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.bias', 'h.11.crossattention.c_proj.weight', 'h.6.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.4.crossattention.c_proj.weight', 'h.5.crossattention.c_attn.bias', 'h.7.crossattention.c_proj.bias', 'h.10.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.8.ln_cross_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.5.ln_cross_attn.bias', 'h.9.ln_cross_attn.bias', 'h.1.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.2.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.weight', 'h.0.crossattention.c_proj.weight', 'h.8.crossattention.c_attn.bias', 'h.2.ln_cross_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.11.ln_cross_attn.bias', 'h.1.crossattention.c_attn.bias', 'h.8.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.0.ln_cross_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.1.crossattention.q_attn.weight', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.weight', 'h.10.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.1.crossattention.q_attn.bias', 'h.7.crossattention.c_proj.weight', 'h.10.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.9.crossattention.c_proj.bias', 'h.7.ln_cross_attn.bias', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.6.crossattention.c_proj.weight', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.11.ln_cross_attn.weight', 'h.8.crossattention.q_attn.weight', 'h.5.crossattention.q_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.8.crossattention.q_attn.bias', 'h.8.ln_cross_attn.bias', 'h.8.crossattention.c_proj.bias', 'h.6.crossattention.q_attn.bias', 'h.10.ln_cross_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.11.crossattention.c_attn.weight', 'h.8.crossattention.c_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [70], line 62\u001b[0m\n\u001b[1;32m     57\u001b[0m         running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m#val_loss, val_bleu = evaluate_model(model, eval_data_loader, criterion, gpt2_tokenizer)\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#print(f'Epoch [{epoch + 1}/{N_EPOCHS}], Evaluation Loss: {val_loss:.4f}, Evaluation BLEU: {val_bleu:.4f}')\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m val_loss, val_bleu, val_sari \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpt2_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref_sentences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Evaluation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Evaluation BLEU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_bleu\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Evaluation SARI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_sari\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [69], line 52\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, data_loader, criterion, target_tokenizer, ref_sentences)\u001b[0m\n\u001b[1;32m     50\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits  \u001b[38;5;66;03m# Extract the logits from Seq2SeqLMOutput\u001b[39;00m\n\u001b[1;32m     51\u001b[0m predicted_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Get the predicted IDs\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m bleu_score \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_bleu_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Compute BLEU score\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Calculate SARI score\u001b[39;00m\n\u001b[1;32m     55\u001b[0m sari_scores \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn [69], line 6\u001b[0m, in \u001b[0;36mcompute_bleu_score\u001b[0;34m(logits, labels)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_bleu_score\u001b[39m(logits, labels):\n\u001b[0;32m----> 6\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[43mget_sent_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     weights \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2.0\u001b[39m, )\n\u001b[1;32m      8\u001b[0m     score \u001b[38;5;241m=\u001b[39m corpus_bleu(refs, logits\u001b[38;5;241m.\u001b[39mtolist(), smoothing_function\u001b[38;5;241m=\u001b[39mSmoothingFunction(epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-10\u001b[39m)\u001b[38;5;241m.\u001b[39mmethod1, weights\u001b[38;5;241m=\u001b[39mweights)\n",
      "Cell \u001b[0;32mIn [3], line 86\u001b[0m, in \u001b[0;36mget_sent_tokens\u001b[0;34m(sents)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03mThis function tokenizes input sentences and prepares them for downstream processing, perhaps for model input or further manipulation.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03mThe function returns a list (ref) containing lists of tokenized sentences, where each inner list represents the tokenized form of a sentence from the input.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     85\u001b[0m ref \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 86\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mgpt2_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlongest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m tokens\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mtolist():\n\u001b[1;32m     91\u001b[0m     ref\u001b[38;5;241m.\u001b[39mappend([tok])\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2806\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2804\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2805\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2806\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2808\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2864\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2861\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2863\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2864\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2867\u001b[0m     )\n\u001b[1;32m   2869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2870\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2872\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2873\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "\n",
    "# constants\n",
    "TRAIN_BATCH_SIZE = 3\n",
    "N_EPOCHS = 20\n",
    "max_token_len = 80\n",
    "LOG_EVERY = 10000\n",
    "\n",
    "# encode source and target data using encode_batch function\n",
    "source_train_data_encoded = encode_batch((src_train))\n",
    "target_train_data_encoded = encode_batch((tgt_train))\n",
    "\n",
    "# Initialize models\n",
    "bert_model = BertModel.from_pretrained('bert-base-cased')\n",
    "gpt2_model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Configuration\n",
    "max_token_len = 80\n",
    "start_token_id = bert_tokenizer.cls_token_id\n",
    "end_token_id = gpt2_tokenizer.eos_token_id\n",
    "\n",
    "# Initialize the Encoder-Decoder model with cross-attention enabled\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')\n",
    "\n",
    "# Update configuration for the model\n",
    "model.config.decoder_start_token_id = start_token_id\n",
    "model.config.eos_token_id = end_token_id\n",
    "model.config.max_length = max_token_len\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.add_cross_attention = True  # Enable cross-attention\n",
    "\n",
    "# Loss function and optimizer setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, len(source_train_data_encoded[0]), TRAIN_BATCH_SIZE):\n",
    "        batch_source = source_train_data_encoded[0][i:i+TRAIN_BATCH_SIZE]\n",
    "        batch_target = target_train_data_encoded[0][i:i+TRAIN_BATCH_SIZE]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids=batch_source, decoder_input_ids=batch_target)\n",
    "\n",
    "        logits_flat = outputs.logits.view(-1, outputs.logits.size(-1))\n",
    "        target_flat = batch_target.view(-1)\n",
    "\n",
    "        loss = criterion(logits_flat, target_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % LOG_EVERY == 0 and i > 0:\n",
    "            print(f'Epoch [{epoch + 1}/{N_EPOCHS}], Batch [{i + 1}/{len(source_train_data_encoded[0])}], Loss: {running_loss / LOG_EVERY:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Evaluate\n",
    "    #val_loss, val_bleu = evaluate_model(model, eval_data_loader, criterion, gpt2_tokenizer)\n",
    "    #print(f'Epoch [{epoch + 1}/{N_EPOCHS}], Evaluation Loss: {val_loss:.4f}, Evaluation BLEU: {val_bleu:.4f}')\n",
    "    val_loss, val_bleu, val_sari = evaluate_model(model, eval_data_loader, criterion, gpt2_tokenizer, ref_sentences[0])\n",
    "    print(f'Epoch [{epoch + 1}/{N_EPOCHS}], Evaluation Loss: {val_loss:.4f}, Evaluation BLEU: {val_bleu:.4f}, Evaluation SARI: {val_sari:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader, e_loss):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    eval_loss = e_loss\n",
    "    bleu_score = 0\n",
    "    sari_score = 0\n",
    "    softmax = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(data_loader):\n",
    "            src_tensors, src_attn_tensors, tgt_tensors, tgt_attn_tensors, labels = tokenizer.encode_batch(batch)\n",
    "            loss, logits = model(input_ids = src_tensors.to(device), \n",
    "                            decoder_input_ids = tgt_tensors.to(device),\n",
    "                            attention_mask = src_attn_tensors.to(device),\n",
    "                            decoder_attention_mask = tgt_attn_tensors.to(device),\n",
    "                            labels = labels.to(device))[:2]\n",
    "            outputs = softmax(logits)\n",
    "            score = compute_bleu_score(torch.argmax(outputs, dim=-1), batch[1])\n",
    "            s_score = compute_sari(batch[0], torch.argmax(outputs, dim=-1), batch[2])\n",
    "            if step == 0:\n",
    "                eval_loss = loss.item()\n",
    "                bleu_score = score\n",
    "                sari_score = s_score\n",
    "            else:\n",
    "                eval_loss = (1/2.0)*(eval_loss + loss.item())\n",
    "                bleu_score = (1/2.0)* (bleu_score+score)\n",
    "                sari_score = (1/2.0)* (sari_score+s_score)\n",
    "        \n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return eval_loss, bleu_score, sari_score\n",
    "\n",
    "def load_checkpt(checkpt_path, optimizer=None):\n",
    "    checkpoint = torch.load(checkpt_path)\n",
    "    if device == \"cpu\":\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"], map_location=torch.device(\"cpu\"))\n",
    "        if optimizer is not None:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"], map_location=torch.device(\"cpu\"))\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        if optimizer is not None:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    eval_loss = checkpoint[\"eval_loss\"]\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "    return optimizer, eval_loss, epoch\n",
    "\n",
    "def save_model_checkpt(state, is_best, check_pt_path, best_model_path):\n",
    "    f_path = check_pt_path\n",
    "    torch.save(state, f_path)\n",
    "\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(start_epoch, eval_loss, loaders, optimizer, check_pt_path, best_model_path):\n",
    "    best_eval_loss = eval_loss\n",
    "    print(\"Model training started...\")\n",
    "    for epoch in range(start_epoch, N_EPOCH):\n",
    "        print(f\"Epoch {epoch} running...\")\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_train_loss = 0\n",
    "        epoch_eval_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(loaders[0]):\n",
    "            src_tensors, src_attn_tensors, tgt_tensors, tgt_attn_tensors, labels = tokenizer.encode_batch(batch)\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            loss = model(input_ids = src_tensors.to(device), \n",
    "                            decoder_input_ids = tgt_tensors.to(device),\n",
    "                            attention_mask = src_attn_tensors.to(device),\n",
    "                            decoder_attention_mask = tgt_attn_tensors.to(device),\n",
    "                            labels = labels.to(device))[0]\n",
    "            if step == 0:\n",
    "                epoch_train_loss = loss.item()\n",
    "            else:\n",
    "                epoch_train_loss = (1/2.0)*(epoch_train_loss + loss.item())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (step+1) % LOG_EVERY == 0:\n",
    "                print(f'Epoch: {epoch} | iter: {step+1} | avg. train loss: {epoch_train_loss} | time elapsed: {time.time() - epoch_start_time}')\n",
    "                logging.info(f'Epoch: {epoch} | iter: {step+1} | avg. train loss: {epoch_train_loss} | time elapsed: {time.time() - epoch_start_time}')\n",
    "        \n",
    "        eval_start_time = time.time()\n",
    "        epoch_eval_loss, bleu_score, sari_score = evaluate(loaders[1], epoch_eval_loss)\n",
    "        epoch_eval_loss = epoch_eval_loss/TRAIN_BATCH_SIZE\n",
    "        print(f'Completed Epoch: {epoch} | avg. eval loss: {epoch_eval_loss:.5f} | blue score: {bleu_score} | Sari score: {sari_score} | time elapsed: {time.time() - eval_start_time}')\n",
    "        logging.info(f'Completed Epoch: {epoch} | avg. eval loss: {epoch_eval_loss:.5f} | blue score: {bleu_score}| Sari score: {sari_score} | time elapsed: {time.time() - eval_start_time}')\n",
    "\n",
    "        check_pt = {\n",
    "            'epoch': epoch+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'eval_loss': epoch_eval_loss,\n",
    "            'sari_score': sari_score,\n",
    "            'bleu_score': bleu_score\n",
    "        }\n",
    "        check_pt_time = time.time()\n",
    "        print(\"Saving Checkpoint.......\")\n",
    "        if epoch_eval_loss < best_eval_loss:\n",
    "            print(\"New best model found\")\n",
    "            logging.info(f\"New best model found\")\n",
    "            best_eval_loss = epoch_eval_loss\n",
    "            save_model_checkpt(check_pt, True, check_pt_path, best_model_path)\n",
    "        else:\n",
    "            save_model_checkpt(check_pt, False, check_pt_path, best_model_path)  \n",
    "        print(f\"Checkpoint saved successfully with time: {time.time() - check_pt_time}\")\n",
    "        logging.info(f\"Checkpoint saved successfully with time: {time.time() - check_pt_time}\")\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
