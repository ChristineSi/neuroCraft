{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import datasets\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from utils import *\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import RobertaForMaskedLM\n",
    "from transformers import LineByLineTextDataset\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET DATA ###############################################################################################################\n",
    "\n",
    "# initialize\n",
    "def initialize_data(mode='train', dir='../dataset'):\n",
    "    data = {}\n",
    "    data['dir'] = dir\n",
    "    data['mode'] = mode\n",
    "    data['src'] = read_data(f'src_{mode}.txt', dir)\n",
    "    data['tgt'] = read_data(f'tgt_{mode}.txt', dir)\n",
    "\n",
    "    if mode != 'train':\n",
    "        data['pkl'] = read_data(f'ref_{mode}.pkl', dir, pkl=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "# assert length\n",
    "def get_data_length(data):\n",
    "    assert len(data['src']) == len(data['tgt'])\n",
    "    return len(data['src'])\n",
    "\n",
    "# het item\n",
    "def get_item(data, idx):\n",
    "    if data['mode'] == 'train':\n",
    "        return data['src'][idx], data['tgt'][idx]\n",
    "    return data['src'][idx], data['tgt'][idx], data['pkl'][idx]\n",
    "\n",
    "# read data\n",
    "def read_data(file, dir, pkl=False):\n",
    "    if pkl:\n",
    "        return pickle.load(open(f'{dir}/{file}', 'rb'))\n",
    "\n",
    "    with open(f'{dir}/{file}', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        return list(map(lambda x: x.strip(), lines))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data loader\n",
    "def get_dataloader(batch_size):\n",
    "    train_data = initialize_data(mode='train')\n",
    "    val_data = initialize_data(mode='valid')\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        CustomDataset(train_data),  # Replace CustomDataset with your Dataset creation function\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        CustomDataset(val_data),  # Replace CustomDataset with your Dataset creation function\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=4\n",
    "    )\n",
    "    return iter(train_loader), iter(val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data loader\n",
    "def get_testloader(batch_size):\n",
    "    test_data = initialize_data(mode='test')\n",
    "    test_loader = DataLoader(\n",
    "        CustomDataset(test_data),  # Replace CustomDataset with your Dataset creation function\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return iter(test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_batch(encoderTokenizer, decoderTokenizer, src, tgt, max_len=100):\n",
    "    src_tok = encoderTokenizer(\n",
    "        src,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    tgt_tok = decoderTokenizer(\n",
    "        tgt,\n",
    "        max_length=max_len,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "    labels = tgt_tok.input_ids.clone()\n",
    "\n",
    "    return src_tok.input_ids, src_tok.attention_mask, tgt_tok.input_ids, tgt_tok.attention_mask, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data\n",
    "train_data = initialize_data(mode='train', dir='../dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING ###############################################################################################################\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "encoderTokenizer, decoderTokenizer = None, None\n",
    "WARMUP_EPOCHS = 2                                                               # start saving model after #WARMUP_EPOCHS\n",
    "PATIENCE = 4                                                                    # #epochs to wait before early stopping\n",
    "\n",
    "def train(model, optimizer, args):\n",
    "    global encoderTokenizer, decoderTokenizer, DEVICE\n",
    "\n",
    "    scores = []\n",
    "    for epoch in range(args.epochs):\n",
    "        print(f\"Epoch {epoch+1}/{args.epochs}\")\n",
    "\n",
    "        trainLoader, valLoader = get_dataloader(args.batch_size)\n",
    "        metric = {'loss': [], 'sari': [], 'bleu': [], 'fkgl': []}\n",
    "\n",
    "        # when dataloader runs out of batches, it throws an exception\n",
    "        try:\n",
    "            for source, target in tqdm(trainLoader):\n",
    "                src_inp, _, _, _, labels = encode_batch(\n",
    "                    encoderTokenizer, decoderTokenizer, source, target\n",
    "                )\n",
    "\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                loss = model(\n",
    "                    input_ids = src_inp.to(DEVICE),\n",
    "                    labels = labels.to(DEVICE)\n",
    "                )[0]\n",
    "\n",
    "                metric['loss'] += [loss.item()]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                # scheduler.step()\n",
    "        except StopIteration:\n",
    "            pass\n",
    "\n",
    "        # get model performace on val set\n",
    "        with torch.no_grad():\n",
    "            try:\n",
    "                for source, target, ref in tqdm(valLoader):\n",
    "                    ref = np.array(ref).T.tolist()                              # transpose ref, order gets changed in datagen\n",
    "\n",
    "                    src_inp, _, _, _, labels = encode_batch(\n",
    "                        encoderTokenizer, decoderTokenizer, source, target\n",
    "                    )\n",
    "\n",
    "                    logits = model.generate(\n",
    "                        input_ids=src_inp.to(DEVICE),\n",
    "                        max_length=args.max_length\n",
    "                    )\n",
    "                    outputs = decoderTokenizer.batch_decode(\n",
    "                        logits, skip_special_tokens=True\n",
    "                    )\n",
    "\n",
    "                    sari = sari_score(source, outputs, ref)\n",
    "                    bleu = bleu_score(outputs, ref)\n",
    "                    fkgl = fkgl_score(outputs)\n",
    "\n",
    "                    metric['sari'] += sari\n",
    "                    metric['bleu'] += bleu\n",
    "                    metric['fkgl'] += fkgl\n",
    "            except StopIteration:\n",
    "                pass\n",
    "\n",
    "        log = []\n",
    "        for key in metric.keys():\n",
    "            log.append(f'{key}: {np.mean(metric[key]):.4f}')\n",
    "        print(' - '.join(log))\n",
    "\n",
    "        scores.append(np.mean(metric['sari']))\n",
    "        # save checkpoint for only the best model\n",
    "        if epoch >= WARMUP_EPOCHS and scores[-1] == np.max(scores):\n",
    "            torch.save({'epoch': epoch+1,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': loss,\n",
    "                        }, args.save_path)\n",
    "            print('checkpoint saved.')\n",
    "        # early stopping\n",
    "        elif len(scores) - np.argmax(scores) > PATIENCE:\n",
    "            print('stopping training.')\n",
    "            break\n",
    "\n",
    "def main(args):\n",
    "    global encoderTokenizer, decoderTokenizer, DEVICE\n",
    "\n",
    "    print('using device:', DEVICE)\n",
    "    print('save path:', args.save_path)\n",
    "    print('model:', args.model)\n",
    "\n",
    "    STEPS = 138500 // args.batch_size                                           # total training samples / batch size\n",
    "    encoderTokenizer, decoderTokenizer, model = select_model(mod=args.model)\n",
    "\n",
    "    model.config.max_length = args.max_length\n",
    "    model.config.no_repeat_ngram_size = 3\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "    # scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    #     optimizer,\n",
    "    #     max_lr=args.lr*10,\n",
    "    #     steps_per_epoch=STEPS,\n",
    "    #     pct_start=0.15,\n",
    "    #     epochs=args.epochs\n",
    "    # )\n",
    "\n",
    "    # start from last checkpoint\n",
    "    if args.init_epoch > 0:\n",
    "        print('loading from', args.save_path)\n",
    "        checkpoint = torch.load(args.save_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        INIT_EPOCH = checkpoint['epoch']\n",
    "\n",
    "    train(model, optimizer, args)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Arguments for training.')\n",
    "    parser.add_argument(\n",
    "        '--model', default='gpt2', type=str,\n",
    "        choices=['gpt2', 'bert', 'bert_gpt2', 'gpt2_bert'],\n",
    "        help='model type'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--max_length', default=80, type=int,\n",
    "        help='maximum length for encoder'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--epochs', default=40, type=int,\n",
    "        help='number of training epochs'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--init_epoch', default=0, type=int,\n",
    "        help='epoch to resume the training from'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--batch_size', default=50, type=int,\n",
    "        help='batch size for training'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--lr', default=1e-4, type=float,\n",
    "        help='learning rate for training'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--save_path', default='../checkpoint/model.pt', type=str,\n",
    "        help='model save path'\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50000\n",
    "oscar_path = '../dataset/oscar.en.txt'\n",
    "tokenizer_path = '../tokenizer'\n",
    "model_path = '../RoBERTa'\n",
    "\n",
    "oscar = datasets.load_dataset(\n",
    "    'oscar',\n",
    "    'unshuffled_deduplicated_en',\n",
    "    split='train',\n",
    "    streaming=True\n",
    ")\n",
    "\n",
    "print('[INFO] reading oscar_en corpus')\n",
    "if not os.path.exists(oscar_path) or os.path.getsize(oscar_path) < 1_000_000:\n",
    "    with open(oscar_path, 'w') as f:\n",
    "        for num, batch in enumerate(oscar):\n",
    "            f.write(batch['text'] + '\\n')\n",
    "            if num > 1_000_000:\n",
    "                break\n",
    "    print('[INFO] saved corpora, file size', os.path.getsize(oscar_path))\n",
    "\n",
    "print('[INFO] training tokenizer')\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "tokenizer.train(\n",
    "    files=[oscar_path],\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\n",
    "        \"<s>\",\n",
    "        \"<pad>\",\n",
    "        \"</s>\",\n",
    "        \"<unk>\",\n",
    "        \"<mask>\",\n",
    "    ]\n",
    ")\n",
    "tokenizer.save_model(tokenizer_path)\n",
    "print('[INFO] saved tokenizer')\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    f'{tokenizer_path}/vocab.json', f'{tokenizer_path}/merges.txt'\n",
    ")\n",
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(tokenizer_path, max_len=512)\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "print('[INFO] model parameters:', model.num_parameters())\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=oscar_path,\n",
    "    block_size=128,\n",
    ")\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_path,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "print(f'[INFO] training RoBERTa on gpu: {torch.cuda.is_available()}')\n",
    "trainer.train()\n",
    "trainer.save_model(model_path)\n",
    "print('[INFO] model saved')\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=model_path,\n",
    "    tokenizer=tokenizer_path\n",
    ")\n",
    "\n",
    "print('[INFO] sanity check')\n",
    "print(fill_mask('Let children play <mask>.'))\n",
    "print(fill_mask('Sun rises in the <east>.'))\n",
    "print(fill_mask('David went to a <mask> store to buy the toilet paper.'))\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuroCraft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
