{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# neuroCraft Project\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Sentence Simplification using a Sentence-to-Sentence (seq2seq) Architecture: BERT-to-GPT2 \n",
    "***\n",
    "<br>\n",
    "- Sentence simplification using Hugging Face transformers.\n",
    "<br>\n",
    "<br>\n",
    "- Encoder-Decoder-Model with a combination of BertConfig and GPT2Config as encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christine/.pyenv/versions/3.10.6/envs/neuroCraft/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from collections import Counter\n",
    "\n",
    "import logging\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "from transformers import BertTokenizer, GPT2Tokenizer\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from transformers import EncoderDecoderModel, BertConfig, EncoderDecoderConfig, GPT2Tokenizer, BertModel, GPT2Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line reader\n",
    "def ReadInFile (filename):\n",
    "    '''\n",
    "    Utility function designed to read lines from a file and return them as a list of strings.\n",
    "\n",
    "    - with open(filename) as f:\n",
    "    Opens the file specified by the filename parameter using a with statement, which ensures that the file is properly closed after its suite finishes, even if an exception occurs.\n",
    "\n",
    "    - lines = f.readlines():\n",
    "    Reads all lines from the file f and stores them in the lines variable as a list of strings. Each line from the file becomes an element in the list.\n",
    "\n",
    "    - lines = [x.strip() for x in lines]:\n",
    "    Strips any leading or trailing whitespaces (like newline characters) from each line in the lines list using a list comprehension.\n",
    "    This step ensures that the lines are clean and don't contain any extra spaces.\n",
    "\n",
    "    - return lines:\n",
    "    Returns the list of cleaned lines from the file.\n",
    "    '''\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [x.strip() for x in lines]\n",
    "    return lines\n",
    "\n",
    "\n",
    "# file opener\n",
    "def open_file(file_path, ref=False):\n",
    "    '''\n",
    "    Versatile file opener that can read text files and binary pickle files in Python.\n",
    "\n",
    "    - reading Text Files:\n",
    "    If the ref parameter is False or not specified, it assumes the file is a text file.\n",
    "    It opens the file specified by file_path in read mode ('r') using utf-8 encoding to handle Unicode characters.\n",
    "\n",
    "    - Loading Pickle Files:\n",
    "    If ref is set to True, it assumes the file is a binary pickle file (ref_data = pickle.load(open(file_path, 'rb'))) and directly returns the loaded data.\n",
    "\n",
    "    - Processing Text Files:\n",
    "    If the file is a text file, it reads lines from the file (sents = f.readlines()) and strips any leading or trailing whitespaces (including newline characters) from each line.\n",
    "    Each cleaned line is appended to the data list.\n",
    "\n",
    "    After processing all lines, it returns the list of cleaned lines (data).\n",
    "    '''\n",
    "    data = []\n",
    "    if ref:\n",
    "        ref_data = pickle.load(open(file_path, 'rb'))\n",
    "        return ref_data\n",
    "    else:\n",
    "        with open(file_path, 'r', encoding=\"utf8\") as f:\n",
    "            sents = f.readlines()\n",
    "            for s in sents:\n",
    "                data.append(s.strip())\n",
    "        return data\n",
    "\n",
    "# data loader\n",
    "def load_dataset(src_path, tgt_path=None, ref_path=None, ref=False):\n",
    "    '''\n",
    "    Function serves as a data loader and orchestrator for multiple files for NLP tasks.\n",
    "\n",
    "    - Loading Source Data:\n",
    "    Loads the source data from the file specified by src_path using the open_file function without any specific reference handling.\n",
    "\n",
    "    - Loading Target Data (Optional):\n",
    "    If a tgt_path is provided, it loads target data using the open_file function and assigns it to the tgt variable.\n",
    "    This can be used when dealing with tasks like translation and simplification (here) where there are source and target language sentences.\n",
    "\n",
    "    - Loading Reference Data (Optional):\n",
    "    If a ref_path is provided, it loads reference data using the open_file function, potentially handling it as a pickle file based on the ref parameter.\n",
    "    This ref parameter determines whether to load the reference data as a pickle file or text file.\n",
    "\n",
    "    - Returning Loaded Data:\n",
    "    returns three variables: src (source data), tgt (target data, if provided), and ref (reference data, if provided)\n",
    "    '''\n",
    "    src = open_file(src_path)\n",
    "    tgt = None\n",
    "    ref = None\n",
    "    if tgt_path is not None:\n",
    "        tgt = open_file(tgt_path)\n",
    "    if ref_path is not None:\n",
    "        ref = open_file(ref_path, ref)\n",
    "    return src, tgt, ref\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimming training data before training\n",
    "def trim_files(src_file, tgt_file, output_src_file, output_tgt_file, num_sentences=2000):\n",
    "    with open(src_file, 'r', encoding='utf-8') as src_f, open(tgt_file, 'r', encoding='utf-8') as tgt_f:\n",
    "        src_lines = src_f.readlines()\n",
    "        tgt_lines = tgt_f.readlines()\n",
    "\n",
    "        # ensure both source and target files have enough lines\n",
    "        if len(src_lines) < num_sentences or len(tgt_lines) < num_sentences:\n",
    "            print(\"Files don't contain enough sentences.\")\n",
    "            return\n",
    "\n",
    "        # trim both files to the same number of sentences\n",
    "        trimmed_src = src_lines[:num_sentences]\n",
    "        trimmed_tgt = tgt_lines[:num_sentences]\n",
    "\n",
    "        # write trimmed sentences to new files\n",
    "        with open(output_src_file, 'w', encoding='utf-8') as out_src_f, open(output_tgt_file, 'w', encoding='utf-8') as out_tgt_f:\n",
    "            out_src_f.writelines(trimmed_src)\n",
    "            out_tgt_f.writelines(trimmed_tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trimming training, valitation, and test data\n",
    "\n",
    "# train\n",
    "src_train_file = '../dataset/src_train.txt'\n",
    "tgt_train_file = '../dataset/tgt_train.txt'\n",
    "output_src_train_file = '../dataset/output_src_train.txt'\n",
    "output_tgt_train_file = '../dataset/output_tgt_train.txt'\n",
    "trim_files(src_train_file, tgt_train_file, output_src_train_file, output_tgt_train_file, num_sentences=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CURRENT DATA\n",
    "\n",
    "\"\"\"\n",
    "1. Training:\n",
    "\n",
    "Wikilarg dataset comprising of parallel corpus of normal sentences and simple sentences is used to train the model.\n",
    "The original dataset consists of around 167k English sentence pairs from the Wikipedia articles.\n",
    "The dataset comprises of mapping of one-to-many, one-to-one and many-to-one sentence pairs.\n",
    "But the dataset was not suitable for the training without preprocessing.\n",
    "Upon tokenizing the sentences, sentences having token length of more than 80 were removed keeping the maximum token length of sentences to 80.\n",
    "The resulting training dataset became 138k from 167k.\n",
    "\n",
    "2. Validation:\n",
    "\n",
    "WikiSmall dataset ...\n",
    "\n",
    "3. Testing:\n",
    "\n",
    "For the evaluation and testing purpose, TurkCorpus is used.\n",
    "The dataset consists of 2k manually prepared sentence pairs with 8 reference sentences and 300 sentences for testing purpose which also has 8 reference sentences.\n",
    "\"\"\"\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep class for data handling for now\n",
    "\n",
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, src_path, tgt_path=None, ref_path=None, ref=False):\n",
    "        self.src = self.open_file(src_path)\n",
    "        self.tgt = None\n",
    "        self.ref = None\n",
    "        if tgt_path is not None:\n",
    "            self.tgt = self.open_file(tgt_path)\n",
    "        if ref_path is not None:\n",
    "            self.ref = self.open_file(ref_path, ref)\n",
    "\n",
    "        self.size = len(self.src)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.tgt is not None and self.ref is not None:\n",
    "            return self.src[index], self.tgt[index], self.ref[index]\n",
    "        elif self.tgt is not None:\n",
    "            return self.src[index], self.tgt[index], None\n",
    "        else:\n",
    "            return self.src[index], None, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    @staticmethod\n",
    "    def open_file(file_path, ref=False):\n",
    "        data = []\n",
    "        if ref:\n",
    "            ref_data = pickle.load(open(file_path, 'rb'))\n",
    "            return ref_data\n",
    "\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding=\"utf8\") as f:\n",
    "                sents = f.readlines()\n",
    "                for s in sents:\n",
    "                    data.append(s.strip())\n",
    "            return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data (Wikilarg)\n",
      "(100, 1)\n",
      "(100, 1)\n",
      "Source: In 1997 , after a failed engagement to a Muslim girl while he was attending Northeastern University , he started a Yahoo !\n",
      "Target: In 1997 , after a marriage engagement to a Muslim girl did not work out , he began a Yahoo !\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "src_train_file = '../dataset/output_src_train.txt'\n",
    "tgt_train_file = '../dataset/output_tgt_train.txt'\n",
    "\n",
    "# original train data and target train data (no ref data)\n",
    "src_train, tgt_train, ref_train = load_dataset(src_train_file, tgt_train_file, ref=False)\n",
    "\n",
    "print('training data (Wikilarg)')\n",
    "print(pd.DataFrame(src_train).shape)\n",
    "print(pd.DataFrame(tgt_train).shape)\n",
    "\n",
    "sample_index = 2\n",
    "src_sample = src_train[sample_index]\n",
    "tgt_sample = tgt_train[sample_index]\n",
    "\n",
    "print(\"Source:\", src_sample)\n",
    "print(\"Target:\", tgt_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "validation data (WikiSmall)\n",
      "(2000, 1)\n",
      "(2000, 1)\n",
      "Source: Since 1980 the senior pastor has been John Piper .\n",
      "Target: Since 1980 the main pastor has been John Piper .\n"
     ]
    }
   ],
   "source": [
    "# validation data\n",
    "src_valid_file = '../dataset/src_valid.txt'\n",
    "tgt_valid_file = '../dataset/tgt_valid.txt'\n",
    "\n",
    "# original train data and target train data (no ref data)\n",
    "src_valid, tgt_valid, ref_valid = load_dataset(src_valid_file, tgt_valid_file, ref=False)\n",
    "\n",
    "print(' ')\n",
    "print('validation data (WikiSmall)')\n",
    "print(pd.DataFrame(src_valid).shape)\n",
    "print(pd.DataFrame(tgt_valid).shape)\n",
    "\n",
    "sample_index = 99\n",
    "src_sample = src_valid[sample_index]\n",
    "tgt_sample = tgt_valid[sample_index]\n",
    "\n",
    "print(\"Source:\", src_sample)\n",
    "print(\"Target:\", tgt_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "test data (Amazon Mechanical Turk workers)\n",
      "(359, 1)\n",
      "(359, 1)\n",
      "Source: Alessandro ( \" Sandro \" ) Mazzola ( born 8 November 1942 ) is an Italian former football player .\n",
      "Target: Alessandro Mazzola is an Italian former football player.\n"
     ]
    }
   ],
   "source": [
    "# testing data\n",
    "src_test_file = '../dataset/src_test.txt'\n",
    "tgt_test_file = '../dataset/tgt_test.txt'\n",
    "\n",
    "# original train data and target train data (no ref data)\n",
    "src_test, tgt_test, ref_test = load_dataset(src_test_file, tgt_test_file, ref=False)\n",
    "\n",
    "print(' ')\n",
    "print('test data (Amazon Mechanical Turk workers)')\n",
    "print(pd.DataFrame(src_test).shape)\n",
    "print(pd.DataFrame(tgt_test).shape)\n",
    "\n",
    "sample_index = 10\n",
    "src_sample = src_test[sample_index]\n",
    "tgt_sample = tgt_test[sample_index]\n",
    "\n",
    "print(\"Source:\", src_sample)\n",
    "print(\"Target:\", tgt_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tokenizers\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.unk_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for encoding the text\n",
    "def encode_batch(batch, max_len=200):\n",
    "    '''\n",
    "    Inputs\n",
    "    - batch:\n",
    "        This function expects a list containing two elements:\n",
    "        - The first element is a string representing the source text.\n",
    "        - The second element is a string representing the target text.\n",
    "\n",
    "    Tokenization\n",
    "    - src_tokens:\n",
    "    The source text (first element of batch) is tokenized using the BERT tokenizer (bert_tokenizer).\n",
    "    It is processed to generate tokens (input_ids) and an attention mask for the source text.\n",
    "    BERT tokenization includes adding special tokens, padding to a maximum length, and truncating if needed.\n",
    "\n",
    "    - tgt_tokens:\n",
    "    The target text (second element of batch) is tokenized using the GPT-2 tokenizer (gpt2_tokenizer).\n",
    "    Similar to the source, tokens (input_ids) and an attention mask for the target text are generated.\n",
    "\n",
    "    Creating Labels\n",
    "    - labels: This is created from the tgt_tokens.input_ids tensor.\n",
    "    It's used for calculating loss during training.\n",
    "    The tgt_tokens.attention_mask is used to identify where the padding is in the target tokens and sets those positions in the labels tensor to -100.\n",
    "\n",
    "    Output\n",
    "    The function returns five values:\n",
    "    - src_tokens.input_ids:\n",
    "    Tensor containing tokenized representation of the source text.\n",
    "    - src_tokens.attention_mask:\n",
    "    Tensor containing attention mask for the source text.\n",
    "    - tgt_tokens.input_ids:\n",
    "    Tensor containing tokenized representation of the target text.\n",
    "    - tgt_tokens.attention_mask:\n",
    "    Tensor containing attention mask for the target text.\n",
    "\n",
    "    Labels:\n",
    "    Tensor containing the labels for the target text, modified with -100 in places corresponding to padding.\n",
    "    '''\n",
    "    src_tokens = bert_tokenizer(batch[0], max_length=max_len, add_special_tokens=True,\n",
    "                                return_token_type_ids=False, padding=\"max_length\", truncation=True,\n",
    "                                return_attention_mask=True, return_tensors=\"pt\")\n",
    "\n",
    "    tgt_tokens = gpt2_tokenizer(batch[1], max_length=max_len, add_special_tokens=True,\n",
    "                                return_token_type_ids=False, padding=\"max_length\", truncation=True,\n",
    "                                return_attention_mask=True, return_tensors=\"pt\")\n",
    "\n",
    "    labels = tgt_tokens.input_ids.clone()\n",
    "    labels[tgt_tokens.attention_mask == 0] = -100\n",
    "\n",
    "    return src_tokens.input_ids, src_tokens.attention_mask, tgt_tokens.input_ids, tgt_tokens.attention_mask, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_tokens(sents):\n",
    "    ref = []\n",
    "    tokens = gpt2_tokenizer(sents, add_special_tokens=True,\n",
    "            return_token_type_ids=False, truncation=True, padding=\"longest\",\n",
    "            return_attention_mask=False, return_tensors=\"pt\")\n",
    "\n",
    "    for tok in tokens.input_ids.tolist():\n",
    "        ref.append([tok])\n",
    "\n",
    "    return ref\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sent(sents, max_len=128):\n",
    "    src_tokens = []\n",
    "    for s in sents:\n",
    "        tokens = bert_tokenizer(s, max_length=max_len, add_special_tokens=True,\n",
    "                                return_token_type_ids=False, truncation=True,\n",
    "                                padding=\"max_length\", return_attention_mask=True, return_tensors=\"pt\")\n",
    "        src_tokens.append([tokens.input_ids, tokens.attention_mask])\n",
    "\n",
    "    return src_tokens\n",
    "\n",
    "def decode_sent_tokens(data):\n",
    "    sents_list = []\n",
    "    for sent_pair in data:\n",
    "        decoded = gpt2_tokenizer.decode(sent_pair, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        sents_list.append(decoded)\n",
    "        print(decoded)  # Print the decoded sentence directly in the notebook\n",
    "\n",
    "    return sents_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARI (System-level Automatic Readability Index) Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing function for calculating sari -gram F1 scores (SS task)\n",
    "\n",
    "def SARIngram(sgrams, cgrams, rgramslist, numref):\n",
    "    '''\n",
    "    Function calculating the SARI (System-level Automatic Readability Index) score for a given set of n-grams.\n",
    "    The SARI score assesses the quality of simplified sentences compared to both the original and reference sentences.\n",
    "\n",
    "    Counting n-grams:\n",
    "    The function takes as input three types of n-grams: sgrams (simplified sentence n-grams), cgrams (candidate n-grams), and rgramslist (a list of reference sentence n-grams).\n",
    "    It creates counters to count occurrences of each n-gram type in these inputs.\n",
    "\n",
    "    Handling Repetitions:\n",
    "    It adjusts the counts for sgrams and cgrams by multiplying them by numref, which represents the number of reference sentences for comparison.\n",
    "\n",
    "    Calculating Scores for Three Aspects (Keep, Deletion, Addition):\n",
    "    - Keep: Compares n-grams present in both the simplified and candidate sentences against those in the reference sentences.\n",
    "    - Deletion: Looks at n-grams in the simplified sentences that are absent in the candidate sentences and compares them against reference n-grams.\n",
    "    - Addition: Considers n-grams in the candidate sentences not present in the simplified sentences and compares them against reference n-grams.\n",
    "\n",
    "    Precision and Recall Calculations:\n",
    "    For each aspect (Keep, Deletion, Addition), it calculates precision and recall scores based on the counts of n-grams.\n",
    "\n",
    "    SARI Score Calculation:\n",
    "    It computes the final SARI score using the precision and recall scores for Keep, Deletion, and Addition aspects.\n",
    "\n",
    "    Return:\n",
    "    Function returns a tuple containing the SARI score, precision score for deletions, and the score for additions.\n",
    "    '''\n",
    "    rgramsall = [rgram for rgrams in rgramslist for rgram in rgrams]\n",
    "    rgramcounter = Counter(rgramsall)\n",
    "\n",
    "    sgramcounter = Counter(sgrams)\n",
    "    sgramcounter_rep = Counter()\n",
    "    for sgram, scount in sgramcounter.items():\n",
    "        sgramcounter_rep[sgram] = scount * numref\n",
    "\n",
    "    cgramcounter = Counter(cgrams)\n",
    "    cgramcounter_rep = Counter()\n",
    "    for cgram, ccount in cgramcounter.items():\n",
    "        cgramcounter_rep[cgram] = ccount * numref\n",
    "\n",
    "\n",
    "    # KEEP\n",
    "    keepgramcounter_rep = sgramcounter_rep & cgramcounter_rep\n",
    "    keepgramcountergood_rep = keepgramcounter_rep & rgramcounter\n",
    "    keepgramcounterall_rep = sgramcounter_rep & rgramcounter\n",
    "\n",
    "    keeptmpscore1 = 0\n",
    "    keeptmpscore2 = 0\n",
    "    for keepgram in keepgramcountergood_rep:\n",
    "        keeptmpscore1 += keepgramcountergood_rep[keepgram] / keepgramcounter_rep[keepgram]\n",
    "        keeptmpscore2 += keepgramcountergood_rep[keepgram] / keepgramcounterall_rep[keepgram]\n",
    "        #print \"KEEP\", keepgram, keepscore, cgramcounter[keepgram], sgramcounter[keepgram], rgramcounter[keepgram]\n",
    "    keepscore_precision = 0\n",
    "    if len(keepgramcounter_rep) > 0:\n",
    "    \tkeepscore_precision = keeptmpscore1 / len(keepgramcounter_rep)\n",
    "    keepscore_recall = 0\n",
    "    if len(keepgramcounterall_rep) > 0:\n",
    "    \tkeepscore_recall = keeptmpscore2 / len(keepgramcounterall_rep)\n",
    "    keepscore = 0\n",
    "    if keepscore_precision > 0 or keepscore_recall > 0:\n",
    "        keepscore = 2 * keepscore_precision * keepscore_recall / (keepscore_precision + keepscore_recall)\n",
    "\n",
    "\n",
    "    # DELETION\n",
    "    delgramcounter_rep = sgramcounter_rep - cgramcounter_rep\n",
    "    delgramcountergood_rep = delgramcounter_rep - rgramcounter\n",
    "    delgramcounterall_rep = sgramcounter_rep - rgramcounter\n",
    "    deltmpscore1 = 0\n",
    "    deltmpscore2 = 0\n",
    "    for delgram in delgramcountergood_rep:\n",
    "        deltmpscore1 += delgramcountergood_rep[delgram] / delgramcounter_rep[delgram]\n",
    "        deltmpscore2 += delgramcountergood_rep[delgram] / delgramcounterall_rep[delgram]\n",
    "    delscore_precision = 0\n",
    "    if len(delgramcounter_rep) > 0:\n",
    "    \tdelscore_precision = deltmpscore1 / len(delgramcounter_rep)\n",
    "    delscore_recall = 0\n",
    "    if len(delgramcounterall_rep) > 0:\n",
    "    \tdelscore_recall = deltmpscore1 / len(delgramcounterall_rep)\n",
    "    delscore = 0\n",
    "    if delscore_precision > 0 or delscore_recall > 0:\n",
    "        delscore = 2 * delscore_precision * delscore_recall / (delscore_precision + delscore_recall)\n",
    "\n",
    "\n",
    "    # ADDITION\n",
    "    addgramcounter = set(cgramcounter) - set(sgramcounter)\n",
    "    addgramcountergood = set(addgramcounter) & set(rgramcounter)\n",
    "    addgramcounterall = set(rgramcounter) - set(sgramcounter)\n",
    "\n",
    "    addtmpscore = 0\n",
    "    for addgram in addgramcountergood:\n",
    "        addtmpscore += 1\n",
    "\n",
    "    addscore_precision = 0\n",
    "    addscore_recall = 0\n",
    "    if len(addgramcounter) > 0:\n",
    "    \taddscore_precision = addtmpscore / len(addgramcounter)\n",
    "    if len(addgramcounterall) > 0:\n",
    "    \taddscore_recall = addtmpscore / len(addgramcounterall)\n",
    "    addscore = 0\n",
    "    if addscore_precision > 0 or addscore_recall > 0:\n",
    "        addscore = 2 * addscore_precision * addscore_recall / (addscore_precision + addscore_recall)\n",
    "\n",
    "    return (keepscore, delscore_precision, addscore)\n",
    "\n",
    "\n",
    "\n",
    "def SARIsent(ssent, csent, rsents):\n",
    "    '''\n",
    "    This function computes the System-level Automatic Readability Index (SARI) score for a set of sentences.\n",
    "\n",
    "    Tokenizing Sentences:\n",
    "\n",
    "    Takes in three inputs\n",
    "    - ssent (simplified sentence)\n",
    "    - csent (candidate sentence)\n",
    "    - and rsents (a list of reference sentences)\n",
    "    The function begins by splitting these sentences into unigrams, bigrams, trigrams, and quadgrams\n",
    "    (s1grams, s2grams, s3grams, s4grams, c1grams, c2grams, c3grams, c4grams, r1gramslist, r2gramslist, r3gramslist, r4gramslist)\n",
    "    using spaces as delimiters.\n",
    "\n",
    "    Creating n-grams from Reference Sentences:\n",
    "\n",
    "    Creates n-grams\n",
    "    (unigrams, bigrams, trigrams, and quadgrams)\n",
    "    from the reference sentences (rsents) and stores them in separate lists.\n",
    "\n",
    "    Calculating Scores for Different n-gram Levels:\n",
    "\n",
    "    For each level of n-grams (from unigrams to quadgrams) in both the simplified and candidate sentences,\n",
    "    it calls the SARIngram function to compute scores for keep, delete, and add operations for each level of n-grams.\n",
    "\n",
    "    Aggregating Scores:\n",
    "    It averages the scores for keep, delete, and add operations across different n-gram levels to get average keep, delete, and add scores.\n",
    "\n",
    "    Computing the Final SARI Score:\n",
    "    Computes the final SARI score by averaging the average keep, delete, and add scores, and then returns the final score.\n",
    "    '''\n",
    "    numref = len(rsents)\n",
    "\n",
    "    s1grams = ssent.lower().split(\" \")\n",
    "    c1grams = csent.lower().split(\" \")\n",
    "    s2grams = []\n",
    "    c2grams = []\n",
    "    s3grams = []\n",
    "    c3grams = []\n",
    "    s4grams = []\n",
    "    c4grams = []\n",
    "\n",
    "    r1gramslist = []\n",
    "    r2gramslist = []\n",
    "    r3gramslist = []\n",
    "    r4gramslist = []\n",
    "    for rsent in rsents:\n",
    "        r1grams = rsent.lower().split(\" \")\n",
    "        r2grams = []\n",
    "        r3grams = []\n",
    "        r4grams = []\n",
    "        r1gramslist.append(r1grams)\n",
    "        for i in range(0, len(r1grams)-1) :\n",
    "            if i < len(r1grams) - 1:\n",
    "                r2gram = r1grams[i] + \" \" + r1grams[i+1]\n",
    "                r2grams.append(r2gram)\n",
    "            if i < len(r1grams)-2:\n",
    "                r3gram = r1grams[i] + \" \" + r1grams[i+1] + \" \" + r1grams[i+2]\n",
    "                r3grams.append(r3gram)\n",
    "            if i < len(r1grams)-3:\n",
    "                r4gram = r1grams[i] + \" \" + r1grams[i+1] + \" \" + r1grams[i+2] + \" \" + r1grams[i+3]\n",
    "                r4grams.append(r4gram)\n",
    "        r2gramslist.append(r2grams)\n",
    "        r3gramslist.append(r3grams)\n",
    "        r4gramslist.append(r4grams)\n",
    "\n",
    "    for i in range(0, len(s1grams)-1) :\n",
    "        if i < len(s1grams) - 1:\n",
    "            s2gram = s1grams[i] + \" \" + s1grams[i+1]\n",
    "            s2grams.append(s2gram)\n",
    "        if i < len(s1grams)-2:\n",
    "            s3gram = s1grams[i] + \" \" + s1grams[i+1] + \" \" + s1grams[i+2]\n",
    "            s3grams.append(s3gram)\n",
    "        if i < len(s1grams)-3:\n",
    "            s4gram = s1grams[i] + \" \" + s1grams[i+1] + \" \" + s1grams[i+2] + \" \" + s1grams[i+3]\n",
    "            s4grams.append(s4gram)\n",
    "\n",
    "    for i in range(0, len(c1grams)-1) :\n",
    "        if i < len(c1grams) - 1:\n",
    "            c2gram = c1grams[i] + \" \" + c1grams[i+1]\n",
    "            c2grams.append(c2gram)\n",
    "        if i < len(c1grams)-2:\n",
    "            c3gram = c1grams[i] + \" \" + c1grams[i+1] + \" \" + c1grams[i+2]\n",
    "            c3grams.append(c3gram)\n",
    "        if i < len(c1grams)-3:\n",
    "            c4gram = c1grams[i] + \" \" + c1grams[i+1] + \" \" + c1grams[i+2] + \" \" + c1grams[i+3]\n",
    "            c4grams.append(c4gram)\n",
    "\n",
    "    (keep1score, del1score, add1score) = SARIngram(s1grams, c1grams, r1gramslist, numref)\n",
    "    (keep2score, del2score, add2score) = SARIngram(s2grams, c2grams, r2gramslist, numref)\n",
    "    (keep3score, del3score, add3score) = SARIngram(s3grams, c3grams, r3gramslist, numref)\n",
    "    (keep4score, del4score, add4score) = SARIngram(s4grams, c4grams, r4gramslist, numref)\n",
    "    avgkeepscore = sum([keep1score,keep2score,keep3score,keep4score])/4\n",
    "    avgdelscore = sum([del1score,del2score,del3score,del4score])/4\n",
    "    avgaddscore = sum([add1score,add2score,add3score,add4score])/4\n",
    "    finalscore = (avgkeepscore + avgdelscore + avgaddscore ) / 3\n",
    "\n",
    "    return finalscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final computation function for sentence level SARI\n",
    "def compute_sari(norm, pred_tensor, ref):\n",
    "    '''\n",
    "    computes the Sentence-level SARI score for a set of predicted sentences compared to their corresponding reference sentences.\n",
    "\n",
    "    Decoding Predicted Sentences:\n",
    "    decodes the predicted sentences from the pred_tensor using the tokenizer.\n",
    "    This step converts tokenized or numerical representations back into human-readable sentences.\n",
    "\n",
    "    SARI Score Calculation:\n",
    "    The function initializes a score variable to accumulate the SARI scores for individual sentences.\n",
    "    It iterates over each step (index) and corresponding items in the ref list.\n",
    "\n",
    "    For each step:\n",
    "    Computes the SARI score using sari.SARIsent(norm[step], pred[step], item).\n",
    "    This compares the normalized (norm[step]) predicted sentence with the reference sentence (item) and computes the SARI score.\n",
    "    Adds up the individual SARI scores to the score variable.\n",
    "\n",
    "    Normalization and Return:\n",
    "    The final computed score is divided by the TRAIN_BATCH_SIZE to normalize the score.\n",
    "\n",
    "    Returns the normalized SARI score, which is the cumulative SARI score of the predicted sentences against their reference sentences, divided by the batch size.\n",
    "    '''\n",
    "    pred = decode_sent_tokens(pred_tensor)\n",
    "    score = 0\n",
    "    for step, item in enumerate(ref):\n",
    "        score += SARIsent(norm[step], pred[step], item)\n",
    "    return score/TRAIN_BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation function\n",
    "def compute_bleu_score(logits, labels):\n",
    "    '''\n",
    "    This function takes predicted sequences (logits) and their corresponding reference sequences (labels),\n",
    "    tokenizes the reference sentences, and computes a weighted BLEU score using NLTK's corpus_bleu function.\n",
    "    The BLEU score is a metric commonly used to evaluate the quality of machine-translated text against reference translations,\n",
    "    considering n-gram precision with specified weights.\n",
    "\n",
    "    Tokenization of Reference Sentences:\n",
    "    It retrieves the reference sentences by tokenizing the labels input.\n",
    "    This step involves converting human-readable sentences into tokens suitable for BLEU score computation.\n",
    "\n",
    "    Weighted BLEU Score Calculation:\n",
    "    weights are assigned to n-gram precision values (e.g., 1-gram precision and 2-gram precision).\n",
    "\n",
    "    BLEU Score Calculation:\n",
    "    computes the BLEU score using the corpus_bleu function from NLTK's nltk.translate.bleu_score module.\n",
    "    The refs variable contains the reference sentences (tokenized), and logits.tolist() contain the predicted sequences, converted to a list format.\n",
    "    The smoothing_function parameter applies a smoothing method to handle cases where certain n-grams are missing in the predicted sequences.\n",
    "    The computed BLEU score based on these inputs is stored in the score variable.\n",
    "\n",
    "    Returns the computed BLEU score as the output of the function.\n",
    "    '''\n",
    "    refs = get_sent_tokens(labels)\n",
    "    weights = (1.0/2.0, 1.0/2.0, )\n",
    "    score = corpus_bleu(refs, logits.tolist(), smoothing_function=SmoothingFunction(epsilon=1e-10).method1, weights=weights)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation after training (unseen test data)\n",
    "def evaluate(data_loader, e_loss):\n",
    "    '''\n",
    "    Performs evaluation on the trained model using the provided data loader (data_loader).\n",
    "    Assesses the model's performance in terms of loss, BLEU score, and SARI score on the test dataset.\n",
    "\n",
    "    Setting Model Evaluation Mode\n",
    "    - was_training = model.training:\n",
    "    Stores the model's current training state.\n",
    "\n",
    "    - model.eval():\n",
    "    Puts the model in evaluation mode, which disables dropout and batch normalization layers as they behave differently during training and evaluation.\n",
    "\n",
    "    - initialization:\n",
    "    Initializes variables for evaluation metrics (eval_loss, bleu_score, sari_score) to keep track of loss, BLEU score, and SARI score.\n",
    "\n",
    "    LogSoftmax and No Gradient Calculation\n",
    "    - softmax = nn.LogSoftmax(dim=-1): Initializes a LogSoftmax layer for computing probabilities along the last dimension (-1) of the tensor.\n",
    "    - with torch.no_grad():: Temporarily disables gradient calculation for efficiency during evaluation.\n",
    "\n",
    "    Iterating Over Data Loader Batches:\n",
    "    Loops over batches in the data_loader.\n",
    "    Encodes the batch data using the tokenizer.\n",
    "    Passes the encoded tensors to the model to get predictions (logits) and calculate the loss.\n",
    "    Computes the BLEU score and SARI score using the compute_bleu_score and compute_sari functions,\n",
    "    comparing the model's predicted outputs with the ground truth labels.\n",
    "    Updates the evaluation metrics (eval_loss, bleu_score, sari_score) by averaging across batches.\n",
    "\n",
    "    Restoring Model State and Return:\n",
    "    Restores the model to its original training state (model.train()) if it was in training mode before evaluation.\n",
    "\n",
    "    Returns the evaluated loss (eval_loss), BLEU score (bleu_score), and SARI score (sari_score) as a tuple.\n",
    "    '''\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    eval_loss = e_loss\n",
    "    bleu_score = 0\n",
    "    sari_score = 0\n",
    "    softmax = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(data_loader):\n",
    "            src_tensors, src_attn_tensors, tgt_tensors, tgt_attn_tensors, labels = encode_batch(batch)\n",
    "            loss, logits = model(input_ids = src_tensors.to(device),\n",
    "                            decoder_input_ids = tgt_tensors.to(device),\n",
    "                            attention_mask = src_attn_tensors.to(device),\n",
    "                            decoder_attention_mask = tgt_attn_tensors.to(device),\n",
    "                            labels = labels.to(device))[:2]\n",
    "            outputs = softmax(logits)\n",
    "            score = compute_bleu_score(torch.argmax(outputs, dim=-1), batch[1])\n",
    "            s_score = compute_sari(batch[0], torch.argmax(outputs, dim=-1), batch[2])\n",
    "            if step == 0:\n",
    "                eval_loss = loss.item()\n",
    "                bleu_score = score\n",
    "                sari_score = s_score\n",
    "            else:\n",
    "                eval_loss = (1/2.0)*(eval_loss + loss.item())\n",
    "                bleu_score = (1/2.0)* (bleu_score+score)\n",
    "                sari_score = (1/2.0)* (sari_score+s_score)\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return eval_loss, bleu_score, sari_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing Checkpoints\n",
    "def load_checkpt(checkpt_path, optimizer=None):\n",
    "    '''\n",
    "    Loading a checkpoint file containing model weights, optimizer state, evaluation loss, and epoch information. Let's break down its functionality:\n",
    "\n",
    "    Loading Checkpoint\n",
    "    - checkpoint = torch.load(checkpt_path):\n",
    "    Loads the checkpoint file from the specified checkpt_path using PyTorch's torch.load() function.\n",
    "\n",
    "    Loading Model and Optimizer States\n",
    "    Depending on the device being used (\"cpu\" or GPU), it loads the model's state and optimizer's state from the checkpoint file.\n",
    "    If the device is \"cpu,\" it maps both the model and optimizer states to the CPU using map_location=torch.device(\"cpu\").\n",
    "    If the device is a GPU, it loads the model and optimizer states directly without any mapping.\n",
    "\n",
    "    Retrieving Evaluation Loss and Epoch:\n",
    "    Extracts the evaluation loss (eval_loss) and epoch information (epoch) from the loaded checkpoint.\n",
    "\n",
    "    Returns the loaded optimizer (if provided and updated), evaluation loss, and epoch.\n",
    "    '''\n",
    "    checkpoint = torch.load(checkpt_path)\n",
    "    if device == \"cpu\":\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"], map_location=torch.device(\"cpu\"))\n",
    "        if optimizer is not None:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"], map_location=torch.device(\"cpu\"))\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        if optimizer is not None:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    eval_loss = checkpoint[\"eval_loss\"]\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "    return optimizer, eval_loss, epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Training Checkpoint\n",
    "def save_model_checkpt(state, is_best, check_pt_path, best_model_path):\n",
    "    '''\n",
    "    Saves model checkpoints at regular intervals during training and, if specified,\n",
    "    also keeps a copy of the best-performing model in a separate file.\n",
    "    This practice allows for the restoration of model states,\n",
    "    retraining from specific points,\n",
    "    or retrieving the best model for deployment or further evaluation.\n",
    "    '''\n",
    "    f_path = check_pt_path\n",
    "    torch.save(state, f_path)\n",
    "\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, target_tokenizer, ref_sentences):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch_source = batch[0]\n",
    "            batch_target = batch[1]\n",
    "\n",
    "            outputs = model(input_ids=batch_source, decoder_input_ids=batch_target)\n",
    "\n",
    "            logits_flat = outputs.logits.view(-1, outputs.logits.size(-1))\n",
    "            target_flat = batch_target.view(-1)\n",
    "\n",
    "            loss = criterion(logits_flat, target_flat)\n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "\n",
    "            predicted_ids = outputs.logits.argmax(-1)\n",
    "            predicted_sentences = [target_tokenizer.decode(ids, skip_special_tokens=True) for ids in predicted_ids]\n",
    "            target_sentences = [target_tokenizer.decode(ids, skip_special_tokens=True) for ids in batch_target]\n",
    "\n",
    "            references.extend([sent.split() for sent in target_sentences])\n",
    "            hypotheses.extend([sent.split() for sent in predicted_sentences])\n",
    "\n",
    "    avg_loss = total_loss / total_batches\n",
    "\n",
    "    # Calculate BLEU score using your function\n",
    "    #bleu_score = compute_bleu_score(torch.argmax(outputs, dim=-1), batch[1])\n",
    "    logits = outputs.logits  # Extract the logits from Seq2SeqLMOutput\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)  # Get the predicted IDs\n",
    "    bleu_score = compute_bleu_score(predicted_ids, batch[1])  # Compute BLEU score\n",
    "\n",
    "    # Calculate SARI score\n",
    "    sari_scores = []\n",
    "    for idx, gen_sent in enumerate(hypotheses):\n",
    "        sari_score = compute_sari(ref_sentences[idx], gen_sent)\n",
    "        sari_scores.append(sari_score)\n",
    "\n",
    "    avg_sari_score = sum(sari_scores) / len(sari_scores)\n",
    "\n",
    "    return avg_loss, bleu_score, avg_sari_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dataset/ref_valid.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 26\u001b[0m\n\u001b[1;32m     22\u001b[0m eval_data_loader \u001b[38;5;241m=\u001b[39m DataLoader(eval_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# ref sentences\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# load data from the pickle file\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../dataset/ref_valid.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     27\u001b[0m     ref_sentences \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/neuroCraft/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/ref_valid.pkl'"
     ]
    }
   ],
   "source": [
    "# prepare validation dataset\n",
    "\n",
    "# function to load evaluation data\n",
    "def load_eval_data(src_path, tgt_path):\n",
    "    src_data = open_file(src_path)\n",
    "    tgt_data = open_file(tgt_path)\n",
    "    return src_data, tgt_data\n",
    "\n",
    "# evaluation data paths\n",
    "src_eval_file = '../dataset/src_valid.txt'\n",
    "tgt_eval_file = '../dataset/tgt_valid.txt'\n",
    "\n",
    "# load evaluation data\n",
    "src_eval, tgt_eval = load_eval_data(src_eval_file, tgt_eval_file)\n",
    "\n",
    "# prepare evaluation data tensors using encode_batch function\n",
    "source_eval_data_encoded = encode_batch((src_eval))\n",
    "target_eval_data_encoded = encode_batch((tgt_eval))\n",
    "\n",
    "# create evaluation data loader\n",
    "eval_dataset = TensorDataset(*source_eval_data_encoded, *target_eval_data_encoded)\n",
    "eval_data_loader = DataLoader(eval_dataset, batch_size=3, shuffle=False)\n",
    "\n",
    "# ref sentences\n",
    "# load data from the pickle file\n",
    "with open('../dataset/ref_valid.pkl', 'rb') as file:\n",
    "    ref_sentences = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    " # function to invoke the training\n",
    "def train_model(start_epoch, eval_loss, loaders, optimizer, check_pt_path, best_model_path):\n",
    "    best_eval_loss = eval_loss\n",
    "    print(\"Model training started...\")\n",
    "    for epoch in range(start_epoch, N_EPOCHS):\n",
    "        print(f\"Epoch {epoch} running...\")\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_train_loss = 0\n",
    "        epoch_eval_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(loaders[0]):\n",
    "            src_tensors, src_attn_tensors, tgt_tensors, tgt_attn_tensors, labels = encode_batch(batch)\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            loss = model(input_ids = src_tensors.to(device),\n",
    "                            decoder_input_ids = tgt_tensors.to(device),\n",
    "                            attention_mask = src_attn_tensors.to(device),\n",
    "                            decoder_attention_mask = tgt_attn_tensors.to(device),\n",
    "                            labels = labels.to(device))[0]\n",
    "            if step == 0:\n",
    "                epoch_train_loss = loss.item()\n",
    "            else:\n",
    "                epoch_train_loss = (1/2.0)*(epoch_train_loss + loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (step+1) % LOG_EVERY == 0:\n",
    "                print(f'Epoch: {epoch} | iter: {step+1} | avg. train loss: {epoch_train_loss} | time elapsed: {time.time() - epoch_start_time}')\n",
    "                logging.info(f'Epoch: {epoch} | iter: {step+1} | avg. train loss: {epoch_train_loss} | time elapsed: {time.time() - epoch_start_time}')\n",
    "\n",
    "        eval_start_time = time.time()\n",
    "        epoch_eval_loss, bleu_score, sari_score = evaluate(loaders[1], epoch_eval_loss)\n",
    "        epoch_eval_loss = epoch_eval_loss/TRAIN_BATCH_SIZE\n",
    "        print(f'Completed Epoch: {epoch} | avg. eval loss: {epoch_eval_loss:.5f} | blue score: {bleu_score} | Sari score: {sari_score} | time elapsed: {time.time() - eval_start_time}')\n",
    "        logging.info(f'Completed Epoch: {epoch} | avg. eval loss: {epoch_eval_loss:.5f} | blue score: {bleu_score}| Sari score: {sari_score} | time elapsed: {time.time() - eval_start_time}')\n",
    "\n",
    "        check_pt = {\n",
    "            'epoch': epoch+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'eval_loss': epoch_eval_loss,\n",
    "            'sari_score': sari_score,\n",
    "            'bleu_score': bleu_score\n",
    "        }\n",
    "        check_pt_time = time.time()\n",
    "        print(\"Saving Checkpoint.......\")\n",
    "        if epoch_eval_loss < best_eval_loss:\n",
    "            print(\"New best model found\")\n",
    "            logging.info(f\"New best model found\")\n",
    "            best_eval_loss = epoch_eval_loss\n",
    "            save_model_checkpt(check_pt, True, check_pt_path, best_model_path)\n",
    "        else:\n",
    "            save_model_checkpt(check_pt, False, check_pt_path, best_model_path)\n",
    "        print(f\"Checkpoint saved successfully with time: {time.time() - check_pt_time}\")\n",
    "        logging.info(f\"Checkpoint saved successfully with time: {time.time() - check_pt_time}\")\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(base_path=\"../\",\n",
    "          src_train=\"dataset/src_train.txt\",\n",
    "          tgt_train=\"dataset/tgt_train.txt\",\n",
    "          src_valid=\"dataset/src_valid.txt\",\n",
    "          tgt_valid=\"dataset/tgt_valid.txt\",\n",
    "          ref_valid=\"dataset/ref_valid.pkl\",\n",
    "          best_model=\"best_model/model.pt\",\n",
    "          checkpoint_path=\"checkpoint/model_ckpt.pt\", seed=123):\n",
    "\n",
    "    print(\"Loading datasets...\")\n",
    "    #max_samples = 10000\n",
    "    train_dataset = WikiDataset(base_path + src_train, base_path + tgt_train)\n",
    "    valid_dataset = WikiDataset(base_path + src_valid, base_path + tgt_valid, base_path + ref_valid, ref=True)\n",
    "    print(\"Dataset loaded successfully\")\n",
    "\n",
    "    train_dl = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "    valid_dl = DataLoader(valid_dataset, batch_size=TRAIN_BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n",
    "    if os.path.exists(base_path + checkpoint_path):\n",
    "        optimizer, eval_loss, start_epoch = load_checkpt(base_path + checkpoint_path, optimizer)\n",
    "        print(f\"Loading model from checkpoint with start epoch: {start_epoch} and loss: {eval_loss}\")\n",
    "        logging.info(f\"Model loaded from saved checkpoint with start epoch: {start_epoch} and loss: {eval_loss}\")\n",
    "\n",
    "    return \"set-up done\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.3.crossattention.q_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.4.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.2.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.1.crossattention.c_proj.bias', 'h.11.crossattention.q_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.weight', 'h.11.crossattention.c_attn.weight', 'h.8.ln_cross_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.bias', 'h.5.crossattention.q_attn.weight', 'h.10.crossattention.c_attn.weight', 'h.6.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.6.ln_cross_attn.bias', 'h.7.ln_cross_attn.bias', 'h.11.crossattention.c_attn.bias', 'h.5.ln_cross_attn.weight', 'h.2.crossattention.q_attn.weight', 'h.0.ln_cross_attn.weight', 'h.6.ln_cross_attn.weight', 'h.3.crossattention.q_attn.bias', 'h.2.crossattention.c_proj.weight', 'h.5.ln_cross_attn.bias', 'h.9.crossattention.q_attn.bias', 'h.0.ln_cross_attn.bias', 'h.3.crossattention.c_proj.bias', 'h.9.crossattention.c_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.6.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.bias', 'h.8.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.bias', 'h.0.crossattention.q_attn.bias', 'h.1.crossattention.c_proj.weight', 'h.11.crossattention.c_proj.bias', 'h.9.crossattention.q_attn.weight', 'h.4.ln_cross_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.2.ln_cross_attn.weight', 'h.11.crossattention.c_proj.weight', 'h.11.ln_cross_attn.weight', 'h.1.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.4.crossattention.c_proj.weight', 'h.4.ln_cross_attn.bias', 'h.5.crossattention.q_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.0.crossattention.c_attn.weight', 'h.10.crossattention.q_attn.bias', 'h.1.ln_cross_attn.bias', 'h.2.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.bias', 'h.3.ln_cross_attn.bias', 'h.6.crossattention.c_proj.weight', 'h.7.crossattention.c_proj.bias', 'h.9.ln_cross_attn.weight', 'h.7.ln_cross_attn.weight', 'h.5.crossattention.c_proj.weight', 'h.6.crossattention.c_proj.bias', 'h.0.crossattention.c_attn.bias', 'h.9.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.bias', 'h.4.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.q_attn.weight', 'h.3.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.weight', 'h.8.crossattention.q_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.7.crossattention.q_attn.bias', 'h.10.ln_cross_attn.bias', 'h.9.ln_cross_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.3.ln_cross_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.6.crossattention.c_attn.weight', 'h.11.crossattention.q_attn.bias', 'h.8.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu as device\n",
      "Loading datasets...\n",
      "Dataset loaded successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'set-up done'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################### constants\n",
    "TRAIN_BATCH_SIZE = 3 # kept as in the source paper\n",
    "N_EPOCHS = 1 # they trained with 20 epochs\n",
    "max_token_len = 80 # can be configured as needed (Classification data?)\n",
    "LOG_EVERY = 100\n",
    "\n",
    "################### encode source and target data using encode_batch function\n",
    "source_train_data_encoded = encode_batch((src_train))\n",
    "target_train_data_encoded = encode_batch((tgt_train))\n",
    "\n",
    "################### logging during training\n",
    "'''\n",
    "Configures logging settings using the logging.basicConfig method,\n",
    "to write log messages to a file named \"log_file.log\" with the INFO level.\n",
    "format: timestamp, log level, message\n",
    "'''\n",
    "logging.basicConfig(filename=\"log_file.log\", level=logging.INFO,\n",
    "                format=\"%(asctime)s:%(levelname)s: %(message)s\")\n",
    "CONTEXT_SETTINGS = dict(help_option_names = ['-h', '--help'])\n",
    "\n",
    "################### collate data batches\n",
    "def collate_fn(batch):\n",
    "    '''\n",
    "    custom collation function used in PyTorch for data preprocessing in DataLoader\n",
    "    It defines how individual samples within a batch are combined and transformed\n",
    "    before being fed into the model during training and evaluation.\n",
    "    '''\n",
    "    data_list, label_list, ref_list = [], [], []\n",
    "    for _data, _label, _ref in batch:\n",
    "        data_list.append(_data)\n",
    "        label_list.append(_label)\n",
    "        ref_list.append(_ref)\n",
    "    return data_list, label_list, ref_list\n",
    "\n",
    "################### initialize models\n",
    "'''\n",
    "Initializes the model as an Encoder-Decoder architecture using the EncoderDecoderModel from Hugging Face's transformers library.\n",
    "Model combines an encoder (BERT in this case) and a decoder (GPT-2 in this case) for sequence-to-sequence task.\n",
    "'''\n",
    "bert_model = BertModel.from_pretrained('bert-base-cased')\n",
    "gpt2_model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "################### configuration\n",
    "start_token_id = bert_tokenizer.cls_token_id\n",
    "end_token_id = gpt2_tokenizer.eos_token_id\n",
    "\n",
    "################### initialize the Encoder-Decoder model with cross-attention enabled\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')\n",
    "\n",
    "################### update configuration for the model\n",
    "'''\n",
    "Updates the configuration for the encoder-decoder model:\n",
    "- Configure the start and end token IDs for the encoder and decoder\n",
    "- Set the maximum length for tokenized sequences\n",
    "- Enable cross-attention in the model (model.config.add_cross_attention = True),\n",
    "  allowing the decoder to attend to different parts of the encoded input sequence during decoding.\n",
    "'''\n",
    "model.config.decoder_start_token_id = start_token_id\n",
    "model.config.eos_token_id = end_token_id\n",
    "model.config.max_length = max_token_len\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.add_cross_attention = True\n",
    "\n",
    "#################### set device\n",
    "'''\n",
    "Checks for GPU availability,\n",
    "selects the appropriate device (\"cuda\" or \"cpu\"),\n",
    "prints the chosen device,\n",
    "and then moves the model to that device for computation.\n",
    "TAllows the code to leverage GPU acceleration if a compatible GPU is available,\n",
    "enhancing computational performance for deep learning tasks.\n",
    "'''\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} as device\")\n",
    "model.to(device)\n",
    "\n",
    "################## loss function and optimizer\n",
    "'''\n",
    "Loss Function\n",
    "- nn.CrossEntropyLoss():\n",
    "Initializes the cross-entropy loss function from the torch.nn module.\n",
    "Cross-entropy loss is commonly used for multi-class classification tasks, where the model's output represents class probabilities.\n",
    "\n",
    "Optimizer:\n",
    "- torch.optim.Adam(model.parameters(), lr=0.001):\n",
    "Initializes the Adam optimizer from the torch.optim module.\n",
    "It optimizes the model's parameters (model.parameters()) using the Adam optimization algorithm, a popular variant of stochastic gradient descent (SGD).\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "train(base_path=\"../\",\n",
    "      src_train=\"dataset/selfgen_src_train.txt\",\n",
    "      tgt_train=\"dataset/selfgen_tgt_train.txt\",\n",
    "      src_valid=\"dataset/src_valid.txt\",\n",
    "      tgt_valid=\"dataset/tgt_valid.txt\",\n",
    "      ref_valid=\"dataset/ref_valid.pkl\",\n",
    "      best_model=\"best_model/model.pt\",\n",
    "      checkpoint_path=\"checkpoint/model_ckpt.pt\",\n",
    "      seed=123)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderDecoderModel(\n",
      "  (encoder): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (decoder): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(50257, 768)\n",
      "      (wpe): Embedding(1024, 768)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-11): 12 x GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (crossattention): GPT2Attention(\n",
      "            (c_attn): Conv1D()\n",
      "            (q_attn): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_cross_attn): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D()\n",
      "            (c_proj): Conv1D()\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training started...\n",
      "Epoch 0 running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christine/.pyenv/versions/3.10.6/envs/neuroCraft/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_model(start_epoch=0,\n",
    "            eval_loss=0,\n",
    "            loaders=load_dataset(src_path=\"../dataset/src_train.txt\",\n",
    "                                 tgt_path=\"../dataset/tgt_train.txt\",\n",
    "                                 ref=False\n",
    "                                 ),\n",
    "            optimizer=optimizer,\n",
    "            check_pt_path=\"../checkpoint\",\n",
    "            best_model_path=\"../best_model\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for testing the model\n",
    "def test(base_path=\"../\",\n",
    "         src_test=\"dataset/src_test.txt\",\n",
    "         tgt_test=\"dataset/tgt_test.txt\",\n",
    "         ref_test=\"dataset/ref_test.pkl\",\n",
    "         best_model=\"best_model/model.pt\"):\n",
    "    '''\n",
    "    This function conducts testing of a trained model on a provided test dataset. It follows these steps:\n",
    "\n",
    "    1. Initialization:\n",
    "    - Logging and Setup:\n",
    "    Logs the initiation of the testing process and loads the best model's checkpoint.\n",
    "\n",
    "    2.*Model Preparation:\n",
    "    - Model Configuration:\n",
    "    Sets the model to evaluation mode (`model.eval()`).\n",
    "    - Dataset Loading:\n",
    "    Loads the test dataset using paths provided or defaults, creating a `WikiDataset` for testing.\n",
    "\n",
    "    3. Testing Process:\n",
    "    - Data Loading:\n",
    "    Creates a DataLoader for the test dataset with predefined batch size and collation function.\n",
    "    - Evaluation:\n",
    "    Evaluates the model on the test dataset, capturing test loss, BLEU score, and SARI score.\n",
    "\n",
    "    4. Results and Reporting:\n",
    "    - Metrics Calculation:\n",
    "    Calculates average evaluation loss, BLEU score, and SARI score, dividing the loss by the batch size.\n",
    "    - Reporting:\n",
    "    Prints and logs the calculated metrics and the elapsed time for the testing process.\n",
    "\n",
    "    5. Completion:\n",
    "    - inalization:\n",
    "    Prints \"Test Complete!\" to indicate the conclusion of the testing phase.\n",
    "    '''\n",
    "\n",
    "    print(\"Testing Model module executing...\")\n",
    "    logging.info(\"Test module invoked.\")\n",
    "\n",
    "    _, _, _ = load_checkpt(base_path + best_model)\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    model.eval()\n",
    "    test_dataset = WikiDataset(base_path + src_test, base_path + tgt_test, base_path + ref_test, ref=True)\n",
    "    test_dl = DataLoader(test_dataset, batch_size=TRAIN_BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "    test_start_time = time.time()\n",
    "    test_loss, bleu_score, sari_score = evaluate(test_dl, 0)\n",
    "    test_loss = test_loss / TRAIN_BATCH_SIZE\n",
    "\n",
    "    print(f'Avg. eval loss: {test_loss:.5f} | blue score: {bleu_score} | sari score: {sari_score} | time elapsed: {time.time() - test_start_time}')\n",
    "    logging.info(f'Avg. eval loss: {test_loss:.5f} | blue score: {bleu_score} | sari score: {sari_score} | time elapsed: {time.time() - test_start_time}')\n",
    "\n",
    "    print(\"Test Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpt(checkpt_path, model, optimizer=None, device='cpu'):\n",
    "    checkpoint = torch.load(checkpt_path, map_location=device)\n",
    "    if optimizer is not None:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    return model, optimizer, checkpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(base_path=\"../\",\n",
    "         src_test=\"dataset/src_test.txt\",\n",
    "         tgt_test=\"dataset/tgt_test.txt\",\n",
    "         ref_test=\"dataset/ref_test.pkl\",\n",
    "         best_model=\"checkpoint\",\n",
    "         model=None,\n",
    "         optimizer=None):\n",
    "    '''\n",
    "    This function conducts testing of a trained model on a provided test dataset. It follows these steps:\n",
    "\n",
    "    1. Initialization:\n",
    "    - Logging and Setup:\n",
    "    Logs the initiation of the testing process and loads the best model's checkpoint.\n",
    "\n",
    "    2.*Model Preparation:\n",
    "    - Model Configuration:\n",
    "    Sets the model to evaluation mode (`model.eval()`).\n",
    "    - Dataset Loading:\n",
    "    Loads the test dataset using paths provided or defaults, creating a `WikiDataset` for testing.\n",
    "\n",
    "    3. Testing Process:\n",
    "    - Data Loading:\n",
    "    Creates a DataLoader for the test dataset with predefined batch size and collation function.\n",
    "    - Evaluation:\n",
    "    Evaluates the model on the test dataset, capturing test loss, BLEU score, and SARI score.\n",
    "\n",
    "    4. Results and Reporting:\n",
    "    - Metrics Calculation:\n",
    "    Calculates average evaluation loss, BLEU score, and SARI score, dividing the loss by the batch size.\n",
    "    - Reporting:\n",
    "    Prints and logs the calculated metrics and the elapsed time for the testing process.\n",
    "\n",
    "    5. Completion:\n",
    "    - Initialization:\n",
    "    Prints \"Test Complete!\" to indicate the conclusion of the testing phase.\n",
    "    '''\n",
    "\n",
    "    print(\"Testing Model module executing...\")\n",
    "    logging.info(\"Test module invoked.\")\n",
    "\n",
    "    # Load checkpoint and update the model and optimizer\n",
    "    model, optimizer, _ = load_checkpt(base_path + best_model, model, optimizer)\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    model.eval()\n",
    "    test_dataset = WikiDataset(base_path + src_test, base_path + tgt_test, base_path + ref_test, ref=True)\n",
    "    test_dl = DataLoader(test_dataset, batch_size=TRAIN_BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "    test_start_time = time.time()\n",
    "    test_loss, bleu_score, sari_score = evaluate(test_dl, 0)\n",
    "    test_loss = test_loss / TRAIN_BATCH_SIZE\n",
    "\n",
    "    print(f'Avg. eval loss: {test_loss:.5f} | BLEU score: {bleu_score} | SARI score: {sari_score} | time elapsed: {time.time() - test_start_time}')\n",
    "    logging.info(f'Avg. eval loss: {test_loss:.5f} | BLEU score: {bleu_score} | SARI score: {sari_score} | time elapsed: {time.time() - test_start_time}')\n",
    "\n",
    "    print(\"Test Complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Model module executing...\n",
      "Model loaded.\n",
      "hhhh a following of timehhh the singlehh to the amount spent effort spent by\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh otherh a messh a gianth to the shiphh\" and a it entire. the on andhh. of the Cruzh.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh beenhh thousandhh the campush\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhh thehh Hh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhh,hh, thehhh online, is beenhhh the internethh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhh associatedh thehhhhhhhopathyh ah thehh hearth a conditionh beh ahhhhhhh.\n",
      "h the beh the.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh foundhh thehhh the trainh. Railh Saturdayhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhh. Martinhhhhh Holy-h fathers theh,\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhh Londonhhhhh )\n",
      " a younghhh tohh thehh the UKh. Hehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh thehhhhhhhhh ahhh parth theh.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh before Ihhh,h hishhhhh the work task ofhh thest and the and thehh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh the most toh had thathh theh heh. thehhh a.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh a additionalh for for his roleh thehh the film film,hh the otherh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh ithhhhhhh thehdayshhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh beenhhhhhh hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh asked first creatorh for the bandhh band, andh. who he after the album even got their single album.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh theh for long as he is she is his office of the people househ.hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh twohhhhhh that hehingeh a ahhay is done back\"hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh, theh a for a spot of the-- Hhh, the in a unlawful robbery. on the vicinity of thehh.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhh a ah ahh 5hh day. Sheh also hadh ahh had a lothhhsthhhhh the timehh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhlyh theh, thehhich.hhhh the north,, theth, ah from thehhhhhhh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h of who theonyms,hh and Johnhont, have their following \"h in which which, which by the abouth in Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh ahh the fatherh hish as a of theh followers supportersh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh a a days stages, The is the as the'shher, The is a after thehioneicohini, the Italian astronomerAmerican astronomerh The other is is known as the'suyhheh.\n",
      " is named after theophhhuygens,\n",
      " is the Greek astronomer. andhematich, astronomer. Hehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh somethinghhhhhhhh thehh thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhh, the H, the, thehh The\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh the inventionhhh's a same common- methodhh theh technologyh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh thehhhhh andhhhhh, thehhh comeh. well officials. of people was them as be'as thehhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhh the thanhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh: theh a \"h the of a\" the days\" and he was met hish and himh he-h the middle ofh he up the open space very- \" and and placeh was us story of the book of other.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhh be theh- \" the himhhhhh the ishhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhh hehhh noth\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h,hhhhh, thehallhh thehh a new'sh theh,\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h a bit point for the to to get theh, andhhh thelderhhh,\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhh hehhh thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h the,hh has in thehhhh but other the likes-apenaureh, Behind, The Endh and Behind,, Thees,, and the Behind III The War War, which which he was ahhh Hh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh thehhh theh, thehhh theh theh the extendedh after.ingsh. thehh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh Polyh,hhhhh polyhh the known used for polyhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh on the interneth, the first- Sundayh,hh its 23, 1994.\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh theh they have the drumsirenh is call to do ahhhh.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhh byhhhh 18, 2014h Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhed,hhhhhhhhhh thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh to theh washh theh to theh. thenhh and the the theoorh. then at the west side of the. George's theoor..\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh-a-half hours,, I's still with die the the. the streets of the Ireland. a next two years. and in a housesages and orphan from the and no of people children people.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h also ith thehh's a to from the godsVD, a for his he forth by a the in the pastist. the. which past Union.\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hwesternhh- the most-hidhhhshh, Hayalam, andh anduguh Hundh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh out in a, the policehhh,h allegedly alleged-h wasolationh behavior.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh fee ofh for thehation for theh and if workhh by the person, and, or, or other person of the medicalh.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh thehhhhh,\n",
      "  are also lot- of ofh-language mangah, thehpe's and to series of the takes published by the-sh theh to the was to to thehaw. the Lastings\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h of of thehhites are the the, andh of the, the Democratic States. Theyennonites are live in the proximity in the least two countries. Earth continents. including inh the world of the countries.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh of the warth century,hhhhh the known for its highh,.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh, it the tongue of thecerhhh foodsus membranes which is the and and the out of the mouthh. the to keep theh otherites.h is to the,\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh Hhhhhh Hhh win the matchhh.hh00hhh Hh a shortQh drophhhhhhhhhhhhhhhh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh dayhh redhh brownhhhhhhhh ahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhh hehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhh to, thehhhhh the foresth\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh name, the can thing-fully theh be theh \" of in the country of byh by and in. by a person or and\n",
      " the, the would a to the \" of are the government.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhh faceh heh glance, hehh be it.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh ahhh but ith by the 'h,h and hhatic theh Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h, worldhhhh been themselveshh whohh, andwynhhh and Hh andhh and Hh, andhh, andhh andine'hhh thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h the if the who are the dungeonhways the certain, a right of the h they is no who and to the.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhh thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh HhhhH Institute University ),\n",
      " been that the \" for ah is that the is are not sustainableologically sustainable. and exampleing thehhh, thehaterhshum,\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h ahh on the+h and in in the website,h process- of\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h past Union,h Sovietviks hadh of thehhs most buildingshal 'hh andhh and the.'s ( theizhhh19h ).1809 )..\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhh ah, in thehh the in theh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh a ministerhh theh the- from, a as as 1948- 1949. He\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h:hhhhh8\n",
      "h\n",
      "hhhh in theanh the H part of the Zealand Wales. the. thehhoresh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhh also from thehh thath the Hh. which the the the ofh thehh the future Treatyh the the lot-hdrawal agreementhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh ah a Day, the, the to a arrival of the new River and York and Hhhh. Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhacedhhh whichh lifeh and a betterh to\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh the is is a becausehh's that's character and and he theaining hish'sh, he's a more for the, is for\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h--h in in with in with charges own, with treasoniny, murder, the part in the overthrow-. the subsequentsadju War .\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhh veryagellh formozoanh that isizes the infectces in the gut intestine. Journal aoutia,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh the ocean, theh thehhhhhh theh thehthhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh the,h, the is the of a and varioush thehshh,, composed of of a of ice waterh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh the, mehh his project developmenth the cityham.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhh thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhh thehhhh offh side of an animal orhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh in  thehh Hhh, hish the timehhhh by the 11, 2014. the the the,\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh veryh the city boroughh Hroyh, and. Hehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh numberh the mosthh'sh been been a with a \"hhhhhh of hish.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh theh of the lasthhh thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh Times Police Authorityh Department said the little- agency that the York City, the to 1956. Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhh lot good numbery waterhhh, the sideh Ithhatedh the waterh is be of the,cap, Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh thehhhhh and which the player has to set of fourh to the game game. Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh that the peoplehh are have been thish not bit of the sameh I, other of is your from have be part part of the conspiracy.\n",
      "hh also you who know no been. person.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh a ahh a hairhh andh usually to be a extremelyh a godhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh the newhhhhh guitarh and andhh guitar ) and andhhh drums ), and Hhh ( drums ).hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh Norwayh the agreement of the, the the and not to accept the war king by the Scottishh treaty treaty.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh acrosshhh North\n",
      " also involved into the coasth, found to coasthh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhes, Hh, the ofh the westh Verdes,, Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh own \"hh and from the Berlin's government and had a own postalhh theh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhh ith themh from thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh peoplehh to theh Hehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhh restaurant, ah, and ah coffee store,\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh, thehh the fromh-, h the Creative-profitrictive license- license. and the the time, companies- had been their limits. and the was to be free used open system. the. otherh.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh said that thethe is thehine that fault in led the the water to escape from else the numbers, the new Coast, and, the parts of the Africa.\"\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh-hhhhette. a-hh and the theh histhyear-old,-hhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhh thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh to natureh the other, a coloredh hairh red,h deeph lighth moons amountsh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh ahhh they're fromhhh they ahh they senth\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h the otherh who, or or or, orph or anyone else involved to the, is fact way,, is be considered for the edits of this site. in this linked to this pagespages.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh a greath for the studentsh the United States, Ithhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh, a artist-- player who Hehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh.h, and a \" of the would known \"hh the moonh thehh Septemberh, 1963.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhh lot of damage, thehh Keysh it loth water ah 20hh the ground.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh of the islandhhh the from the resthh the the mainlandhbrides, the seah sea of the Northh, and watershh, the Hh the Seabrides. Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh day, the, the thehthh the first of thealinghh, theh and which thehhaldhh his lifeh. was wife was Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh the governmenthhhh in the H-Star Gameh the All of the season season..hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhh said as thepperh. the H and firstorh thehh and  was became the'sh the Britain.\n",
      "\n",
      "hhhhh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh and he fatherh andhhh and a veryh ah and his father was H,ackh was a singerh. theh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhh very of sociologyhh the University of Osloh and he daughter ishh is a is born after thehhh is a professor scientistochemist and\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h \"hh is used up thehhh who theh had\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhh)h of theh,\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh to be a onlyrian god of the ancienth, in hhh from the hh,zhh which means the name ofhh whicheneh and to the Assycients to the of thehh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhh Stateshhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhh is a singleh thehum ( thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h own timeh marriage, the NBA Union, was the the bandhboard the to 1991, theocolh theia,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhh ithhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhh \" \"hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h,, the are a great to \" \"h, is theh to secondary sources to establish the historyh. Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhh ithhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhh thehhashhhhh others Hhhh the, Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh: a hourshh him downhh. hehnth couldn away of time.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhh thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhh theh placeh the ofershh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh the manhh not as thehhh hhhhhh Theh\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhh found latesth:hhers. andh runtimeh, ahh.h\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh most character to the, andh second place. and means -bodiedh can expected to enter-hh the. a a lifetime.hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhh, thehhh and,hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h thehhhhh, I, and the things, the worldithh world World, be sold for ah pay money.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh theh h are been been so as be ah the other, now. andh has thought that theh been distinct that the with the, and thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhh a of of ahh were made together a form ofh, were a on wereuth intowards the sidesh back.. and the back and Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhh thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh theyhhh seem difficulth be ah ithhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhh the middlehhhh theh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh thehh on the facthh by the bookh Lukeh and,h Ihh Lukeh and - the\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh Syriahh hhhh theh by thehh thehhh.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh ah amounthh the gamehh,hh well characterh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh to the de and his a, and became a kill for workmost of the, the Frenchursions of the peoples-. Hehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh has shown that with more likely to be a lack than they is a else is/ she is. and, or is or is about.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh way a thingsh thehhathah\n",
      "h of thehhhh andhh theh of Hhhatha, thehhatha, Deatharture.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh are a interesting- follow interface,. a well the the to to the. cultureh.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh leave the Hussein the forh the he he US of his a trial were been been been revealed.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh I is became called called thethehhh is a of the city where the battlehhh meets.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh, beenhhhhhhhhhhhhhhhh heh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh a a manh the Househh and he was as a officeriser, the least theth to his death.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh,hh a to be at ahhhhh thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhh thehh.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh tohhh thehhh thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh ofh thehh are have have thehershh Theyhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h characterh the film is that thehies the forces, is out on massive of events that will to the deathin of the father. resources. the a from his. Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhopic studies have shown thath theration andh theica inh is to the ahhh,.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhh hehh that haveh 2 andhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhh a samehhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h peoplehh beh, they the and women will they is be the rules,hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh-hh the gamehh into and started theh theh Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh to beh place in the groundh,h theh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh theh, the oneh been added in of the river city of HhNh '-day,hh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhH-hhsh a smalle of the middleharyh of the cityquehhandieh of the France. Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h past since the death year, theth, the he was to for the warquest of thehh, heh in the father-, the small in the mountains mountains of\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhh a first-, to win on firsthhh the-h\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh a veryhh the Frenchhhhhmonement du Paris France. a the city ofvence dedepes-Côteh'hhh\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h,h are sense interestinghh for ahhh andhh hhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h a, firsthhh,hhhh, theh of theh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhh theyhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh-,hhh gameh to the gameh-hhhh-h\"hh a a to thehhhs standards styleh.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhh thehhh, theillerh deonhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhh arehhhhhhhhhhhh is veryhh behhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhh )\n",
      " a young, theh New. aimshh hish- of thehh. andhh the hh. ( Hehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh werehHhhh Hh Hhh killed to of a fire battleh\n",
      " was reported theh Saturday 11, 1945,\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h be beodge the from it the into the ground.. a to get it contents. then for it body meal..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h is most-h, a first Fighting Night, and it MMA martialer was Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh, thehth,hh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhh Afghanistanistanhlineshh north,hh the south, and to the south, east easthhhhh, the east,\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh 1930, the of buildingsites this and exampleh vision, were andh department, and hospital for the medical sciences, ahh, a-, a,, and a large halls for built. the area.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhh the Hhh Hhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhh thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh, the waysh like youhh. vibr a soundsh and it timeh to a different soundhhhhh andhhh h.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhh are, in the gamehhhh. the world. Hhh of which countryese region and also the middle areas of the regionese Alps. in in the borderon of Bernle and anderhh andlh, andlandersenh, Hichh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h it greath me Obama, the, \" Depressionances, the White House,, and was a in the. the same Theater Company.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh the difficulthh it thehh on ahh a lot place wellh.h a truckherh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh it theyhhh. shehhhhh. the groundh of thehh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh me. fighthh theh to otherh.hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh to to mehhh talk him the school schoolatory. he he went back ash of the schoolhhhh. Hehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh kindh ahh JavaScripthh is be usedh by using the elementh the app pagehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh had a than than the other two,hh but's has throughhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhh hehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh much the a differencehh substance is in to make the personh, the ahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh,\n",
      " a action film wrestling match-per-view.. by the Wrestling Entertainment.W ) and the will take place on July 29rd 2009 at the Center in Los Angeles, California. Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh they bodiesh, and lot that 'atich. which inally or or in thehh h,oeshhh or ash oftenh hatichhhhhh hhumentalhhhhosh).).hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh ahhhhh taking-. and same was that vote-person finish.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhh the sameh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh of-h of theopotamerica, is the majorh diverse regionh the northh of the Centralhre to the subth theuch. Ithhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh I is  meh most feeling of a,,h, have,h theh and't toh that the audienceh have been like.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhh foundhhhh so like. themhh for.hh a only as why the hhh theh redhh red).).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hersh by, water, and, and, clothing, andages, other variety-h.. the who risk camp.\n",
      "hhhhhhhhhhhhhh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh of thehhh thehh, the are livehh the.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh the own's blogh ahhh,hhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh ahhh a a differenth different other a thing, then another,\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h gamehhh, theth, the the battlehh had theh was a by over long time by the- Persianhh were from the Hhate. Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh daysh thehhhhhhh 'hhhhhhhhhhh\n",
      " other title was Thehhhh Otherhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhh, Braunhhhh ) 8 March 1912 )\n",
      " a first- of wife the the time period, the of theh, Shehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh the end to theh the the resth the, Andhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhh by Marvelh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh thehhhhh thehh, He was a in a tomb of thehhhhhhhh 'hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh Hhh,h.1-h not have a with theh, they of them people were did been in laborersh the streets were also to get to each, day.\n",
      "hhhhhhhhhhhhh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "h\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh the sameh the countryh, the the isaves the Baltic of Hathhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh clothh whitehh been addedh makeate theh andh otherhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhh youhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhh theyhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh a High more Brotherhood, the likely to the-h countries to the world Middle East to have suicide, a place to recruit their relations in\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh See of p with the Holy,olytus, was said ancient in the, the with story of theymhsalm and thehhh, he hy verse of theh hyhh the music. Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh this subject matter been expressed up in the years-h the). but the of agreed by the media scholarsh andhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhh a head cage match\n",
      " become theh Tagcontinental Championship at buth was theishi with aakerh theishi with a bodyash.h\n",
      "\n",
      "\n",
      "hhhhhhhhh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhh ofh thehhhh the officialh official.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhh Ih ahh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhukuhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh to have ah the goddesshh for \"hh of the sunhheshhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhh thehhh the next of Representativeshhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhh theh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h been.S. been Corps,. the,, the U Francisco- Pacific reported that the pilotsh that existence of theh,hombs in the civilianshmen the Battle stages of the. The\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh Luke, Lukeh John) all likely in, in theh\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhh 1945. was from a degree in Economicsh Hehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h ah peoplehhh thehhh, and of memberswichhh Hh, and the stadiumh Andhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhh-h the a newh deviceh the namehh namehhhhhh be foundh they.hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh hhh thehh hah a a to his body of andh.. the ground. then it ballh his his might.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh of thehhhh and Berlinzh\n",
      " is the the member lecturer at the Institute for European Studies and in Frankfurtnh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh, hehhh that hehhh \" of the \" people, and that one by the death that he only people know him even about him. artist.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh the story manh the up theese,, police otherhhhh the groupese army group from the northar-Sianhoud tribeh the north partawahhh. the. Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhchnichhh of peoplehhes,h as theh and the temple, and theoh iniosh who also admireded the the their their and the United-speaking world. the times. 1860s.hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh to a pictures videosh theH's\" and theh'h \"I Dance\"\" by Katy.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhh of thehhhh I ofhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh littleh timehhh, wellhhhhh ah of up to tenh as a Front-Cover Text andhh of the page. things Texts.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h proximity to the other the three, respective and the case Election, the parties of divided to follows:\n",
      "h Party (HDP) Freend, Free Democratic Party'sh ( (HHH 1 members; and Democratic Party (SD 2 members; and thehhs Party (HHP). 2 member.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h theh9hh the study survey call andhh a by bestth who be in the show show.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh,hh away the direction ofhon condition, the law, a of the Church of the and, thehh, and theforth he authority was on the with- the of the Church, the Holy, and withh ) with that State States and Great,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h-, thehhh a by saying hishhs hair showhh the toh in and the'ss \" andh the \" show to thehh,\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h theh wash to talk the world as the of had been up around it. a. the. and theanto was still really by the people States. its languages organizations.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhh.hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhh mosthhhhhh.hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhh\" the Year\" after a.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh lothh up new,hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh the hhhhh.hh the and andas, the to the people state political sectorsshoots of the. create to to theow the interests of the people people.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhh.ian-h and a hehesarhhhhh, ah were also the goodhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h-h Prixhhh, place in thehhh therugh, Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh said a lothhh ahhhhhh the York Cityhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhberthh a French of the Hichheh of the city of theuyhh-H-Hhh the France. Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhh thehh State, Hehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h is no morehh,h landh the long result of the be aed,\n",
      " is a theh not in the highway,h to a areahh.\n",
      "hpasshh a to be the ah\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhh ahhh you a sorth is a lot textureh the back of\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh a great of the and lungs, help create energy energy and heat energy.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhh ah,\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh the wordhh the peoplehhhh's not to the the personnelations the islandh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh were written in day day, the paper calledbiology Analysisations of these theequality,, theh al. Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhh Hhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh behhhhgeryhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh fewh ofh by the hillhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh in in thehhhh a man wrestlerhhh and h been in thehhh the USh,\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh the newhh ash the, inshhh'ss \"hhh \"h openinghhhhhh \" 19, 2014hh \"h the countryh uph than in of the's \" \"hh Marchhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh hehhhhhhh of 1hh,hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhHhhhhhh, the French-iser, andist, andh songagh Shehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhh in theh and I not published in the ofh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh reportedh of the otherh thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhh thehhhh thehhhh,hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhh thehhhhhhh the h-hh theh thehh theth. andhhhhh, Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh bed Statesh heh, was I husband were to theh Shehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhh the gameioh..h a samehhh.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh:hh go him out the missing \". Americah. he's a of hehh be to kill the city. the to kill the.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh in fighth enemyhh,\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh be a named as the name, a a a-, if the is longer exists the market. and that town to the so is exists.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh the crowd Isles scene sceneh the''s, the oneised by a like by the rock-,, the sames and's.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh mehhh gohhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhh of hhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh in the previoushhhhh theh. Hh his H. Hh, the,\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhh forhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh land palaceh thehhh a.5% of the vote vote.\n",
      "hhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh of the problemhh'hh was through the way side of the highwayhh Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh themselveshh as be a of the holy sites. all and theh and and thehh\n",
      "\n",
      "\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhh the of the \"eoushhhhhh the H of H.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhh ahh, ahes to the,1939, the Nazis was completed to the new location.\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h the-five,h, the firsth the, and one wereenders were been ined into\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhh the firsth the storm Guardh was down centerh thegrounds, the Easthh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hh to a otherhhh theh, the, and the inh. Donkey Kong.. Hh the Sameh. 1995. and though the game was actually knownhh\"\"\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h battles, 1990s, the Unitedsw wet andized more out more built to the other. theh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h been been said case crisish thehhh with the rest of the of distribution of theh,\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhh, a small of the artist artisth, Gihhhhh Thehhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h'thhhhh is it a most single in the, and it largestventh largest in the world.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "h\n",
      "\n",
      "\n",
      "\n",
      "h\n",
      "\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhanceh smallhh be used into thehh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhh theyhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhh called to beh notread in the, Ihhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "h:hhh cityhhh theh transporth and rail, otherwaysh Ithhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhh the bodyhh the countryhhh of the damageh fall the\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "hhhhhhhh 'helhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "Avg. eval loss: 3.86803 | BLEU score: 0.0035416612217382187 | SARI score: 0.12474570292173764 | time elapsed: 60.16937971115112\n",
      "Test Complete!\n"
     ]
    }
   ],
   "source": [
    "if model is not None:\n",
    "    # apply test function with model and optimizer\n",
    "    test(base_path=\"../\",\n",
    "         src_test=\"dataset/src_test.txt\",\n",
    "         tgt_test=\"dataset/tgt_test.txt\",\n",
    "         ref_test=\"dataset/ref_test.pkl\",\n",
    "         best_model=\"checkpoint\",\n",
    "         model=model,\n",
    "         optimizer=optimizer)\n",
    "else:\n",
    "    print(\"Model is None. Please initialize the model before calling the test function.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the Model for Simplification of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpt(checkpt_path, optimizer=None):\n",
    "    checkpoint = torch.load(checkpt_path)\n",
    "    if device == \"cpu\":\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"], map_location=torch.device(\"cpu\"))\n",
    "        if optimizer is not None:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"], map_location=torch.device(\"cpu\"))\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        if optimizer is not None:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    eval_loss = checkpoint[\"eval_loss\"]\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "    return optimizer, eval_loss, epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify new sentences\n",
    "def decode(base_path=\"../\",\n",
    "           src_file=\"dataset/src_file.txt\",\n",
    "           best_model=\"checkpoint\",\n",
    "           output=\"decoded.txt\"):\n",
    "    print(\"Simplification of sentences is executing...\")\n",
    "    logging.info(\"Decode module invoked.\")\n",
    "\n",
    "    checkpoint = torch.load(base_path + best_model, map_location=torch.device('cpu'))  # Load checkpoint\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])  # Load model state_dict\n",
    "\n",
    "    model.eval()\n",
    "    dataset = WikiDataset(base_path + src_file)\n",
    "    predicted_list = []\n",
    "    sent_tensors = encode_sent(dataset.src)\n",
    "\n",
    "    print(\"Decoding Sentences...\")\n",
    "    for sent in sent_tensors:\n",
    "        with torch.no_grad():\n",
    "            predicted = model.generate(sent[0].to(device), attention_mask=sent[1].to(device), decoder_start_token_id=model.config.decoder.decoder_start_token_id)\n",
    "            predicted_list.append(predicted.squeeze())\n",
    "\n",
    "    output_sentences = decode_sent_tokens(predicted_list)\n",
    "    print(output_sentences)\n",
    "\n",
    "    # check the length of output_sentences\n",
    "    print(f\"Number of sentences decoded: {len(output_sentences)}\")\n",
    "\n",
    "    # write all decoded sentences to the file\n",
    "    with open(base_path + output, \"w\") as f:\n",
    "        for sentence in output_sentences:\n",
    "            f.write(sentence + \"\\n\")\n",
    "\n",
    "    print(\"Output file saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplification of sentences is executing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding Sentences...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "�hhh-hhmmhh.hh/hh,hh)hh),hh]hh\\u\\uh\\h\\t\\u/h\\r\\u\\/h\\nh\\d\\u-h\\uff\\uha\\uw\\u30a\\uhi\\u5e\\u6e\\\n",
      "�hhh-hhmmhh.hh/hh,hh)hh>hh\n",
      "\n",
      "hh:hh;hh?hh\\h\\uh\\i\\hh(hh),hh<h\\nh\\t\\u\\h(s\\u)h\\s\\h,\\u/h\n",
      "�hhhkhhmhh-hhkhhh,hh)hh.hh>hh\n",
      "\n",
      "hh:hh!hh?hh;hh...hh…hh+hh/hh\\hh*hh}hh-.hh{hh]hh<hh|hh\n",
      "�hhhkhhmhh-hhkhhh.hh,hh)hh>hh\n",
      "\n",
      "hh:hh?hh!hh;hh...hh…hh+hh/hh\\hh*hh<hh}hh-.hh{hh]hh(hh\n",
      "�hhh-hh.hh,hh)hh>hhmmhh\n",
      "\n",
      "hh:hh?hh;hh!hh...hh/hh\\hh<hh}hh(hh]hh|hh{hh+hh*hh-.hhohh[hh\n",
      "�hhh-hh.hh,hh)hh>hh:hh\n",
      "\n",
      "hh :hh;hh!hh?hh...hh…hh..hh/hh\\hh<hh+hh}hh*hh(hh]hh-.hh{hh|hh\n",
      "�hhhkhhmhhkhhh-hhhhhh,hh.hh!hh>hh…hh?hh...hh\n",
      "\n",
      "hh:hhahhhbhhqhh/hhchh+hh\\hh*hhohhnhhfhh\n",
      "�hhhkhhmhh-hhkhhh.hh,hh)hh>hh\n",
      "\n",
      "hh:hh;hh?hh!hh...hh…hh+hh\n",
      "\n",
      "\n",
      "h:ahh/hh\\hh<hh*hh(hh]hh-.h\n",
      "�hhh-hhmmhh.hh/hh,hh)hh>hh\\h\\uh\\i\\hh\n",
      "\n",
      "h\\t\\h\n",
      ".h\\r\\h>\\h\n",
      "\n",
      "\n",
      "\\h<hh1hh2\\u\\u\n",
      "\n",
      "\\u/h\\nh\n",
      "\n",
      "\n",
      "\n",
      "<\n",
      "�hhh-hhmmhh.hh/hh,hh)hh>hh\\h\\uhh\n",
      "\n",
      "hhchhfhhqhhmhhghhbhh+hh]hh<hh*hh}hh?hh...hh(hh),hh\n",
      "�hhh-hh.hh,hh)hh>hhmmhh\n",
      "\n",
      "hh:hh?hh!hh;hh...hh\n",
      "\n",
      "\n",
      "h:ohh<hh\\hh}hh{hh]hh(hh)?hh|hh</hh {hh[h\n",
      "�hhhmhhmmhhhhhh-hhahhh,hh.hh/hh>hh...hh…hh?hh!hh\n",
      "\n",
      "hh:hh+hhbhhkhhohhnhh\\hhchhqhh]hhghh\n",
      "�hhhkhhmhh-hhkhhh,hh)hh.hh>hh\n",
      "\n",
      "hh:hh?hh!hh;hh...hh…hh+hh/hh\\hh*hh<hh}hh-.hh{hh]hh</hh\n",
      "�hhh-hh.hh,hh)hh>hhmmhh\n",
      "\n",
      "hh:hh?hh/hh...hh\\hh<hh}hh+hh*hh(hh]hh|hh{hhohh[hh][hh=hh-.hh\n",
      "�hhhmhhmmhhhhhh-hh,hh.hh/hh...hh…hh>hh?hh\n",
      "\n",
      "hh:hh!hh\\hh+hh*hhbhhohhahhhnhhchhkhhqhhphh\n",
      "�hhhmhhmmhhhhhh-hhhmhhahhhbhhnhhkhhhnhhkhhh\n",
      "hhfhh.hh,hh…hhhahh>hhohhaahh!hhghhphhqhhchhd\n",
      "�hhhmhhmmhhhhhh-hhhmhh,hhahhh.hh/hh>hh...hh…hhbhh?hh!hh\n",
      "hh:hh+hh\\hh*hhqhhkhhnhhchhdhhf\n",
      "�hhhhhhhmhhmmhhhmhh-hhkhh.hh,hh/hh…hhbhh>hh...hhahhhhahhnhhqhh\n",
      "hhaahhhnhhkhhhfhhphhdhhohhc\n",
      "�hhhhhhhmhhhmhhmmhh…hhhnhhkhhnhhkhhhhahhahhhbhhhihhhrhh-hhfhhhlhhqhhaahh hhhHhhohh.hhphhshhd\n",
      "�hhhkhhmhh-hh,hh)hh.hh?hh>hh!hh\n",
      "\n",
      "hh:hh;hh...hh…hh..hh\n",
      "\n",
      "\n",
      "h:ohh+hh/hh\\hh<hh*hh(hh]hh-.h\n",
      "�hhhkhhmhh-hhkhhh,hh)hh.hh?hh>hh!hh\n",
      "\n",
      "hh:hh;hh+hh\\hh}hh...hh-.hh\n",
      "\n",
      "\n",
      "h-ahh*hh<hh(hh]hh-,h\n",
      "�hhhkhhmhh-hhkhhh,hh)hh.hh>hh:hh?hh!hh\n",
      "\n",
      "hh…hh...hh..hhahhhbhh/hh+hhqhh\\hh*hhchh~hh(hh\n",
      "�hhhhhhhmhhmmhh-hhhmhh,hh.hh/hh...hh…hh>hhahhh?hh!hh\n",
      "\n",
      "hh:hh+hhbhhkhhohhnhhqhhphhchhfhh\n",
      "�hhhkhhmhhkhhh-hhhhhh,hh.hh!hh?hh>hh…hh...hh\n",
      "\n",
      "hh:hhahhhbhhqhh/hhchh+hh\\hh*hh<hhohhnhh\n",
      "�hhh-hh.hh:hh,hh)hh>hh\n",
      "\n",
      "hhmmhhmhhfhhbhh/hhghhchh+hhkhhdhhqhh\\hh]hh?hh...hh!hh<hh}hh\n",
      "�hhh-hh.hh,hh)hh>hh:hh\n",
      "\n",
      "hh :hh;hh?hh!hh...hh…hh/hh\\hh<hh}hh+hh*hh(hh]hh-.hh{hh|hh[hh\n",
      "�hhh-hhmmhh.hh/hh,hh)hh>hh:hh;h:i;h;i;i:h;ohh\n",
      "\n",
      "h:oh;s:h)oh:s:i,h;uh;/h:uh:mh;p>h:\n",
      "�hhhkhhmhh-hhkhhh,hh)hh.hh>hh\n",
      "\n",
      "hh:hh?hh!hh;hh...hh…hh+hh/hh\\hh*hh<hh}hh(hh]hh</hh|hh\n",
      "�hhhkhhmhh-hhkhhh,hh)hh.hh?hh>hh\n",
      "\n",
      "hh:hh!hh;hh...hh…hh+hh/hh\\hh*hh<hh}hh(hh]hh</hh|hh\n",
      "�hhhmhhmmhhhhhh-hhhmhh\n",
      "\n",
      "hh/hh>hh\\hhbhhchhnhh.hhfhh,hhahhh]hh...hh…hh?hh!hhohh+hh*hhqhh\n",
      "�hhh-hh.hh,hh)hh>hh:hh\n",
      "\n",
      "hh :hh;hh?hh!hh...hh…hh/hh\\hh<hh}hh+hh*hh(hh]hh-.hh{hh|hh[hh\n",
      "�hhhkhhmhh-hhkhhh,hh)hh.hh>hh\n",
      "\n",
      "hh:hh?hh!hh;hh...hh…hh+hh/hh\\hh*hh<hh}hh-.hh{hh]hh(hh\n",
      "�hhhkhhmhhkhhh-hhhhhh,hh.hh!hh>hh?hh…hh...hh\n",
      "\n",
      "hh:hhahhh hhhbhhqhh/hhchh+hh\\hh*hhohhnhh\n",
      "�hhhkhhmhh-hh,hh)hh.hh?hh>hh\n",
      "\n",
      "hh:hh;hh!hh...hh…hh+hh/hh\\hh*hh(hh]hh<hh}hh|hh{hhohh\n",
      "Number of sentences decoded: 34\n",
      "Output file saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Call the decode function\n",
    "decode(base_path=\"./\",\n",
    "       src_file=\"dataset/src_file.txt\",\n",
    "       best_model=\"checkpoint\",\n",
    "       output=\"../decoded.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
