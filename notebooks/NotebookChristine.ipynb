{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# neuroCraft Project\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Sentence Simplification using a Sentence-to-Sentence (seq2seq) Architecture: BERT-to-GPT2 \n",
    "***\n",
    "<br>\n",
    "- Sentence simplification using Hugging Face transformers.\n",
    "<br>\n",
    "<br>\n",
    "- Encoder-Decoder-Model with a combination of BertConfig and GPT2Config as encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import click\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from collections import Counter\n",
    "\n",
    "import tqdm\n",
    "import logging\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "from transformers import BertTokenizer, GPT2Tokenizer\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from transformers import EncoderDecoderModel, BertConfig, EncoderDecoderConfig, GPT2Tokenizer, BertModel, GPT2Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line reader\n",
    "def ReadInFile (filename):\n",
    "    '''\n",
    "    Utility function designed to read lines from a file and return them as a list of strings.\n",
    "\n",
    "    - with open(filename) as f:\n",
    "    Opens the file specified by the filename parameter using a with statement, which ensures that the file is properly closed after its suite finishes, even if an exception occurs.\n",
    "\n",
    "    - lines = f.readlines():\n",
    "    Reads all lines from the file f and stores them in the lines variable as a list of strings. Each line from the file becomes an element in the list.\n",
    "\n",
    "    - lines = [x.strip() for x in lines]:\n",
    "    Strips any leading or trailing whitespaces (like newline characters) from each line in the lines list using a list comprehension.\n",
    "    This step ensures that the lines are clean and don't contain any extra spaces.\n",
    "\n",
    "    - return lines:\n",
    "    Returns the list of cleaned lines from the file.\n",
    "    '''\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "        lines = [x.strip() for x in lines]\n",
    "    return lines\n",
    "\n",
    "\n",
    "# file opener\n",
    "def open_file(file_path, ref=False):\n",
    "    '''\n",
    "    Versatile file opener that can read text files and binary pickle files in Python.\n",
    "\n",
    "    - reading Text Files:\n",
    "    If the ref parameter is False or not specified, it assumes the file is a text file.\n",
    "    It opens the file specified by file_path in read mode ('r') using utf-8 encoding to handle Unicode characters.\n",
    "\n",
    "    - Loading Pickle Files:\n",
    "    If ref is set to True, it assumes the file is a binary pickle file (ref_data = pickle.load(open(file_path, 'rb'))) and directly returns the loaded data.\n",
    "\n",
    "    - Processing Text Files:\n",
    "    If the file is a text file, it reads lines from the file (sents = f.readlines()) and strips any leading or trailing whitespaces (including newline characters) from each line.\n",
    "    Each cleaned line is appended to the data list.\n",
    "\n",
    "    After processing all lines, it returns the list of cleaned lines (data).\n",
    "    '''\n",
    "    data = []\n",
    "    if ref:\n",
    "        ref_data = pickle.load(open(file_path, 'rb'))\n",
    "        return ref_data\n",
    "    else:\n",
    "        with open(file_path, 'r', encoding=\"utf8\") as f:\n",
    "            sents = f.readlines()\n",
    "            for s in sents:\n",
    "                data.append(s.strip())\n",
    "        return data\n",
    "\n",
    "# data loader\n",
    "def load_dataset(src_path, tgt_path=None, ref_path=None, ref=False):\n",
    "    '''\n",
    "    Function serves as a data loader and orchestrator for multiple files for NLP tasks.\n",
    "\n",
    "    - Loading Source Data:\n",
    "    Loads the source data from the file specified by src_path using the open_file function without any specific reference handling.\n",
    "\n",
    "    - Loading Target Data (Optional):\n",
    "    If a tgt_path is provided, it loads target data using the open_file function and assigns it to the tgt variable.\n",
    "    This can be used when dealing with tasks like translation and simplification (here) where there are source and target language sentences.\n",
    "\n",
    "    - Loading Reference Data (Optional):\n",
    "    If a ref_path is provided, it loads reference data using the open_file function, potentially handling it as a pickle file based on the ref parameter.\n",
    "    This ref parameter determines whether to load the reference data as a pickle file or text file.\n",
    "\n",
    "    - Returning Loaded Data:\n",
    "    returns three variables: src (source data), tgt (target data, if provided), and ref (reference data, if provided)\n",
    "    '''\n",
    "    src = open_file(src_path)\n",
    "    tgt = None\n",
    "    ref = None\n",
    "    if tgt_path is not None:\n",
    "        tgt = open_file(tgt_path)\n",
    "    if ref_path is not None:\n",
    "        ref = open_file(ref_path, ref)\n",
    "    return src, tgt, ref\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# CURRENT DATA\n",
    "\n",
    "\"\"\"\n",
    "1. Training:\n",
    "\n",
    "Wikilarg dataset comprising of parallel corpus of normal sentences and simple sentences is used to train the model.\n",
    "The original dataset consists of around 167k English sentence pairs from the Wikipedia articles.\n",
    "The dataset comprises of mapping of one-to-many, one-to-one and many-to-one sentence pairs.\n",
    "But the dataset was not suitable for the training without preprocessing.\n",
    "Upon tokenizing the sentences, sentences having token length of more than 80 were removed keeping the maximum token length of sentences to 80.\n",
    "The resulting training dataset became 138k from 167k.\n",
    "\n",
    "2. Validation:\n",
    "\n",
    "WikiSmall dataset ...\n",
    "\n",
    "3. Testing:\n",
    "\n",
    "For the evaluation and testing purpose, TurkCorpus is used.\n",
    "The dataset consists of 2k manually prepared sentence pairs with 8 reference sentences and 300 sentences for testing purpose which also has 8 reference sentences.\n",
    "\"\"\"\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep class for data handling for now\n",
    "\n",
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, src_path, tgt_path=None, ref_path=None, ref=False):\n",
    "        self.src = self.open_file(src_path)\n",
    "        self.tgt = None\n",
    "        self.ref = None\n",
    "        if tgt_path is not None:\n",
    "            self.tgt = self.open_file(tgt_path)\n",
    "        if ref_path is not None:\n",
    "            self.ref = self.open_file(ref_path, ref)\n",
    "\n",
    "        self.size = len(self.src)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.tgt is not None and self.ref is not None:\n",
    "            return self.src[index], self.tgt[index], self.ref[index]\n",
    "        elif self.tgt is not None:\n",
    "            return self.src[index], self.tgt[index], None\n",
    "        else:\n",
    "            return self.src[index], None, None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    @staticmethod\n",
    "    def open_file(file_path, ref=False):\n",
    "        data = []\n",
    "        if ref:\n",
    "            ref_data = pickle.load(open(file_path, 'rb'))\n",
    "            return ref_data\n",
    "\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding=\"utf8\") as f:\n",
    "                sents = f.readlines()\n",
    "                for s in sents:\n",
    "                    data.append(s.strip())\n",
    "            return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data (Wikilarg)\n",
      "(138413, 1)\n",
      "(138413, 1)\n",
      "Source: Quincy in 1767 was the `` north precinct '' of Braintree , Massachusetts .\n",
      "Target: He was born in Braintree , Massachusetts , in 1767 .\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "src_train_file = '../dataset/src_train.txt'\n",
    "tgt_train_file = '../dataset/tgt_train.txt'\n",
    "\n",
    "# original train data and target train data (no ref data)\n",
    "src_train, tgt_train, ref_train = load_dataset(src_train_file, tgt_train_file, ref=False)\n",
    "\n",
    "print('training data (Wikilarg)')\n",
    "print(pd.DataFrame(src_train).shape)\n",
    "print(pd.DataFrame(tgt_train).shape)\n",
    "\n",
    "sample_index = 1000\n",
    "src_sample = src_train[sample_index]\n",
    "tgt_sample = tgt_train[sample_index]\n",
    "\n",
    "print(\"Source:\", src_sample)\n",
    "print(\"Target:\", tgt_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "validation data (WikiSmall)\n",
      "(2000, 1)\n",
      "(2000, 1)\n",
      "Source: Since 1980 the senior pastor has been John Piper .\n",
      "Target: Since 1980 the main pastor has been John Piper .\n"
     ]
    }
   ],
   "source": [
    "# validation data\n",
    "src_valid_file = '../dataset/src_valid.txt'\n",
    "tgt_valid_file = '../dataset/tgt_valid.txt'\n",
    "\n",
    "# original train data and target train data (no ref data)\n",
    "src_valid, tgt_valid, ref_valid = load_dataset(src_valid_file, tgt_valid_file, ref=False)\n",
    "\n",
    "print(' ')\n",
    "print('validation data (WikiSmall)')\n",
    "print(pd.DataFrame(src_valid).shape)\n",
    "print(pd.DataFrame(tgt_valid).shape)\n",
    "\n",
    "sample_index = 99\n",
    "src_sample = src_valid[sample_index]\n",
    "tgt_sample = tgt_valid[sample_index]\n",
    "\n",
    "print(\"Source:\", src_sample)\n",
    "print(\"Target:\", tgt_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "test data (Amazon Mechanical Turk workers)\n",
      "(359, 1)\n",
      "(359, 1)\n",
      "Source: Alessandro ( \" Sandro \" ) Mazzola ( born 8 November 1942 ) is an Italian former football player .\n",
      "Target: Alessandro Mazzola is an Italian former football player.\n"
     ]
    }
   ],
   "source": [
    "# testing data\n",
    "src_test_file = '../dataset/src_test.txt'\n",
    "tgt_test_file = '../dataset/tgt_test.txt'\n",
    "\n",
    "# original train data and target train data (no ref data)\n",
    "src_test, tgt_test, ref_test = load_dataset(src_test_file, tgt_test_file, ref=False)\n",
    "\n",
    "print(' ')\n",
    "print('test data (Amazon Mechanical Turk workers)')\n",
    "print(pd.DataFrame(src_test).shape)\n",
    "print(pd.DataFrame(tgt_test).shape)\n",
    "\n",
    "sample_index = 10\n",
    "src_sample = src_test[sample_index]\n",
    "tgt_sample = tgt_test[sample_index]\n",
    "\n",
    "print(\"Source:\", src_sample)\n",
    "print(\"Target:\", tgt_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up tokenizers\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.unk_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for encoding the text\n",
    "def encode_batch(batch, max_len=80):\n",
    "    '''\n",
    "    Inputs\n",
    "    - batch:\n",
    "        This function expects a list containing two elements:\n",
    "        - The first element is a string representing the source text.\n",
    "        - The second element is a string representing the target text.\n",
    "\n",
    "    Tokenization\n",
    "    - src_tokens:\n",
    "    The source text (first element of batch) is tokenized using the BERT tokenizer (bert_tokenizer).\n",
    "    It is processed to generate tokens (input_ids) and an attention mask for the source text.\n",
    "    BERT tokenization includes adding special tokens, padding to a maximum length, and truncating if needed.\n",
    "\n",
    "    - tgt_tokens:\n",
    "    The target text (second element of batch) is tokenized using the GPT-2 tokenizer (gpt2_tokenizer).\n",
    "    Similar to the source, tokens (input_ids) and an attention mask for the target text are generated.\n",
    "\n",
    "    Creating Labels\n",
    "    - labels: This is created from the tgt_tokens.input_ids tensor.\n",
    "    It's used for calculating loss during training.\n",
    "    The tgt_tokens.attention_mask is used to identify where the padding is in the target tokens and sets those positions in the labels tensor to -100.\n",
    "\n",
    "    Output\n",
    "    The function returns five values:\n",
    "    - src_tokens.input_ids:\n",
    "    Tensor containing tokenized representation of the source text.\n",
    "    - src_tokens.attention_mask:\n",
    "    Tensor containing attention mask for the source text.\n",
    "    - tgt_tokens.input_ids:\n",
    "    Tensor containing tokenized representation of the target text.\n",
    "    - tgt_tokens.attention_mask:\n",
    "    Tensor containing attention mask for the target text.\n",
    "\n",
    "    Labels:\n",
    "    Tensor containing the labels for the target text, modified with -100 in places corresponding to padding.\n",
    "    '''\n",
    "    src_tokens = bert_tokenizer(batch[0], max_length=max_len, add_special_tokens=True,\n",
    "                                return_token_type_ids=False, padding=\"max_length\", truncation=True,\n",
    "                                return_attention_mask=True, return_tensors=\"pt\")\n",
    "\n",
    "    tgt_tokens = gpt2_tokenizer(batch[1], max_length=max_len, add_special_tokens=True,\n",
    "                                return_token_type_ids=False, padding=\"max_length\", truncation=True,\n",
    "                                return_attention_mask=True, return_tensors=\"pt\")\n",
    "\n",
    "    labels = tgt_tokens.input_ids.clone()\n",
    "    labels[tgt_tokens.attention_mask == 0] = -100\n",
    "\n",
    "    return src_tokens.input_ids, src_tokens.attention_mask, tgt_tokens.input_ids, tgt_tokens.attention_mask, labels\n",
    "\n",
    "# function for decoding the tokenized sentences to human readable text\n",
    "def decode_sent_tokens(data):\n",
    "    '''\n",
    "    Takes a list of tokenized sentences (data) and decodes them back into human-readable sentences.\n",
    "\n",
    "    Inputs:\n",
    "    - data:\n",
    "    A list containing tokenized sentences.\n",
    "\n",
    "    Functionality:\n",
    "    It iterates through each tokenized sentence in the data list.\n",
    "    For each tokenized sentence, it uses the GPT-2 tokenizer (gpt2_tokenizer) to decode the tokens into a human-readable sentence (s),\n",
    "    skipping special tokens and cleaning up tokenization spaces.\n",
    "    The decoded sentences (s) are added to a list (sents_list).\n",
    "\n",
    "    Output:\n",
    "    The function returns a list (sents_list) containing the decoded sentences\n",
    "    '''\n",
    "    sents_list = []\n",
    "    for sent in data:\n",
    "        s = gpt2_tokenizer.decode(sent, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        sents_list.append(s)\n",
    "\n",
    "    return sents_list\n",
    "\n",
    "# function to tokenize input sentences for downstream processing\n",
    "def get_sent_tokens(sents):\n",
    "    '''\n",
    "    Tokenizes input sentences and prepares them for downstream processing, e.g., for model input or further manipulation.\n",
    "\n",
    "    Inputs:\n",
    "    - sents:\n",
    "    A list or string containing sentences.\n",
    "\n",
    "    Functionality:\n",
    "    Tokenizes the input sentences using the GPT-2 tokenizer (gpt2_tokenizer) with specific settings:\n",
    "    - Adding special tokens.\n",
    "    - Not returning token type IDs.\n",
    "    - Truncating sequences if needed.\n",
    "    - Padding to the longest sequence in the batch.\n",
    "    - Not returning attention masks (only input tensors are returned).\n",
    "\n",
    "    Output:\n",
    "    returns a list (ref) containing lists of tokenized sentences, where each inner list represents the tokenized form of a sentence from the input.\n",
    "    '''\n",
    "    ref = []\n",
    "    tokens = gpt2_tokenizer(sents, add_special_tokens=True,\n",
    "                            return_token_type_ids=False, truncation=True, padding=\"longest\",\n",
    "                            return_attention_mask=False, return_tensors=\"pt\")\n",
    "\n",
    "    for tok in tokens.input_ids.tolist():\n",
    "        ref.append([tok])\n",
    "\n",
    "    return ref\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARI (System-level Automatic Readability Index) Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing function for calculating sari -gram F1 scores (SS task)\n",
    "\n",
    "def SARIngram(sgrams, cgrams, rgramslist, numref):\n",
    "    '''\n",
    "    Function calculating the SARI (System-level Automatic Readability Index) score for a given set of n-grams.\n",
    "    The SARI score assesses the quality of simplified sentences compared to both the original and reference sentences.\n",
    "\n",
    "    Counting n-grams:\n",
    "    The function takes as input three types of n-grams: sgrams (simplified sentence n-grams), cgrams (candidate n-grams), and rgramslist (a list of reference sentence n-grams).\n",
    "    It creates counters to count occurrences of each n-gram type in these inputs.\n",
    "\n",
    "    Handling Repetitions:\n",
    "    It adjusts the counts for sgrams and cgrams by multiplying them by numref, which represents the number of reference sentences for comparison.\n",
    "\n",
    "    Calculating Scores for Three Aspects (Keep, Deletion, Addition):\n",
    "    - Keep: Compares n-grams present in both the simplified and candidate sentences against those in the reference sentences.\n",
    "    - Deletion: Looks at n-grams in the simplified sentences that are absent in the candidate sentences and compares them against reference n-grams.\n",
    "    - Addition: Considers n-grams in the candidate sentences not present in the simplified sentences and compares them against reference n-grams.\n",
    "\n",
    "    Precision and Recall Calculations:\n",
    "    For each aspect (Keep, Deletion, Addition), it calculates precision and recall scores based on the counts of n-grams.\n",
    "\n",
    "    SARI Score Calculation:\n",
    "    It computes the final SARI score using the precision and recall scores for Keep, Deletion, and Addition aspects.\n",
    "\n",
    "    Return:\n",
    "    Function returns a tuple containing the SARI score, precision score for deletions, and the score for additions.\n",
    "    '''\n",
    "    rgramsall = [rgram for rgrams in rgramslist for rgram in rgrams]\n",
    "    rgramcounter = Counter(rgramsall)\n",
    "\n",
    "    sgramcounter = Counter(sgrams)\n",
    "    sgramcounter_rep = Counter()\n",
    "    for sgram, scount in sgramcounter.items():\n",
    "        sgramcounter_rep[sgram] = scount * numref\n",
    "\n",
    "    cgramcounter = Counter(cgrams)\n",
    "    cgramcounter_rep = Counter()\n",
    "    for cgram, ccount in cgramcounter.items():\n",
    "        cgramcounter_rep[cgram] = ccount * numref\n",
    "\n",
    "\n",
    "    # KEEP\n",
    "    keepgramcounter_rep = sgramcounter_rep & cgramcounter_rep\n",
    "    keepgramcountergood_rep = keepgramcounter_rep & rgramcounter\n",
    "    keepgramcounterall_rep = sgramcounter_rep & rgramcounter\n",
    "\n",
    "    keeptmpscore1 = 0\n",
    "    keeptmpscore2 = 0\n",
    "    for keepgram in keepgramcountergood_rep:\n",
    "        keeptmpscore1 += keepgramcountergood_rep[keepgram] / keepgramcounter_rep[keepgram]\n",
    "        keeptmpscore2 += keepgramcountergood_rep[keepgram] / keepgramcounterall_rep[keepgram]\n",
    "        #print \"KEEP\", keepgram, keepscore, cgramcounter[keepgram], sgramcounter[keepgram], rgramcounter[keepgram]\n",
    "    keepscore_precision = 0\n",
    "    if len(keepgramcounter_rep) > 0:\n",
    "    \tkeepscore_precision = keeptmpscore1 / len(keepgramcounter_rep)\n",
    "    keepscore_recall = 0\n",
    "    if len(keepgramcounterall_rep) > 0:\n",
    "    \tkeepscore_recall = keeptmpscore2 / len(keepgramcounterall_rep)\n",
    "    keepscore = 0\n",
    "    if keepscore_precision > 0 or keepscore_recall > 0:\n",
    "        keepscore = 2 * keepscore_precision * keepscore_recall / (keepscore_precision + keepscore_recall)\n",
    "\n",
    "\n",
    "    # DELETION\n",
    "    delgramcounter_rep = sgramcounter_rep - cgramcounter_rep\n",
    "    delgramcountergood_rep = delgramcounter_rep - rgramcounter\n",
    "    delgramcounterall_rep = sgramcounter_rep - rgramcounter\n",
    "    deltmpscore1 = 0\n",
    "    deltmpscore2 = 0\n",
    "    for delgram in delgramcountergood_rep:\n",
    "        deltmpscore1 += delgramcountergood_rep[delgram] / delgramcounter_rep[delgram]\n",
    "        deltmpscore2 += delgramcountergood_rep[delgram] / delgramcounterall_rep[delgram]\n",
    "    delscore_precision = 0\n",
    "    if len(delgramcounter_rep) > 0:\n",
    "    \tdelscore_precision = deltmpscore1 / len(delgramcounter_rep)\n",
    "    delscore_recall = 0\n",
    "    if len(delgramcounterall_rep) > 0:\n",
    "    \tdelscore_recall = deltmpscore1 / len(delgramcounterall_rep)\n",
    "    delscore = 0\n",
    "    if delscore_precision > 0 or delscore_recall > 0:\n",
    "        delscore = 2 * delscore_precision * delscore_recall / (delscore_precision + delscore_recall)\n",
    "\n",
    "\n",
    "    # ADDITION\n",
    "    addgramcounter = set(cgramcounter) - set(sgramcounter)\n",
    "    addgramcountergood = set(addgramcounter) & set(rgramcounter)\n",
    "    addgramcounterall = set(rgramcounter) - set(sgramcounter)\n",
    "\n",
    "    addtmpscore = 0\n",
    "    for addgram in addgramcountergood:\n",
    "        addtmpscore += 1\n",
    "\n",
    "    addscore_precision = 0\n",
    "    addscore_recall = 0\n",
    "    if len(addgramcounter) > 0:\n",
    "    \taddscore_precision = addtmpscore / len(addgramcounter)\n",
    "    if len(addgramcounterall) > 0:\n",
    "    \taddscore_recall = addtmpscore / len(addgramcounterall)\n",
    "    addscore = 0\n",
    "    if addscore_precision > 0 or addscore_recall > 0:\n",
    "        addscore = 2 * addscore_precision * addscore_recall / (addscore_precision + addscore_recall)\n",
    "\n",
    "    return (keepscore, delscore_precision, addscore)\n",
    "\n",
    "\n",
    "\n",
    "def SARIsent(ssent, csent, rsents):\n",
    "    '''\n",
    "    This function computes the System-level Automatic Readability Index (SARI) score for a set of sentences.\n",
    "\n",
    "    Tokenizing Sentences:\n",
    "\n",
    "    Takes in three inputs\n",
    "    - ssent (simplified sentence)\n",
    "    - csent (candidate sentence)\n",
    "    - and rsents (a list of reference sentences)\n",
    "    The function begins by splitting these sentences into unigrams, bigrams, trigrams, and quadgrams\n",
    "    (s1grams, s2grams, s3grams, s4grams, c1grams, c2grams, c3grams, c4grams, r1gramslist, r2gramslist, r3gramslist, r4gramslist)\n",
    "    using spaces as delimiters.\n",
    "\n",
    "    Creating n-grams from Reference Sentences:\n",
    "\n",
    "    Creates n-grams\n",
    "    (unigrams, bigrams, trigrams, and quadgrams)\n",
    "    from the reference sentences (rsents) and stores them in separate lists.\n",
    "\n",
    "    Calculating Scores for Different n-gram Levels:\n",
    "\n",
    "    For each level of n-grams (from unigrams to quadgrams) in both the simplified and candidate sentences,\n",
    "    it calls the SARIngram function to compute scores for keep, delete, and add operations for each level of n-grams.\n",
    "\n",
    "    Aggregating Scores:\n",
    "    It averages the scores for keep, delete, and add operations across different n-gram levels to get average keep, delete, and add scores.\n",
    "\n",
    "    Computing the Final SARI Score:\n",
    "    Computes the final SARI score by averaging the average keep, delete, and add scores, and then returns the final score.\n",
    "    '''\n",
    "    numref = len(rsents)\n",
    "\n",
    "    s1grams = ssent.lower().split(\" \")\n",
    "    c1grams = csent.lower().split(\" \")\n",
    "    s2grams = []\n",
    "    c2grams = []\n",
    "    s3grams = []\n",
    "    c3grams = []\n",
    "    s4grams = []\n",
    "    c4grams = []\n",
    "\n",
    "    r1gramslist = []\n",
    "    r2gramslist = []\n",
    "    r3gramslist = []\n",
    "    r4gramslist = []\n",
    "    for rsent in rsents:\n",
    "        r1grams = rsent.lower().split(\" \")\n",
    "        r2grams = []\n",
    "        r3grams = []\n",
    "        r4grams = []\n",
    "        r1gramslist.append(r1grams)\n",
    "        for i in range(0, len(r1grams)-1) :\n",
    "            if i < len(r1grams) - 1:\n",
    "                r2gram = r1grams[i] + \" \" + r1grams[i+1]\n",
    "                r2grams.append(r2gram)\n",
    "            if i < len(r1grams)-2:\n",
    "                r3gram = r1grams[i] + \" \" + r1grams[i+1] + \" \" + r1grams[i+2]\n",
    "                r3grams.append(r3gram)\n",
    "            if i < len(r1grams)-3:\n",
    "                r4gram = r1grams[i] + \" \" + r1grams[i+1] + \" \" + r1grams[i+2] + \" \" + r1grams[i+3]\n",
    "                r4grams.append(r4gram)\n",
    "        r2gramslist.append(r2grams)\n",
    "        r3gramslist.append(r3grams)\n",
    "        r4gramslist.append(r4grams)\n",
    "\n",
    "    for i in range(0, len(s1grams)-1) :\n",
    "        if i < len(s1grams) - 1:\n",
    "            s2gram = s1grams[i] + \" \" + s1grams[i+1]\n",
    "            s2grams.append(s2gram)\n",
    "        if i < len(s1grams)-2:\n",
    "            s3gram = s1grams[i] + \" \" + s1grams[i+1] + \" \" + s1grams[i+2]\n",
    "            s3grams.append(s3gram)\n",
    "        if i < len(s1grams)-3:\n",
    "            s4gram = s1grams[i] + \" \" + s1grams[i+1] + \" \" + s1grams[i+2] + \" \" + s1grams[i+3]\n",
    "            s4grams.append(s4gram)\n",
    "\n",
    "    for i in range(0, len(c1grams)-1) :\n",
    "        if i < len(c1grams) - 1:\n",
    "            c2gram = c1grams[i] + \" \" + c1grams[i+1]\n",
    "            c2grams.append(c2gram)\n",
    "        if i < len(c1grams)-2:\n",
    "            c3gram = c1grams[i] + \" \" + c1grams[i+1] + \" \" + c1grams[i+2]\n",
    "            c3grams.append(c3gram)\n",
    "        if i < len(c1grams)-3:\n",
    "            c4gram = c1grams[i] + \" \" + c1grams[i+1] + \" \" + c1grams[i+2] + \" \" + c1grams[i+3]\n",
    "            c4grams.append(c4gram)\n",
    "\n",
    "    (keep1score, del1score, add1score) = SARIngram(s1grams, c1grams, r1gramslist, numref)\n",
    "    (keep2score, del2score, add2score) = SARIngram(s2grams, c2grams, r2gramslist, numref)\n",
    "    (keep3score, del3score, add3score) = SARIngram(s3grams, c3grams, r3gramslist, numref)\n",
    "    (keep4score, del4score, add4score) = SARIngram(s4grams, c4grams, r4gramslist, numref)\n",
    "    avgkeepscore = sum([keep1score,keep2score,keep3score,keep4score])/4\n",
    "    avgdelscore = sum([del1score,del2score,del3score,del4score])/4\n",
    "    avgaddscore = sum([add1score,add2score,add3score,add4score])/4\n",
    "    finalscore = (avgkeepscore + avgdelscore + avgaddscore ) / 3\n",
    "\n",
    "    return finalscore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final computation function for sentence level SARI\n",
    "def compute_sari(norm, pred_tensor, ref):\n",
    "    '''\n",
    "    computes the Sentence-level SARI score for a set of predicted sentences compared to their corresponding reference sentences.\n",
    "\n",
    "    Decoding Predicted Sentences:\n",
    "    decodes the predicted sentences from the pred_tensor using the tokenizer.\n",
    "    This step converts tokenized or numerical representations back into human-readable sentences.\n",
    "\n",
    "    SARI Score Calculation:\n",
    "    The function initializes a score variable to accumulate the SARI scores for individual sentences.\n",
    "    It iterates over each step (index) and corresponding items in the ref list.\n",
    "\n",
    "    For each step:\n",
    "    Computes the SARI score using sari.SARIsent(norm[step], pred[step], item).\n",
    "    This compares the normalized (norm[step]) predicted sentence with the reference sentence (item) and computes the SARI score.\n",
    "    Adds up the individual SARI scores to the score variable.\n",
    "\n",
    "    Normalization and Return:\n",
    "    The final computed score is divided by the TRAIN_BATCH_SIZE to normalize the score.\n",
    "\n",
    "    Returns the normalized SARI score, which is the cumulative SARI score of the predicted sentences against their reference sentences, divided by the batch size.\n",
    "    '''\n",
    "    pred = decode_sent_tokens(pred_tensor)\n",
    "    score = 0\n",
    "    for step, item in enumerate(ref):\n",
    "        score += SARIsent(norm[step], pred[step], item)\n",
    "    return score/TRAIN_BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computation function\n",
    "def compute_bleu_score(logits, labels):\n",
    "    '''\n",
    "    This function takes predicted sequences (logits) and their corresponding reference sequences (labels),\n",
    "    tokenizes the reference sentences, and computes a weighted BLEU score using NLTK's corpus_bleu function.\n",
    "    The BLEU score is a metric commonly used to evaluate the quality of machine-translated text against reference translations,\n",
    "    considering n-gram precision with specified weights.\n",
    "\n",
    "    Tokenization of Reference Sentences:\n",
    "    It retrieves the reference sentences by tokenizing the labels input.\n",
    "    This step involves converting human-readable sentences into tokens suitable for BLEU score computation.\n",
    "\n",
    "    Weighted BLEU Score Calculation:\n",
    "    weights are assigned to n-gram precision values (e.g., 1-gram precision and 2-gram precision).\n",
    "\n",
    "    BLEU Score Calculation:\n",
    "    computes the BLEU score using the corpus_bleu function from NLTK's nltk.translate.bleu_score module.\n",
    "    The refs variable contains the reference sentences (tokenized), and logits.tolist() contain the predicted sequences, converted to a list format.\n",
    "    The smoothing_function parameter applies a smoothing method to handle cases where certain n-grams are missing in the predicted sequences.\n",
    "    The computed BLEU score based on these inputs is stored in the score variable.\n",
    "\n",
    "    Returns the computed BLEU score as the output of the function.\n",
    "    '''\n",
    "    refs = get_sent_tokens(labels)\n",
    "    weights = (1.0/2.0, 1.0/2.0, )\n",
    "    score = corpus_bleu(refs, logits.tolist(), smoothing_function=SmoothingFunction(epsilon=1e-10).method1, weights=weights)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation after training (unseen test data)\n",
    "def evaluate(data_loader, e_loss):\n",
    "    '''\n",
    "    Performs evaluation on the trained model using the provided data loader (data_loader).\n",
    "    Assesses the model's performance in terms of loss, BLEU score, and SARI score on the test dataset.\n",
    "\n",
    "    Setting Model Evaluation Mode\n",
    "    - was_training = model.training:\n",
    "    Stores the model's current training state.\n",
    "\n",
    "    - model.eval():\n",
    "    Puts the model in evaluation mode, which disables dropout and batch normalization layers as they behave differently during training and evaluation.\n",
    "\n",
    "    - initialization:\n",
    "    Initializes variables for evaluation metrics (eval_loss, bleu_score, sari_score) to keep track of loss, BLEU score, and SARI score.\n",
    "\n",
    "    LogSoftmax and No Gradient Calculation\n",
    "    - softmax = nn.LogSoftmax(dim=-1): Initializes a LogSoftmax layer for computing probabilities along the last dimension (-1) of the tensor.\n",
    "    - with torch.no_grad():: Temporarily disables gradient calculation for efficiency during evaluation.\n",
    "\n",
    "    Iterating Over Data Loader Batches:\n",
    "    Loops over batches in the data_loader.\n",
    "    Encodes the batch data using the tokenizer.\n",
    "    Passes the encoded tensors to the model to get predictions (logits) and calculate the loss.\n",
    "    Computes the BLEU score and SARI score using the compute_bleu_score and compute_sari functions,\n",
    "    comparing the model's predicted outputs with the ground truth labels.\n",
    "    Updates the evaluation metrics (eval_loss, bleu_score, sari_score) by averaging across batches.\n",
    "\n",
    "    Restoring Model State and Return:\n",
    "    Restores the model to its original training state (model.train()) if it was in training mode before evaluation.\n",
    "\n",
    "    Returns the evaluated loss (eval_loss), BLEU score (bleu_score), and SARI score (sari_score) as a tuple.\n",
    "    '''\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    eval_loss = e_loss\n",
    "    bleu_score = 0\n",
    "    sari_score = 0\n",
    "    softmax = nn.LogSoftmax(dim = -1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(data_loader):\n",
    "            src_tensors, src_attn_tensors, tgt_tensors, tgt_attn_tensors, labels = encode_batch(batch)\n",
    "            loss, logits = model(input_ids = src_tensors.to(device),\n",
    "                            decoder_input_ids = tgt_tensors.to(device),\n",
    "                            attention_mask = src_attn_tensors.to(device),\n",
    "                            decoder_attention_mask = tgt_attn_tensors.to(device),\n",
    "                            labels = labels.to(device))[:2]\n",
    "            outputs = softmax(logits)\n",
    "            score = compute_bleu_score(torch.argmax(outputs, dim=-1), batch[1])\n",
    "            s_score = compute_sari(batch[0], torch.argmax(outputs, dim=-1), batch[2])\n",
    "            if step == 0:\n",
    "                eval_loss = loss.item()\n",
    "                bleu_score = score\n",
    "                sari_score = s_score\n",
    "            else:\n",
    "                eval_loss = (1/2.0)*(eval_loss + loss.item())\n",
    "                bleu_score = (1/2.0)* (bleu_score+score)\n",
    "                sari_score = (1/2.0)* (sari_score+s_score)\n",
    "\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return eval_loss, bleu_score, sari_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing Checkpoints\n",
    "def load_checkpt(checkpt_path, optimizer=None):\n",
    "    '''\n",
    "    Loading a checkpoint file containing model weights, optimizer state, evaluation loss, and epoch information. Let's break down its functionality:\n",
    "\n",
    "    Loading Checkpoint\n",
    "    - checkpoint = torch.load(checkpt_path):\n",
    "    Loads the checkpoint file from the specified checkpt_path using PyTorch's torch.load() function.\n",
    "\n",
    "    Loading Model and Optimizer States\n",
    "    Depending on the device being used (\"cpu\" or GPU), it loads the model's state and optimizer's state from the checkpoint file.\n",
    "    If the device is \"cpu,\" it maps both the model and optimizer states to the CPU using map_location=torch.device(\"cpu\").\n",
    "    If the device is a GPU, it loads the model and optimizer states directly without any mapping.\n",
    "\n",
    "    Retrieving Evaluation Loss and Epoch:\n",
    "    Extracts the evaluation loss (eval_loss) and epoch information (epoch) from the loaded checkpoint.\n",
    "\n",
    "    Returns the loaded optimizer (if provided and updated), evaluation loss, and epoch.\n",
    "    '''\n",
    "    checkpoint = torch.load(checkpt_path)\n",
    "    if device == \"cpu\":\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"], map_location=torch.device(\"cpu\"))\n",
    "        if optimizer is not None:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"], map_location=torch.device(\"cpu\"))\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        if optimizer is not None:\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    eval_loss = checkpoint[\"eval_loss\"]\n",
    "    epoch = checkpoint[\"epoch\"]\n",
    "\n",
    "    return optimizer, eval_loss, epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Training Checkpoint\n",
    "def save_model_checkpt(state, is_best, check_pt_path, best_model_path):\n",
    "    '''\n",
    "    Saves model checkpoints at regular intervals during training and, if specified,\n",
    "    also keeps a copy of the best-performing model in a separate file.\n",
    "    This practice allows for the restoration of model states,\n",
    "    retraining from specific points,\n",
    "    or retrieving the best model for deployment or further evaluation.\n",
    "    '''\n",
    "    f_path = check_pt_path\n",
    "    torch.save(state, f_path)\n",
    "\n",
    "    if is_best:\n",
    "        best_fpath = best_model_path\n",
    "        shutil.copyfile(f_path, best_fpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, target_tokenizer, ref_sentences):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch_source = batch[0]\n",
    "            batch_target = batch[1]\n",
    "\n",
    "            outputs = model(input_ids=batch_source, decoder_input_ids=batch_target)\n",
    "\n",
    "            logits_flat = outputs.logits.view(-1, outputs.logits.size(-1))\n",
    "            target_flat = batch_target.view(-1)\n",
    "\n",
    "            loss = criterion(logits_flat, target_flat)\n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "\n",
    "            predicted_ids = outputs.logits.argmax(-1)\n",
    "            predicted_sentences = [target_tokenizer.decode(ids, skip_special_tokens=True) for ids in predicted_ids]\n",
    "            target_sentences = [target_tokenizer.decode(ids, skip_special_tokens=True) for ids in batch_target]\n",
    "\n",
    "            references.extend([sent.split() for sent in target_sentences])\n",
    "            hypotheses.extend([sent.split() for sent in predicted_sentences])\n",
    "\n",
    "    avg_loss = total_loss / total_batches\n",
    "\n",
    "    # Calculate BLEU score using your function\n",
    "    #bleu_score = compute_bleu_score(torch.argmax(outputs, dim=-1), batch[1])\n",
    "    logits = outputs.logits  # Extract the logits from Seq2SeqLMOutput\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)  # Get the predicted IDs\n",
    "    bleu_score = compute_bleu_score(predicted_ids, batch[1])  # Compute BLEU score\n",
    "\n",
    "    # Calculate SARI score\n",
    "    sari_scores = []\n",
    "    for idx, gen_sent in enumerate(hypotheses):\n",
    "        sari_score = compute_sari(ref_sentences[idx], gen_sent)\n",
    "        sari_scores.append(sari_score)\n",
    "\n",
    "    avg_sari_score = sum(sari_scores) / len(sari_scores)\n",
    "\n",
    "    return avg_loss, bleu_score, avg_sari_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare validation dataset\n",
    "\n",
    "# function to load evaluation data\n",
    "def load_eval_data(src_path, tgt_path):\n",
    "    src_data = open_file(src_path)\n",
    "    tgt_data = open_file(tgt_path)\n",
    "    return src_data, tgt_data\n",
    "\n",
    "# evaluation data paths\n",
    "src_eval_file = '../dataset/src_valid.txt'\n",
    "tgt_eval_file = '../dataset/tgt_valid.txt'\n",
    "\n",
    "# load evaluation data\n",
    "src_eval, tgt_eval = load_eval_data(src_eval_file, tgt_eval_file)\n",
    "\n",
    "# prepare evaluation data tensors using encode_batch function\n",
    "source_eval_data_encoded = encode_batch((src_eval))\n",
    "target_eval_data_encoded = encode_batch((tgt_eval))\n",
    "\n",
    "# create evaluation data loader\n",
    "eval_dataset = TensorDataset(*source_eval_data_encoded, *target_eval_data_encoded)\n",
    "eval_data_loader = DataLoader(eval_dataset, batch_size=3, shuffle=False)\n",
    "\n",
    "# ref sentences\n",
    "# load data from the pickle file\n",
    "with open('../dataset/ref_valid.pkl', 'rb') as file:\n",
    "    ref_sentences = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# WORK IN PROGRESS\n",
    "\n",
    "'''\n",
    "To be implemented:\n",
    "- Logging\n",
    "- Save trained Model\n",
    "- Evaluation Metric (SARI)\n",
    "- Prediction\n",
    "'''\n",
    "\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    " # function to invoke the training\n",
    "def train_model(start_epoch, eval_loss, loaders, optimizer, check_pt_path, best_model_path):\n",
    "    best_eval_loss = eval_loss\n",
    "    print(\"Model training started...\")\n",
    "    for epoch in range(start_epoch, N_EPOCHS):\n",
    "        print(f\"Epoch {epoch} running...\")\n",
    "        epoch_start_time = time.time()\n",
    "        epoch_train_loss = 0\n",
    "        epoch_eval_loss = 0\n",
    "        model.train()\n",
    "        for step, batch in enumerate(loaders[0]):\n",
    "            src_tensors, src_attn_tensors, tgt_tensors, tgt_attn_tensors, labels = encode_batch(batch)\n",
    "            optimizer.zero_grad()\n",
    "            model.zero_grad()\n",
    "            loss = model(input_ids = src_tensors.to(device),\n",
    "                            decoder_input_ids = tgt_tensors.to(device),\n",
    "                            attention_mask = src_attn_tensors.to(device),\n",
    "                            decoder_attention_mask = tgt_attn_tensors.to(device),\n",
    "                            labels = labels.to(device))[0]\n",
    "            if step == 0:\n",
    "                epoch_train_loss = loss.item()\n",
    "            else:\n",
    "                epoch_train_loss = (1/2.0)*(epoch_train_loss + loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (step+1) % LOG_EVERY == 0:\n",
    "                print(f'Epoch: {epoch} | iter: {step+1} | avg. train loss: {epoch_train_loss} | time elapsed: {time.time() - epoch_start_time}')\n",
    "                logging.info(f'Epoch: {epoch} | iter: {step+1} | avg. train loss: {epoch_train_loss} | time elapsed: {time.time() - epoch_start_time}')\n",
    "\n",
    "        eval_start_time = time.time()\n",
    "        epoch_eval_loss, bleu_score, sari_score = evaluate(loaders[1], epoch_eval_loss)\n",
    "        epoch_eval_loss = epoch_eval_loss/TRAIN_BATCH_SIZE\n",
    "        print(f'Completed Epoch: {epoch} | avg. eval loss: {epoch_eval_loss:.5f} | blue score: {bleu_score} | Sari score: {sari_score} | time elapsed: {time.time() - eval_start_time}')\n",
    "        logging.info(f'Completed Epoch: {epoch} | avg. eval loss: {epoch_eval_loss:.5f} | blue score: {bleu_score}| Sari score: {sari_score} | time elapsed: {time.time() - eval_start_time}')\n",
    "\n",
    "        check_pt = {\n",
    "            'epoch': epoch+1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'eval_loss': epoch_eval_loss,\n",
    "            'sari_score': sari_score,\n",
    "            'bleu_score': bleu_score\n",
    "        }\n",
    "        check_pt_time = time.time()\n",
    "        print(\"Saving Checkpoint.......\")\n",
    "        if epoch_eval_loss < best_eval_loss:\n",
    "            print(\"New best model found\")\n",
    "            logging.info(f\"New best model found\")\n",
    "            best_eval_loss = epoch_eval_loss\n",
    "            save_model_checkpt(check_pt, True, check_pt_path, best_model_path)\n",
    "        else:\n",
    "            save_model_checkpt(check_pt, False, check_pt_path, best_model_path)\n",
    "        print(f\"Checkpoint saved successfully with time: {time.time() - check_pt_time}\")\n",
    "        logging.info(f\"Checkpoint saved successfully with time: {time.time() - check_pt_time}\")\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train(base_path=\"../\",\n",
    "          src_train=\"dataset/src_train.txt\",\n",
    "          tgt_train=\"dataset/tgt_train.txt\",\n",
    "          src_valid=\"dataset/src_valid.txt\",\n",
    "          tgt_valid=\"dataset/tgt_valid.txt\",\n",
    "          ref_valid=\"dataset/ref_valid.pkl\",\n",
    "          best_model=\"best_model/model.pt\",\n",
    "          checkpoint_path=\"checkpoint/model_ckpt.pt\", seed=123):\n",
    "\n",
    "    print(\"Loading datasets...\")\n",
    "    train_dataset = WikiDataset(base_path + src_train, base_path + tgt_train)\n",
    "    valid_dataset = WikiDataset(base_path + src_valid, base_path + tgt_valid, base_path + ref_valid, ref=True)\n",
    "    print(\"Dataset loaded successfully\")\n",
    "\n",
    "    train_dl = DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "    valid_dl = DataLoader(valid_dataset, batch_size=TRAIN_BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n",
    "    if os.path.exists(base_path + checkpoint_path):\n",
    "        optimizer, eval_loss, start_epoch = load_checkpt(base_path + checkpoint_path, optimizer)\n",
    "        print(f\"Loading model from checkpoint with start epoch: {start_epoch} and loss: {eval_loss}\")\n",
    "        logging.info(f\"Model loaded from saved checkpoint with start epoch: {start_epoch} and loss: {eval_loss}\")\n",
    "\n",
    "    return \"set-up done\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.7.crossattention.q_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.weight', 'h.11.ln_cross_attn.bias', 'h.4.crossattention.c_attn.bias', 'h.3.ln_cross_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.9.crossattention.c_attn.weight', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.q_attn.weight', 'h.0.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.8.ln_cross_attn.weight', 'h.0.crossattention.q_attn.bias', 'h.9.ln_cross_attn.weight', 'h.1.crossattention.q_attn.bias', 'h.4.ln_cross_attn.bias', 'h.8.crossattention.c_attn.bias', 'h.9.ln_cross_attn.bias', 'h.0.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.11.crossattention.q_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.5.ln_cross_attn.weight', 'h.3.ln_cross_attn.weight', 'h.6.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.11.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.11.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.weight', 'h.4.crossattention.q_attn.weight', 'h.7.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.c_proj.bias', 'h.8.ln_cross_attn.bias', 'h.9.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.11.crossattention.c_attn.bias', 'h.8.crossattention.c_proj.bias', 'h.7.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.4.ln_cross_attn.weight', 'h.2.crossattention.q_attn.bias', 'h.0.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.8.crossattention.c_proj.weight', 'h.1.ln_cross_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.9.crossattention.q_attn.bias', 'h.1.crossattention.c_proj.bias', 'h.11.crossattention.q_attn.bias', 'h.2.crossattention.c_attn.bias', 'h.1.crossattention.c_proj.weight', 'h.5.crossattention.c_attn.bias', 'h.2.ln_cross_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.4.crossattention.q_attn.bias', 'h.10.ln_cross_attn.weight', 'h.10.ln_cross_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.1.ln_cross_attn.weight', 'h.0.ln_cross_attn.bias', 'h.6.crossattention.c_proj.weight', 'h.11.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.10.crossattention.q_attn.weight', 'h.0.crossattention.q_attn.weight', 'h.8.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.2.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.5.crossattention.c_proj.weight', 'h.10.crossattention.c_proj.weight', 'h.6.ln_cross_attn.weight', 'h.2.crossattention.c_proj.weight', 'h.11.ln_cross_attn.weight', 'h.5.crossattention.q_attn.bias', 'h.4.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.9.crossattention.c_attn.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu as device\n",
      "Loading datasets...\n",
      "Dataset loaded successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'set-up done'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################### constants\n",
    "TRAIN_BATCH_SIZE = 3 # kept as in the source paper\n",
    "N_EPOCHS = 20 # they trained with 20 epochs\n",
    "max_token_len = 80 # can be configured as needed (Classification data?)\n",
    "LOG_EVERY = 10000\n",
    "\n",
    "################### encode source and target data using encode_batch function\n",
    "source_train_data_encoded = encode_batch((src_train))\n",
    "target_train_data_encoded = encode_batch((tgt_train))\n",
    "\n",
    "################### logging during training\n",
    "'''\n",
    "Configures logging settings using the logging.basicConfig method,\n",
    "to write log messages to a file named \"log_file.log\" with the INFO level.\n",
    "format: timestamp, log level, message\n",
    "'''\n",
    "logging.basicConfig(filename=\"log_file.log\", level=logging.INFO,\n",
    "                format=\"%(asctime)s:%(levelname)s: %(message)s\")\n",
    "CONTEXT_SETTINGS = dict(help_option_names = ['-h', '--help'])\n",
    "\n",
    "################### collate data batches\n",
    "def collate_fn(batch):\n",
    "    '''\n",
    "    custom collation function used in PyTorch for data preprocessing in DataLoader\n",
    "    It defines how individual samples within a batch are combined and transformed\n",
    "    before being fed into the model during training and evaluation.\n",
    "    '''\n",
    "    data_list, label_list, ref_list = [], [], []\n",
    "    for _data, _label, _ref in batch:\n",
    "        data_list.append(_data)\n",
    "        label_list.append(_label)\n",
    "        ref_list.append(_ref)\n",
    "    return data_list, label_list, ref_list\n",
    "\n",
    "################### initialize models\n",
    "'''\n",
    "Initializes the model as an Encoder-Decoder architecture using the EncoderDecoderModel from Hugging Face's transformers library.\n",
    "Model combines an encoder (BERT in this case) and a decoder (GPT-2 in this case) for sequence-to-sequence task.\n",
    "'''\n",
    "bert_model = BertModel.from_pretrained('bert-base-cased')\n",
    "gpt2_model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "################### configuration\n",
    "max_token_len = 80\n",
    "start_token_id = bert_tokenizer.cls_token_id\n",
    "end_token_id = gpt2_tokenizer.eos_token_id\n",
    "\n",
    "################### initialize the Encoder-Decoder model with cross-attention enabled\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')\n",
    "\n",
    "################### update configuration for the model\n",
    "'''\n",
    "Updates the configuration for the encoder-decoder model:\n",
    "- Configure the start and end token IDs for the encoder and decoder\n",
    "- Set the maximum length for tokenized sequences\n",
    "- Enable cross-attention in the model (model.config.add_cross_attention = True),\n",
    "  allowing the decoder to attend to different parts of the encoded input sequence during decoding.\n",
    "'''\n",
    "model.config.decoder_start_token_id = start_token_id\n",
    "model.config.eos_token_id = end_token_id\n",
    "model.config.max_length = max_token_len\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.add_cross_attention = True\n",
    "\n",
    "#################### set device\n",
    "'''\n",
    "Checks for GPU availability,\n",
    "selects the appropriate device (\"cuda\" or \"cpu\"),\n",
    "prints the chosen device,\n",
    "and then moves the model to that device for computation.\n",
    "TAllows the code to leverage GPU acceleration if a compatible GPU is available,\n",
    "enhancing computational performance for deep learning tasks.\n",
    "'''\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} as device\")\n",
    "model.to(device)\n",
    "\n",
    "################## loss function and optimizer\n",
    "'''\n",
    "Loss Function\n",
    "- nn.CrossEntropyLoss():\n",
    "Initializes the cross-entropy loss function from the torch.nn module.\n",
    "Cross-entropy loss is commonly used for multi-class classification tasks, where the model's output represents class probabilities.\n",
    "\n",
    "Optimizer:\n",
    "- torch.optim.Adam(model.parameters(), lr=0.001):\n",
    "Initializes the Adam optimizer from the torch.optim module.\n",
    "It optimizes the model's parameters (model.parameters()) using the Adam optimization algorithm, a popular variant of stochastic gradient descent (SGD).\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "train(base_path=\"../\",\n",
    "      src_train=\"dataset/src_train.txt\",\n",
    "      tgt_train=\"dataset/tgt_train.txt\",\n",
    "      src_valid=\"dataset/src_valid.txt\",\n",
    "      tgt_valid=\"dataset/tgt_valid.txt\",\n",
    "      ref_valid=\"dataset/ref_valid.pkl\",\n",
    "      best_model=\"best_model/model.pt\",\n",
    "      checkpoint_path=\"checkpoint/model_ckpt.pt\",\n",
    "      seed=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training started...\n",
      "Epoch 0 running...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christine/.pyenv/versions/3.10.6/envs/neuroCraft/lib/python3.10/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:639: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_model(start_epoch=0,\n",
    "            val_loss=0,\n",
    "            loaders=load_dataset(src_path=\"../dataset/src_train.txt\",\n",
    "                                 gt_path=\"../dataset/tgt_train.txt\",\n",
    "                                 ref=False\n",
    "                                 ),\n",
    "            optimizer=optimizer,\n",
    "            check_pt_path=\"checkpoint\",\n",
    "            best_model_path=\"best_model\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function for testing the model\n",
    "def test(base_path=\"../\",\n",
    "         src_test=\"dataset/src_test.txt\",\n",
    "         tgt_test=\"dataset/tgt_test.txt\",\n",
    "         ref_test=\"dataset/ref_test.pkl\",\n",
    "         best_model=\"best_model/model.pt\"):\n",
    "    '''\n",
    "    This function conducts testing of a trained model on a provided test dataset. It follows these steps:\n",
    "\n",
    "    1. Initialization:\n",
    "    - Logging and Setup:\n",
    "    Logs the initiation of the testing process and loads the best model's checkpoint.\n",
    "\n",
    "    2.*Model Preparation:\n",
    "    - Model Configuration:\n",
    "    Sets the model to evaluation mode (`model.eval()`).\n",
    "    - Dataset Loading:\n",
    "    Loads the test dataset using paths provided or defaults, creating a `WikiDataset` for testing.\n",
    "\n",
    "    3. Testing Process:\n",
    "    - Data Loading:\n",
    "    Creates a DataLoader for the test dataset with predefined batch size and collation function.\n",
    "    - Evaluation:\n",
    "    Evaluates the model on the test dataset, capturing test loss, BLEU score, and SARI score.\n",
    "\n",
    "    4. Results and Reporting:\n",
    "    - Metrics Calculation:\n",
    "    Calculates average evaluation loss, BLEU score, and SARI score, dividing the loss by the batch size.\n",
    "    - Reporting:\n",
    "    Prints and logs the calculated metrics and the elapsed time for the testing process.\n",
    "\n",
    "    5. Completion:\n",
    "    - inalization:\n",
    "    Prints \"Test Complete!\" to indicate the conclusion of the testing phase.\n",
    "    '''\n",
    "\n",
    "    print(\"Testing Model module executing...\")\n",
    "    logging.info(\"Test module invoked.\")\n",
    "\n",
    "    _, _, _ = load_checkpt(base_path + best_model)\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    model.eval()\n",
    "    test_dataset = WikiDataset(base_path + src_test, base_path + tgt_test, base_path + ref_test, ref=True)\n",
    "    test_dl = DataLoader(test_dataset, batch_size=TRAIN_BATCH_SIZE, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "    test_start_time = time.time()\n",
    "    test_loss, bleu_score, sari_score = evaluate(test_dl, 0)\n",
    "    test_loss = test_loss / TRAIN_BATCH_SIZE\n",
    "\n",
    "    print(f'Avg. eval loss: {test_loss:.5f} | blue score: {bleu_score} | sari score: {sari_score} | time elapsed: {time.time() - test_start_time}')\n",
    "    logging.info(f'Avg. eval loss: {test_loss:.5f} | blue score: {bleu_score} | sari score: {sari_score} | time elapsed: {time.time() - test_start_time}')\n",
    "\n",
    "    print(\"Test Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the Model for Simplification of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(base_path=\"../\",\n",
    "           src_file=\"dataset/src_file.txt\",\n",
    "           best_model=\"best_model/model.pt\",\n",
    "           output=\"outputs/decoded.txt\"):\n",
    "    '''\n",
    "     loads a pre-trained model, decodes sentences from a specified source file using the model, and saves the decoded sentences to an output file.\n",
    "     It uses the tokenizer to encode and decode sentences and conducts these operations sequentially within the function.\n",
    "    '''\n",
    "    print(\"Decoding sentences module executing...\")\n",
    "    logging.info(\"Decode module invoked.\")\n",
    "\n",
    "    _, _, _ = load_checkpt(base_path + best_model)\n",
    "    print(\"Model loaded.\")\n",
    "\n",
    "    model.eval()\n",
    "    dataset = WikiDataset(base_path + src_file)\n",
    "    predicted_list = []\n",
    "    sent_tensors = tokenizer.encode_sent(dataset.src)\n",
    "\n",
    "    print(\"Decoding Sentences...\")\n",
    "    for sent in sent_tensors:\n",
    "        with torch.no_grad():\n",
    "            predicted = model.generate(sent[0].to(device), attention_mask=sent[1].to(device), decoder_start_token_id=model.config.decoder.decoder_start_token_id)\n",
    "            predicted_list.append(predicted.squeeze())\n",
    "\n",
    "    output = tokenizer.decode_sent_tokens(predicted_list)\n",
    "\n",
    "    with open(base_path + output, \"w\") as f:\n",
    "        for sent in output:\n",
    "            f.write(sent + \"\\n\")\n",
    "\n",
    "    print(\"Output file saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation of the model during training\n",
    "\n",
    "# function to evaluate model\n",
    "def evaluate_model(model, data_loader, criterion, target_tokenizer):\n",
    "    '''\n",
    "    Inputs:\n",
    "    model: The trained seq2seq model to evaluate.\n",
    "    data_loader: The data loader providing batches of evaluation data. It should contain pairs of source and target sequences.\n",
    "\n",
    "    criterion: The loss function used for training the model, typically nn.CrossEntropyLoss() or similar.\n",
    "    target_tokenizer: The tokenizer used for tokenizing the target sequences.\n",
    "\n",
    "    Functionality:\n",
    "    Sets the model to evaluation mode (model.eval()).\n",
    "    Iterates through the evaluation data in batches.\n",
    "    Passes the source sequences through the model to generate predictions.\n",
    "    Computes the loss between predicted sequences and actual target sequences.\n",
    "    Converts model predictions and target sequences from token IDs to text.\n",
    "    Stores references (actual target sentences) and hypotheses (predicted sentences) to calculate BLEU score.\n",
    "    Calculates the average loss and BLEU score over the evaluation dataset.\n",
    "\n",
    "    Outputs:\n",
    "    avg_loss: Average loss over the evaluation dataset.\n",
    "    bleu_score: BLEU score indicating the quality of the model's translations (here simplification) compared to the ground truth.\n",
    "    BLEU (Bilingual Evaluation Understudy) is a metric used to evaluate the quality of machine-translated text against one or more reference translations.\n",
    "    It's widely used in natural language processing and machine translation tasks, including simplification.\n",
    "    '''\n",
    "\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_batches = 0\n",
    "    references = []\n",
    "    hypotheses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            batch_source = batch[0]\n",
    "            batch_target = batch[1]\n",
    "\n",
    "            outputs = model(input_ids=batch_source, decoder_input_ids=batch_target)\n",
    "\n",
    "            logits_flat = outputs.logits.view(-1, outputs.logits.size(-1))\n",
    "            target_flat = batch_target.view(-1)\n",
    "\n",
    "            loss = criterion(logits_flat, target_flat)\n",
    "            total_loss += loss.item()\n",
    "            total_batches += 1\n",
    "\n",
    "            predicted_ids = outputs.logits.argmax(-1)\n",
    "            predicted_sentences = [target_tokenizer.decode(ids, skip_special_tokens=True) for ids in predicted_ids]\n",
    "            target_sentences = [target_tokenizer.decode(ids, skip_special_tokens=True) for ids in batch_target]\n",
    "\n",
    "            references.extend([sent.split() for sent in target_sentences])\n",
    "            hypotheses.extend([sent.split() for sent in predicted_sentences])\n",
    "\n",
    "    avg_loss = total_loss / total_batches\n",
    "\n",
    "    # Calculate BLEU score with smoothing\n",
    "    smooth_func = SmoothingFunction().method4\n",
    "    bleu_score = corpus_bleu(references, hypotheses, smoothing_function=smooth_func)\n",
    "\n",
    "    return avg_loss, bleu_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.3.crossattention.c_attn.weight', 'h.5.ln_cross_attn.weight', 'h.6.ln_cross_attn.weight', 'h.9.crossattention.q_attn.bias', 'h.9.ln_cross_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.6.crossattention.q_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.3.ln_cross_attn.bias', 'h.7.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.2.crossattention.c_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.bias', 'h.11.crossattention.q_attn.bias', 'h.4.ln_cross_attn.weight', 'h.0.ln_cross_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.7.crossattention.q_attn.bias', 'h.5.crossattention.c_proj.bias', 'h.1.ln_cross_attn.weight', 'h.4.crossattention.q_attn.bias', 'h.1.crossattention.c_proj.weight', 'h.10.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.weight', 'h.10.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.4.ln_cross_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.2.crossattention.q_attn.bias', 'h.11.crossattention.c_proj.weight', 'h.6.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.4.crossattention.c_proj.weight', 'h.5.crossattention.c_attn.bias', 'h.7.crossattention.c_proj.bias', 'h.10.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.8.ln_cross_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.5.ln_cross_attn.bias', 'h.9.ln_cross_attn.bias', 'h.1.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.2.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.weight', 'h.0.crossattention.c_proj.weight', 'h.8.crossattention.c_attn.bias', 'h.2.ln_cross_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.11.ln_cross_attn.bias', 'h.1.crossattention.c_attn.bias', 'h.8.crossattention.c_proj.weight', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.0.ln_cross_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.1.crossattention.q_attn.weight', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.weight', 'h.10.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.1.crossattention.q_attn.bias', 'h.7.crossattention.c_proj.weight', 'h.10.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.9.crossattention.c_proj.bias', 'h.7.ln_cross_attn.bias', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.6.crossattention.c_proj.weight', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.q_attn.weight', 'h.11.ln_cross_attn.weight', 'h.8.crossattention.q_attn.weight', 'h.5.crossattention.q_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.8.crossattention.q_attn.bias', 'h.8.ln_cross_attn.bias', 'h.8.crossattention.c_proj.bias', 'h.6.crossattention.q_attn.bias', 'h.10.ln_cross_attn.weight', 'h.9.crossattention.q_attn.weight', 'h.11.crossattention.c_attn.weight', 'h.8.crossattention.c_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Evaluation Loss: 1.3135, Evaluation BLEU: 0.0000\n",
      "Epoch [2/2000], Evaluation Loss: 4.9308, Evaluation BLEU: 0.0000\n",
      "Epoch [3/2000], Evaluation Loss: 8.6167, Evaluation BLEU: 0.0000\n",
      "Epoch [4/2000], Evaluation Loss: 8.1539, Evaluation BLEU: 0.0000\n",
      "Epoch [5/2000], Evaluation Loss: 9.7744, Evaluation BLEU: 0.0000\n",
      "Epoch [6/2000], Evaluation Loss: 5.3611, Evaluation BLEU: 0.0000\n",
      "Epoch [7/2000], Evaluation Loss: 5.2928, Evaluation BLEU: 0.0000\n",
      "Epoch [8/2000], Evaluation Loss: 13.3926, Evaluation BLEU: 0.0000\n",
      "Epoch [9/2000], Evaluation Loss: 6.7120, Evaluation BLEU: 0.0000\n",
      "Epoch [10/2000], Evaluation Loss: 10.0990, Evaluation BLEU: 0.0000\n",
      "Epoch [11/2000], Evaluation Loss: 9.5260, Evaluation BLEU: 0.0000\n",
      "Epoch [12/2000], Evaluation Loss: 8.7591, Evaluation BLEU: 0.0000\n",
      "Epoch [13/2000], Evaluation Loss: 8.6172, Evaluation BLEU: 0.0000\n",
      "Epoch [14/2000], Evaluation Loss: 9.0559, Evaluation BLEU: 0.0000\n",
      "Epoch [15/2000], Evaluation Loss: 9.2633, Evaluation BLEU: 0.0000\n",
      "Epoch [16/2000], Evaluation Loss: 9.4647, Evaluation BLEU: 0.0000\n",
      "Epoch [17/2000], Evaluation Loss: 9.6736, Evaluation BLEU: 0.0000\n",
      "Epoch [18/2000], Evaluation Loss: 9.5711, Evaluation BLEU: 0.0000\n",
      "Epoch [19/2000], Evaluation Loss: 9.6535, Evaluation BLEU: 0.0000\n",
      "Epoch [20/2000], Evaluation Loss: 9.5636, Evaluation BLEU: 0.0000\n",
      "Epoch [21/2000], Evaluation Loss: 9.5798, Evaluation BLEU: 0.0005\n",
      "Epoch [22/2000], Evaluation Loss: 9.5961, Evaluation BLEU: 0.0000\n",
      "Epoch [23/2000], Evaluation Loss: 9.5012, Evaluation BLEU: 0.0000\n",
      "Epoch [24/2000], Evaluation Loss: 9.2994, Evaluation BLEU: 0.0000\n",
      "Epoch [25/2000], Evaluation Loss: 9.1307, Evaluation BLEU: 0.0000\n",
      "Epoch [26/2000], Evaluation Loss: 9.1679, Evaluation BLEU: 0.0000\n",
      "Epoch [27/2000], Evaluation Loss: 9.3277, Evaluation BLEU: 0.0000\n",
      "Epoch [28/2000], Evaluation Loss: 9.1922, Evaluation BLEU: 0.0000\n",
      "Epoch [29/2000], Evaluation Loss: 9.0526, Evaluation BLEU: 0.0000\n",
      "Epoch [30/2000], Evaluation Loss: 9.1751, Evaluation BLEU: 0.0000\n",
      "Epoch [31/2000], Evaluation Loss: 10.1909, Evaluation BLEU: 0.0000\n",
      "Epoch [32/2000], Evaluation Loss: 11.7618, Evaluation BLEU: 0.0000\n",
      "Epoch [33/2000], Evaluation Loss: 13.9300, Evaluation BLEU: 0.0000\n",
      "Epoch [34/2000], Evaluation Loss: 11.4601, Evaluation BLEU: 0.0000\n",
      "Epoch [35/2000], Evaluation Loss: 10.7437, Evaluation BLEU: 0.0000\n",
      "Epoch [36/2000], Evaluation Loss: 9.7769, Evaluation BLEU: 0.0000\n",
      "Epoch [37/2000], Evaluation Loss: 9.1975, Evaluation BLEU: 0.0000\n",
      "Epoch [38/2000], Evaluation Loss: 8.9801, Evaluation BLEU: 0.0000\n",
      "Epoch [39/2000], Evaluation Loss: 8.9117, Evaluation BLEU: 0.0000\n",
      "Epoch [40/2000], Evaluation Loss: 8.8913, Evaluation BLEU: 0.0000\n",
      "Epoch [41/2000], Evaluation Loss: 8.9691, Evaluation BLEU: 0.0000\n",
      "Epoch [42/2000], Evaluation Loss: 9.1677, Evaluation BLEU: 0.0000\n",
      "Epoch [43/2000], Evaluation Loss: 9.6563, Evaluation BLEU: 0.0000\n",
      "Epoch [44/2000], Evaluation Loss: 10.8880, Evaluation BLEU: 0.0000\n",
      "Epoch [45/2000], Evaluation Loss: 13.1605, Evaluation BLEU: 0.0000\n",
      "Epoch [46/2000], Evaluation Loss: 15.2916, Evaluation BLEU: 0.0000\n",
      "Epoch [47/2000], Evaluation Loss: 15.8944, Evaluation BLEU: 0.0000\n",
      "Epoch [48/2000], Evaluation Loss: 16.2490, Evaluation BLEU: 0.0000\n",
      "Epoch [49/2000], Evaluation Loss: 17.5691, Evaluation BLEU: 0.0000\n",
      "Epoch [50/2000], Evaluation Loss: 18.2192, Evaluation BLEU: 0.0000\n",
      "Epoch [51/2000], Evaluation Loss: 17.8092, Evaluation BLEU: 0.0000\n",
      "Epoch [52/2000], Evaluation Loss: 18.0772, Evaluation BLEU: 0.0000\n",
      "Epoch [53/2000], Evaluation Loss: 17.6582, Evaluation BLEU: 0.0000\n",
      "Epoch [54/2000], Evaluation Loss: 16.9985, Evaluation BLEU: 0.0000\n",
      "Epoch [55/2000], Evaluation Loss: 17.0762, Evaluation BLEU: 0.0000\n",
      "Epoch [56/2000], Evaluation Loss: 17.7842, Evaluation BLEU: 0.0000\n",
      "Epoch [57/2000], Evaluation Loss: 18.5845, Evaluation BLEU: 0.0000\n",
      "Epoch [58/2000], Evaluation Loss: 19.1365, Evaluation BLEU: 0.0000\n",
      "Epoch [59/2000], Evaluation Loss: 19.3647, Evaluation BLEU: 0.0000\n",
      "Epoch [60/2000], Evaluation Loss: 19.3557, Evaluation BLEU: 0.0000\n",
      "Epoch [61/2000], Evaluation Loss: 19.5297, Evaluation BLEU: 0.0000\n",
      "Epoch [62/2000], Evaluation Loss: 20.5219, Evaluation BLEU: 0.0000\n",
      "Epoch [63/2000], Evaluation Loss: 22.0741, Evaluation BLEU: 0.0000\n",
      "Epoch [64/2000], Evaluation Loss: 23.3577, Evaluation BLEU: 0.0000\n",
      "Epoch [65/2000], Evaluation Loss: 23.7725, Evaluation BLEU: 0.0000\n",
      "Epoch [66/2000], Evaluation Loss: 23.4594, Evaluation BLEU: 0.0000\n",
      "Epoch [67/2000], Evaluation Loss: 22.8957, Evaluation BLEU: 0.0000\n",
      "Epoch [68/2000], Evaluation Loss: 22.4208, Evaluation BLEU: 0.0000\n",
      "Epoch [69/2000], Evaluation Loss: 22.1536, Evaluation BLEU: 0.0000\n",
      "Epoch [70/2000], Evaluation Loss: 22.0773, Evaluation BLEU: 0.0000\n",
      "Epoch [71/2000], Evaluation Loss: 22.1058, Evaluation BLEU: 0.0000\n",
      "Epoch [72/2000], Evaluation Loss: 22.1661, Evaluation BLEU: 0.0000\n",
      "Epoch [73/2000], Evaluation Loss: 22.2196, Evaluation BLEU: 0.0000\n",
      "Epoch [74/2000], Evaluation Loss: 22.2541, Evaluation BLEU: 0.0000\n",
      "Epoch [75/2000], Evaluation Loss: 22.2701, Evaluation BLEU: 0.0000\n",
      "Epoch [76/2000], Evaluation Loss: 22.2724, Evaluation BLEU: 0.0000\n",
      "Epoch [77/2000], Evaluation Loss: 22.2665, Evaluation BLEU: 0.0000\n",
      "Epoch [78/2000], Evaluation Loss: 22.2570, Evaluation BLEU: 0.0000\n",
      "Epoch [79/2000], Evaluation Loss: 22.2470, Evaluation BLEU: 0.0000\n",
      "Epoch [80/2000], Evaluation Loss: 22.2381, Evaluation BLEU: 0.0000\n",
      "Epoch [81/2000], Evaluation Loss: 22.2309, Evaluation BLEU: 0.0000\n",
      "Epoch [82/2000], Evaluation Loss: 22.2250, Evaluation BLEU: 0.0000\n",
      "Epoch [83/2000], Evaluation Loss: 22.2200, Evaluation BLEU: 0.0000\n",
      "Epoch [84/2000], Evaluation Loss: 22.2157, Evaluation BLEU: 0.0000\n",
      "Epoch [85/2000], Evaluation Loss: 22.2120, Evaluation BLEU: 0.0000\n",
      "Epoch [86/2000], Evaluation Loss: 22.2093, Evaluation BLEU: 0.0000\n",
      "Epoch [87/2000], Evaluation Loss: 22.2085, Evaluation BLEU: 0.0000\n",
      "Epoch [88/2000], Evaluation Loss: 22.2102, Evaluation BLEU: 0.0000\n",
      "Epoch [89/2000], Evaluation Loss: 22.2152, Evaluation BLEU: 0.0000\n",
      "Epoch [90/2000], Evaluation Loss: 22.2239, Evaluation BLEU: 0.0000\n",
      "Epoch [91/2000], Evaluation Loss: 22.2365, Evaluation BLEU: 0.0000\n",
      "Epoch [92/2000], Evaluation Loss: 22.2529, Evaluation BLEU: 0.0000\n",
      "Epoch [93/2000], Evaluation Loss: 22.2724, Evaluation BLEU: 0.0000\n",
      "Epoch [94/2000], Evaluation Loss: 22.2943, Evaluation BLEU: 0.0000\n",
      "Epoch [95/2000], Evaluation Loss: 22.3179, Evaluation BLEU: 0.0000\n",
      "Epoch [96/2000], Evaluation Loss: 22.3421, Evaluation BLEU: 0.0000\n",
      "Epoch [97/2000], Evaluation Loss: 22.3662, Evaluation BLEU: 0.0000\n",
      "Epoch [98/2000], Evaluation Loss: 22.3894, Evaluation BLEU: 0.0000\n",
      "Epoch [99/2000], Evaluation Loss: 22.4112, Evaluation BLEU: 0.0000\n",
      "Epoch [100/2000], Evaluation Loss: 22.4311, Evaluation BLEU: 0.0000\n",
      "Epoch [101/2000], Evaluation Loss: 22.4490, Evaluation BLEU: 0.0000\n",
      "Epoch [102/2000], Evaluation Loss: 22.4648, Evaluation BLEU: 0.0000\n",
      "Epoch [103/2000], Evaluation Loss: 22.4787, Evaluation BLEU: 0.0000\n",
      "Epoch [104/2000], Evaluation Loss: 22.4907, Evaluation BLEU: 0.0000\n",
      "Epoch [105/2000], Evaluation Loss: 22.5011, Evaluation BLEU: 0.0000\n",
      "Epoch [106/2000], Evaluation Loss: 22.5101, Evaluation BLEU: 0.0000\n",
      "Epoch [107/2000], Evaluation Loss: 22.5181, Evaluation BLEU: 0.0000\n",
      "Epoch [108/2000], Evaluation Loss: 22.5252, Evaluation BLEU: 0.0000\n",
      "Epoch [109/2000], Evaluation Loss: 22.5317, Evaluation BLEU: 0.0000\n",
      "Epoch [110/2000], Evaluation Loss: 22.5378, Evaluation BLEU: 0.0000\n",
      "Epoch [111/2000], Evaluation Loss: 22.5435, Evaluation BLEU: 0.0000\n",
      "Epoch [112/2000], Evaluation Loss: 22.5491, Evaluation BLEU: 0.0000\n",
      "Epoch [113/2000], Evaluation Loss: 22.5545, Evaluation BLEU: 0.0000\n",
      "Epoch [114/2000], Evaluation Loss: 22.5599, Evaluation BLEU: 0.0000\n",
      "Epoch [115/2000], Evaluation Loss: 22.5653, Evaluation BLEU: 0.0000\n",
      "Epoch [116/2000], Evaluation Loss: 22.5707, Evaluation BLEU: 0.0000\n",
      "Epoch [117/2000], Evaluation Loss: 22.5761, Evaluation BLEU: 0.0000\n",
      "Epoch [118/2000], Evaluation Loss: 22.5815, Evaluation BLEU: 0.0000\n",
      "Epoch [119/2000], Evaluation Loss: 22.5869, Evaluation BLEU: 0.0000\n",
      "Epoch [120/2000], Evaluation Loss: 22.5923, Evaluation BLEU: 0.0000\n",
      "Epoch [121/2000], Evaluation Loss: 22.5975, Evaluation BLEU: 0.0000\n",
      "Epoch [122/2000], Evaluation Loss: 22.6027, Evaluation BLEU: 0.0000\n",
      "Epoch [123/2000], Evaluation Loss: 22.6079, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [124/2000], Evaluation Loss: 22.6129, Evaluation BLEU: 0.0000\n",
      "Epoch [125/2000], Evaluation Loss: 22.6177, Evaluation BLEU: 0.0000\n",
      "Epoch [126/2000], Evaluation Loss: 22.6225, Evaluation BLEU: 0.0000\n",
      "Epoch [127/2000], Evaluation Loss: 22.6271, Evaluation BLEU: 0.0000\n",
      "Epoch [128/2000], Evaluation Loss: 22.6316, Evaluation BLEU: 0.0000\n",
      "Epoch [129/2000], Evaluation Loss: 22.6359, Evaluation BLEU: 0.0000\n",
      "Epoch [130/2000], Evaluation Loss: 22.6401, Evaluation BLEU: 0.0000\n",
      "Epoch [131/2000], Evaluation Loss: 22.6442, Evaluation BLEU: 0.0000\n",
      "Epoch [132/2000], Evaluation Loss: 22.6482, Evaluation BLEU: 0.0000\n",
      "Epoch [133/2000], Evaluation Loss: 22.6521, Evaluation BLEU: 0.0000\n",
      "Epoch [134/2000], Evaluation Loss: 22.6558, Evaluation BLEU: 0.0000\n",
      "Epoch [135/2000], Evaluation Loss: 22.6595, Evaluation BLEU: 0.0000\n",
      "Epoch [136/2000], Evaluation Loss: 22.6632, Evaluation BLEU: 0.0000\n",
      "Epoch [137/2000], Evaluation Loss: 22.6668, Evaluation BLEU: 0.0000\n",
      "Epoch [138/2000], Evaluation Loss: 22.6703, Evaluation BLEU: 0.0000\n",
      "Epoch [139/2000], Evaluation Loss: 22.6737, Evaluation BLEU: 0.0000\n",
      "Epoch [140/2000], Evaluation Loss: 22.6772, Evaluation BLEU: 0.0000\n",
      "Epoch [141/2000], Evaluation Loss: 22.6806, Evaluation BLEU: 0.0000\n",
      "Epoch [142/2000], Evaluation Loss: 22.6839, Evaluation BLEU: 0.0000\n",
      "Epoch [143/2000], Evaluation Loss: 22.6873, Evaluation BLEU: 0.0000\n",
      "Epoch [144/2000], Evaluation Loss: 22.6906, Evaluation BLEU: 0.0000\n",
      "Epoch [145/2000], Evaluation Loss: 22.6939, Evaluation BLEU: 0.0000\n",
      "Epoch [146/2000], Evaluation Loss: 22.6972, Evaluation BLEU: 0.0000\n",
      "Epoch [147/2000], Evaluation Loss: 22.7005, Evaluation BLEU: 0.0000\n",
      "Epoch [148/2000], Evaluation Loss: 22.7037, Evaluation BLEU: 0.0000\n",
      "Epoch [149/2000], Evaluation Loss: 22.7070, Evaluation BLEU: 0.0000\n",
      "Epoch [150/2000], Evaluation Loss: 22.7102, Evaluation BLEU: 0.0000\n",
      "Epoch [151/2000], Evaluation Loss: 22.7134, Evaluation BLEU: 0.0000\n",
      "Epoch [152/2000], Evaluation Loss: 22.7166, Evaluation BLEU: 0.0000\n",
      "Epoch [153/2000], Evaluation Loss: 22.7198, Evaluation BLEU: 0.0000\n",
      "Epoch [154/2000], Evaluation Loss: 22.7229, Evaluation BLEU: 0.0000\n",
      "Epoch [155/2000], Evaluation Loss: 22.7260, Evaluation BLEU: 0.0000\n",
      "Epoch [156/2000], Evaluation Loss: 22.7291, Evaluation BLEU: 0.0000\n",
      "Epoch [157/2000], Evaluation Loss: 22.7323, Evaluation BLEU: 0.0000\n",
      "Epoch [158/2000], Evaluation Loss: 22.7353, Evaluation BLEU: 0.0000\n",
      "Epoch [159/2000], Evaluation Loss: 22.7384, Evaluation BLEU: 0.0000\n",
      "Epoch [160/2000], Evaluation Loss: 22.7414, Evaluation BLEU: 0.0000\n",
      "Epoch [161/2000], Evaluation Loss: 22.7444, Evaluation BLEU: 0.0000\n",
      "Epoch [162/2000], Evaluation Loss: 22.7474, Evaluation BLEU: 0.0000\n",
      "Epoch [163/2000], Evaluation Loss: 22.7503, Evaluation BLEU: 0.0000\n",
      "Epoch [164/2000], Evaluation Loss: 22.7532, Evaluation BLEU: 0.0000\n",
      "Epoch [165/2000], Evaluation Loss: 22.7561, Evaluation BLEU: 0.0000\n",
      "Epoch [166/2000], Evaluation Loss: 22.7590, Evaluation BLEU: 0.0000\n",
      "Epoch [167/2000], Evaluation Loss: 22.7618, Evaluation BLEU: 0.0000\n",
      "Epoch [168/2000], Evaluation Loss: 22.7646, Evaluation BLEU: 0.0000\n",
      "Epoch [169/2000], Evaluation Loss: 22.7674, Evaluation BLEU: 0.0000\n",
      "Epoch [170/2000], Evaluation Loss: 22.7701, Evaluation BLEU: 0.0000\n",
      "Epoch [171/2000], Evaluation Loss: 22.7729, Evaluation BLEU: 0.0000\n",
      "Epoch [172/2000], Evaluation Loss: 22.7756, Evaluation BLEU: 0.0000\n",
      "Epoch [173/2000], Evaluation Loss: 22.7783, Evaluation BLEU: 0.0000\n",
      "Epoch [174/2000], Evaluation Loss: 22.7809, Evaluation BLEU: 0.0000\n",
      "Epoch [175/2000], Evaluation Loss: 22.7835, Evaluation BLEU: 0.0000\n",
      "Epoch [176/2000], Evaluation Loss: 22.7861, Evaluation BLEU: 0.0000\n",
      "Epoch [177/2000], Evaluation Loss: 22.7887, Evaluation BLEU: 0.0000\n",
      "Epoch [178/2000], Evaluation Loss: 22.7912, Evaluation BLEU: 0.0000\n",
      "Epoch [179/2000], Evaluation Loss: 22.7938, Evaluation BLEU: 0.0000\n",
      "Epoch [180/2000], Evaluation Loss: 22.7962, Evaluation BLEU: 0.0000\n",
      "Epoch [181/2000], Evaluation Loss: 22.7987, Evaluation BLEU: 0.0000\n",
      "Epoch [182/2000], Evaluation Loss: 22.8012, Evaluation BLEU: 0.0000\n",
      "Epoch [183/2000], Evaluation Loss: 22.8036, Evaluation BLEU: 0.0000\n",
      "Epoch [184/2000], Evaluation Loss: 22.8060, Evaluation BLEU: 0.0000\n",
      "Epoch [185/2000], Evaluation Loss: 22.8084, Evaluation BLEU: 0.0000\n",
      "Epoch [186/2000], Evaluation Loss: 22.8108, Evaluation BLEU: 0.0000\n",
      "Epoch [187/2000], Evaluation Loss: 22.8131, Evaluation BLEU: 0.0000\n",
      "Epoch [188/2000], Evaluation Loss: 22.8154, Evaluation BLEU: 0.0000\n",
      "Epoch [189/2000], Evaluation Loss: 22.8177, Evaluation BLEU: 0.0000\n",
      "Epoch [190/2000], Evaluation Loss: 22.8200, Evaluation BLEU: 0.0000\n",
      "Epoch [191/2000], Evaluation Loss: 22.8222, Evaluation BLEU: 0.0000\n",
      "Epoch [192/2000], Evaluation Loss: 22.8245, Evaluation BLEU: 0.0000\n",
      "Epoch [193/2000], Evaluation Loss: 22.8267, Evaluation BLEU: 0.0000\n",
      "Epoch [194/2000], Evaluation Loss: 22.8289, Evaluation BLEU: 0.0000\n",
      "Epoch [195/2000], Evaluation Loss: 22.8311, Evaluation BLEU: 0.0000\n",
      "Epoch [196/2000], Evaluation Loss: 22.8333, Evaluation BLEU: 0.0000\n",
      "Epoch [197/2000], Evaluation Loss: 22.8354, Evaluation BLEU: 0.0000\n",
      "Epoch [198/2000], Evaluation Loss: 22.8375, Evaluation BLEU: 0.0000\n",
      "Epoch [199/2000], Evaluation Loss: 22.8397, Evaluation BLEU: 0.0000\n",
      "Epoch [200/2000], Evaluation Loss: 22.8418, Evaluation BLEU: 0.0000\n",
      "Epoch [201/2000], Evaluation Loss: 22.8439, Evaluation BLEU: 0.0000\n",
      "Epoch [202/2000], Evaluation Loss: 22.8460, Evaluation BLEU: 0.0000\n",
      "Epoch [203/2000], Evaluation Loss: 22.8480, Evaluation BLEU: 0.0000\n",
      "Epoch [204/2000], Evaluation Loss: 22.8501, Evaluation BLEU: 0.0000\n",
      "Epoch [205/2000], Evaluation Loss: 22.8521, Evaluation BLEU: 0.0000\n",
      "Epoch [206/2000], Evaluation Loss: 22.8541, Evaluation BLEU: 0.0000\n",
      "Epoch [207/2000], Evaluation Loss: 22.8561, Evaluation BLEU: 0.0000\n",
      "Epoch [208/2000], Evaluation Loss: 22.8581, Evaluation BLEU: 0.0000\n",
      "Epoch [209/2000], Evaluation Loss: 22.8601, Evaluation BLEU: 0.0000\n",
      "Epoch [210/2000], Evaluation Loss: 22.8621, Evaluation BLEU: 0.0000\n",
      "Epoch [211/2000], Evaluation Loss: 22.8641, Evaluation BLEU: 0.0000\n",
      "Epoch [212/2000], Evaluation Loss: 22.8660, Evaluation BLEU: 0.0000\n",
      "Epoch [213/2000], Evaluation Loss: 22.8679, Evaluation BLEU: 0.0000\n",
      "Epoch [214/2000], Evaluation Loss: 22.8698, Evaluation BLEU: 0.0000\n",
      "Epoch [215/2000], Evaluation Loss: 22.8717, Evaluation BLEU: 0.0000\n",
      "Epoch [216/2000], Evaluation Loss: 22.8736, Evaluation BLEU: 0.0000\n",
      "Epoch [217/2000], Evaluation Loss: 22.8755, Evaluation BLEU: 0.0000\n",
      "Epoch [218/2000], Evaluation Loss: 22.8774, Evaluation BLEU: 0.0000\n",
      "Epoch [219/2000], Evaluation Loss: 22.8792, Evaluation BLEU: 0.0000\n",
      "Epoch [220/2000], Evaluation Loss: 22.8811, Evaluation BLEU: 0.0000\n",
      "Epoch [221/2000], Evaluation Loss: 22.8829, Evaluation BLEU: 0.0000\n",
      "Epoch [222/2000], Evaluation Loss: 22.8848, Evaluation BLEU: 0.0000\n",
      "Epoch [223/2000], Evaluation Loss: 22.8866, Evaluation BLEU: 0.0000\n",
      "Epoch [224/2000], Evaluation Loss: 22.8884, Evaluation BLEU: 0.0000\n",
      "Epoch [225/2000], Evaluation Loss: 22.8902, Evaluation BLEU: 0.0000\n",
      "Epoch [226/2000], Evaluation Loss: 22.8920, Evaluation BLEU: 0.0000\n",
      "Epoch [227/2000], Evaluation Loss: 22.8937, Evaluation BLEU: 0.0000\n",
      "Epoch [228/2000], Evaluation Loss: 22.8955, Evaluation BLEU: 0.0000\n",
      "Epoch [229/2000], Evaluation Loss: 22.8972, Evaluation BLEU: 0.0000\n",
      "Epoch [230/2000], Evaluation Loss: 22.8990, Evaluation BLEU: 0.0000\n",
      "Epoch [231/2000], Evaluation Loss: 22.9007, Evaluation BLEU: 0.0000\n",
      "Epoch [232/2000], Evaluation Loss: 22.9024, Evaluation BLEU: 0.0000\n",
      "Epoch [233/2000], Evaluation Loss: 22.9041, Evaluation BLEU: 0.0000\n",
      "Epoch [234/2000], Evaluation Loss: 22.9058, Evaluation BLEU: 0.0000\n",
      "Epoch [235/2000], Evaluation Loss: 22.9075, Evaluation BLEU: 0.0000\n",
      "Epoch [236/2000], Evaluation Loss: 22.9092, Evaluation BLEU: 0.0000\n",
      "Epoch [237/2000], Evaluation Loss: 22.9108, Evaluation BLEU: 0.0000\n",
      "Epoch [238/2000], Evaluation Loss: 22.9125, Evaluation BLEU: 0.0000\n",
      "Epoch [239/2000], Evaluation Loss: 22.9141, Evaluation BLEU: 0.0000\n",
      "Epoch [240/2000], Evaluation Loss: 22.9158, Evaluation BLEU: 0.0000\n",
      "Epoch [241/2000], Evaluation Loss: 22.9174, Evaluation BLEU: 0.0000\n",
      "Epoch [242/2000], Evaluation Loss: 22.9190, Evaluation BLEU: 0.0000\n",
      "Epoch [243/2000], Evaluation Loss: 22.9205, Evaluation BLEU: 0.0000\n",
      "Epoch [244/2000], Evaluation Loss: 22.9221, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [245/2000], Evaluation Loss: 22.9237, Evaluation BLEU: 0.0000\n",
      "Epoch [246/2000], Evaluation Loss: 22.9253, Evaluation BLEU: 0.0000\n",
      "Epoch [247/2000], Evaluation Loss: 22.9268, Evaluation BLEU: 0.0000\n",
      "Epoch [248/2000], Evaluation Loss: 22.9283, Evaluation BLEU: 0.0000\n",
      "Epoch [249/2000], Evaluation Loss: 22.9299, Evaluation BLEU: 0.0000\n",
      "Epoch [250/2000], Evaluation Loss: 22.9314, Evaluation BLEU: 0.0000\n",
      "Epoch [251/2000], Evaluation Loss: 22.9329, Evaluation BLEU: 0.0000\n",
      "Epoch [252/2000], Evaluation Loss: 22.9344, Evaluation BLEU: 0.0000\n",
      "Epoch [253/2000], Evaluation Loss: 22.9359, Evaluation BLEU: 0.0000\n",
      "Epoch [254/2000], Evaluation Loss: 22.9373, Evaluation BLEU: 0.0000\n",
      "Epoch [255/2000], Evaluation Loss: 22.9388, Evaluation BLEU: 0.0000\n",
      "Epoch [256/2000], Evaluation Loss: 22.9403, Evaluation BLEU: 0.0000\n",
      "Epoch [257/2000], Evaluation Loss: 22.9417, Evaluation BLEU: 0.0000\n",
      "Epoch [258/2000], Evaluation Loss: 22.9431, Evaluation BLEU: 0.0000\n",
      "Epoch [259/2000], Evaluation Loss: 22.9446, Evaluation BLEU: 0.0000\n",
      "Epoch [260/2000], Evaluation Loss: 22.9460, Evaluation BLEU: 0.0000\n",
      "Epoch [261/2000], Evaluation Loss: 22.9474, Evaluation BLEU: 0.0000\n",
      "Epoch [262/2000], Evaluation Loss: 22.9488, Evaluation BLEU: 0.0000\n",
      "Epoch [263/2000], Evaluation Loss: 22.9502, Evaluation BLEU: 0.0000\n",
      "Epoch [264/2000], Evaluation Loss: 22.9516, Evaluation BLEU: 0.0000\n",
      "Epoch [265/2000], Evaluation Loss: 22.9530, Evaluation BLEU: 0.0000\n",
      "Epoch [266/2000], Evaluation Loss: 22.9544, Evaluation BLEU: 0.0000\n",
      "Epoch [267/2000], Evaluation Loss: 22.9558, Evaluation BLEU: 0.0000\n",
      "Epoch [268/2000], Evaluation Loss: 22.9572, Evaluation BLEU: 0.0000\n",
      "Epoch [269/2000], Evaluation Loss: 22.9585, Evaluation BLEU: 0.0000\n",
      "Epoch [270/2000], Evaluation Loss: 22.9599, Evaluation BLEU: 0.0000\n",
      "Epoch [271/2000], Evaluation Loss: 22.9612, Evaluation BLEU: 0.0000\n",
      "Epoch [272/2000], Evaluation Loss: 22.9626, Evaluation BLEU: 0.0000\n",
      "Epoch [273/2000], Evaluation Loss: 22.9640, Evaluation BLEU: 0.0000\n",
      "Epoch [274/2000], Evaluation Loss: 22.9653, Evaluation BLEU: 0.0000\n",
      "Epoch [275/2000], Evaluation Loss: 22.9666, Evaluation BLEU: 0.0000\n",
      "Epoch [276/2000], Evaluation Loss: 22.9680, Evaluation BLEU: 0.0000\n",
      "Epoch [277/2000], Evaluation Loss: 22.9693, Evaluation BLEU: 0.0000\n",
      "Epoch [278/2000], Evaluation Loss: 22.9706, Evaluation BLEU: 0.0000\n",
      "Epoch [279/2000], Evaluation Loss: 22.9719, Evaluation BLEU: 0.0000\n",
      "Epoch [280/2000], Evaluation Loss: 22.9732, Evaluation BLEU: 0.0000\n",
      "Epoch [281/2000], Evaluation Loss: 22.9745, Evaluation BLEU: 0.0000\n",
      "Epoch [282/2000], Evaluation Loss: 22.9757, Evaluation BLEU: 0.0000\n",
      "Epoch [283/2000], Evaluation Loss: 22.9770, Evaluation BLEU: 0.0000\n",
      "Epoch [284/2000], Evaluation Loss: 22.9782, Evaluation BLEU: 0.0000\n",
      "Epoch [285/2000], Evaluation Loss: 22.9795, Evaluation BLEU: 0.0000\n",
      "Epoch [286/2000], Evaluation Loss: 22.9807, Evaluation BLEU: 0.0000\n",
      "Epoch [287/2000], Evaluation Loss: 22.9820, Evaluation BLEU: 0.0000\n",
      "Epoch [288/2000], Evaluation Loss: 22.9832, Evaluation BLEU: 0.0000\n",
      "Epoch [289/2000], Evaluation Loss: 22.9845, Evaluation BLEU: 0.0000\n",
      "Epoch [290/2000], Evaluation Loss: 22.9857, Evaluation BLEU: 0.0000\n",
      "Epoch [291/2000], Evaluation Loss: 22.9869, Evaluation BLEU: 0.0000\n",
      "Epoch [292/2000], Evaluation Loss: 22.9881, Evaluation BLEU: 0.0000\n",
      "Epoch [293/2000], Evaluation Loss: 22.9893, Evaluation BLEU: 0.0000\n",
      "Epoch [294/2000], Evaluation Loss: 22.9905, Evaluation BLEU: 0.0000\n",
      "Epoch [295/2000], Evaluation Loss: 22.9916, Evaluation BLEU: 0.0000\n",
      "Epoch [296/2000], Evaluation Loss: 22.9928, Evaluation BLEU: 0.0000\n",
      "Epoch [297/2000], Evaluation Loss: 22.9940, Evaluation BLEU: 0.0000\n",
      "Epoch [298/2000], Evaluation Loss: 22.9952, Evaluation BLEU: 0.0000\n",
      "Epoch [299/2000], Evaluation Loss: 22.9963, Evaluation BLEU: 0.0000\n",
      "Epoch [300/2000], Evaluation Loss: 22.9975, Evaluation BLEU: 0.0000\n",
      "Epoch [301/2000], Evaluation Loss: 22.9986, Evaluation BLEU: 0.0000\n",
      "Epoch [302/2000], Evaluation Loss: 22.9997, Evaluation BLEU: 0.0000\n",
      "Epoch [303/2000], Evaluation Loss: 23.0009, Evaluation BLEU: 0.0000\n",
      "Epoch [304/2000], Evaluation Loss: 23.0020, Evaluation BLEU: 0.0000\n",
      "Epoch [305/2000], Evaluation Loss: 23.0032, Evaluation BLEU: 0.0000\n",
      "Epoch [306/2000], Evaluation Loss: 23.0043, Evaluation BLEU: 0.0000\n",
      "Epoch [307/2000], Evaluation Loss: 23.0054, Evaluation BLEU: 0.0000\n",
      "Epoch [308/2000], Evaluation Loss: 23.0066, Evaluation BLEU: 0.0000\n",
      "Epoch [309/2000], Evaluation Loss: 23.0077, Evaluation BLEU: 0.0000\n",
      "Epoch [310/2000], Evaluation Loss: 23.0088, Evaluation BLEU: 0.0000\n",
      "Epoch [311/2000], Evaluation Loss: 23.0099, Evaluation BLEU: 0.0000\n",
      "Epoch [312/2000], Evaluation Loss: 23.0110, Evaluation BLEU: 0.0000\n",
      "Epoch [313/2000], Evaluation Loss: 23.0121, Evaluation BLEU: 0.0000\n",
      "Epoch [314/2000], Evaluation Loss: 23.0132, Evaluation BLEU: 0.0000\n",
      "Epoch [315/2000], Evaluation Loss: 23.0142, Evaluation BLEU: 0.0000\n",
      "Epoch [316/2000], Evaluation Loss: 23.0153, Evaluation BLEU: 0.0000\n",
      "Epoch [317/2000], Evaluation Loss: 23.0164, Evaluation BLEU: 0.0000\n",
      "Epoch [318/2000], Evaluation Loss: 23.0175, Evaluation BLEU: 0.0000\n",
      "Epoch [319/2000], Evaluation Loss: 23.0185, Evaluation BLEU: 0.0000\n",
      "Epoch [320/2000], Evaluation Loss: 23.0196, Evaluation BLEU: 0.0000\n",
      "Epoch [321/2000], Evaluation Loss: 23.0206, Evaluation BLEU: 0.0000\n",
      "Epoch [322/2000], Evaluation Loss: 23.0217, Evaluation BLEU: 0.0000\n",
      "Epoch [323/2000], Evaluation Loss: 23.0227, Evaluation BLEU: 0.0000\n",
      "Epoch [324/2000], Evaluation Loss: 23.0237, Evaluation BLEU: 0.0000\n",
      "Epoch [325/2000], Evaluation Loss: 23.0248, Evaluation BLEU: 0.0000\n",
      "Epoch [326/2000], Evaluation Loss: 23.0258, Evaluation BLEU: 0.0000\n",
      "Epoch [327/2000], Evaluation Loss: 23.0268, Evaluation BLEU: 0.0000\n",
      "Epoch [328/2000], Evaluation Loss: 23.0278, Evaluation BLEU: 0.0000\n",
      "Epoch [329/2000], Evaluation Loss: 23.0288, Evaluation BLEU: 0.0000\n",
      "Epoch [330/2000], Evaluation Loss: 23.0298, Evaluation BLEU: 0.0000\n",
      "Epoch [331/2000], Evaluation Loss: 23.0308, Evaluation BLEU: 0.0000\n",
      "Epoch [332/2000], Evaluation Loss: 23.0317, Evaluation BLEU: 0.0000\n",
      "Epoch [333/2000], Evaluation Loss: 23.0327, Evaluation BLEU: 0.0000\n",
      "Epoch [334/2000], Evaluation Loss: 23.0337, Evaluation BLEU: 0.0000\n",
      "Epoch [335/2000], Evaluation Loss: 23.0346, Evaluation BLEU: 0.0000\n",
      "Epoch [336/2000], Evaluation Loss: 23.0355, Evaluation BLEU: 0.0000\n",
      "Epoch [337/2000], Evaluation Loss: 23.0365, Evaluation BLEU: 0.0000\n",
      "Epoch [338/2000], Evaluation Loss: 23.0374, Evaluation BLEU: 0.0000\n",
      "Epoch [339/2000], Evaluation Loss: 23.0383, Evaluation BLEU: 0.0000\n",
      "Epoch [340/2000], Evaluation Loss: 23.0392, Evaluation BLEU: 0.0000\n",
      "Epoch [341/2000], Evaluation Loss: 23.0401, Evaluation BLEU: 0.0000\n",
      "Epoch [342/2000], Evaluation Loss: 23.0410, Evaluation BLEU: 0.0000\n",
      "Epoch [343/2000], Evaluation Loss: 23.0419, Evaluation BLEU: 0.0000\n",
      "Epoch [344/2000], Evaluation Loss: 23.0428, Evaluation BLEU: 0.0000\n",
      "Epoch [345/2000], Evaluation Loss: 23.0436, Evaluation BLEU: 0.0000\n",
      "Epoch [346/2000], Evaluation Loss: 23.0445, Evaluation BLEU: 0.0000\n",
      "Epoch [347/2000], Evaluation Loss: 23.0454, Evaluation BLEU: 0.0000\n",
      "Epoch [348/2000], Evaluation Loss: 23.0463, Evaluation BLEU: 0.0000\n",
      "Epoch [349/2000], Evaluation Loss: 23.0471, Evaluation BLEU: 0.0000\n",
      "Epoch [350/2000], Evaluation Loss: 23.0480, Evaluation BLEU: 0.0000\n",
      "Epoch [351/2000], Evaluation Loss: 23.0489, Evaluation BLEU: 0.0000\n",
      "Epoch [352/2000], Evaluation Loss: 23.0497, Evaluation BLEU: 0.0000\n",
      "Epoch [353/2000], Evaluation Loss: 23.0506, Evaluation BLEU: 0.0000\n",
      "Epoch [354/2000], Evaluation Loss: 23.0514, Evaluation BLEU: 0.0000\n",
      "Epoch [355/2000], Evaluation Loss: 23.0523, Evaluation BLEU: 0.0000\n",
      "Epoch [356/2000], Evaluation Loss: 23.0531, Evaluation BLEU: 0.0000\n",
      "Epoch [357/2000], Evaluation Loss: 23.0540, Evaluation BLEU: 0.0000\n",
      "Epoch [358/2000], Evaluation Loss: 23.0548, Evaluation BLEU: 0.0000\n",
      "Epoch [359/2000], Evaluation Loss: 23.0557, Evaluation BLEU: 0.0000\n",
      "Epoch [360/2000], Evaluation Loss: 23.0565, Evaluation BLEU: 0.0000\n",
      "Epoch [361/2000], Evaluation Loss: 23.0573, Evaluation BLEU: 0.0000\n",
      "Epoch [362/2000], Evaluation Loss: 23.0582, Evaluation BLEU: 0.0000\n",
      "Epoch [363/2000], Evaluation Loss: 23.0590, Evaluation BLEU: 0.0000\n",
      "Epoch [364/2000], Evaluation Loss: 23.0598, Evaluation BLEU: 0.0000\n",
      "Epoch [365/2000], Evaluation Loss: 23.0607, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [366/2000], Evaluation Loss: 23.0615, Evaluation BLEU: 0.0000\n",
      "Epoch [367/2000], Evaluation Loss: 23.0624, Evaluation BLEU: 0.0000\n",
      "Epoch [368/2000], Evaluation Loss: 23.0632, Evaluation BLEU: 0.0000\n",
      "Epoch [369/2000], Evaluation Loss: 23.0640, Evaluation BLEU: 0.0000\n",
      "Epoch [370/2000], Evaluation Loss: 23.0648, Evaluation BLEU: 0.0000\n",
      "Epoch [371/2000], Evaluation Loss: 23.0657, Evaluation BLEU: 0.0000\n",
      "Epoch [372/2000], Evaluation Loss: 23.0665, Evaluation BLEU: 0.0000\n",
      "Epoch [373/2000], Evaluation Loss: 23.0673, Evaluation BLEU: 0.0000\n",
      "Epoch [374/2000], Evaluation Loss: 23.0681, Evaluation BLEU: 0.0000\n",
      "Epoch [375/2000], Evaluation Loss: 23.0689, Evaluation BLEU: 0.0000\n",
      "Epoch [376/2000], Evaluation Loss: 23.0697, Evaluation BLEU: 0.0000\n",
      "Epoch [377/2000], Evaluation Loss: 23.0705, Evaluation BLEU: 0.0000\n",
      "Epoch [378/2000], Evaluation Loss: 23.0713, Evaluation BLEU: 0.0000\n",
      "Epoch [379/2000], Evaluation Loss: 23.0721, Evaluation BLEU: 0.0000\n",
      "Epoch [380/2000], Evaluation Loss: 23.0729, Evaluation BLEU: 0.0000\n",
      "Epoch [381/2000], Evaluation Loss: 23.0737, Evaluation BLEU: 0.0000\n",
      "Epoch [382/2000], Evaluation Loss: 23.0744, Evaluation BLEU: 0.0000\n",
      "Epoch [383/2000], Evaluation Loss: 23.0752, Evaluation BLEU: 0.0000\n",
      "Epoch [384/2000], Evaluation Loss: 23.0760, Evaluation BLEU: 0.0000\n",
      "Epoch [385/2000], Evaluation Loss: 23.0767, Evaluation BLEU: 0.0000\n",
      "Epoch [386/2000], Evaluation Loss: 23.0775, Evaluation BLEU: 0.0000\n",
      "Epoch [387/2000], Evaluation Loss: 23.0783, Evaluation BLEU: 0.0000\n",
      "Epoch [388/2000], Evaluation Loss: 23.0790, Evaluation BLEU: 0.0000\n",
      "Epoch [389/2000], Evaluation Loss: 23.0798, Evaluation BLEU: 0.0000\n",
      "Epoch [390/2000], Evaluation Loss: 23.0805, Evaluation BLEU: 0.0000\n",
      "Epoch [391/2000], Evaluation Loss: 23.0812, Evaluation BLEU: 0.0000\n",
      "Epoch [392/2000], Evaluation Loss: 23.0820, Evaluation BLEU: 0.0000\n",
      "Epoch [393/2000], Evaluation Loss: 23.0828, Evaluation BLEU: 0.0000\n",
      "Epoch [394/2000], Evaluation Loss: 23.0835, Evaluation BLEU: 0.0000\n",
      "Epoch [395/2000], Evaluation Loss: 23.0842, Evaluation BLEU: 0.0000\n",
      "Epoch [396/2000], Evaluation Loss: 23.0850, Evaluation BLEU: 0.0000\n",
      "Epoch [397/2000], Evaluation Loss: 23.0857, Evaluation BLEU: 0.0000\n",
      "Epoch [398/2000], Evaluation Loss: 23.0864, Evaluation BLEU: 0.0000\n",
      "Epoch [399/2000], Evaluation Loss: 23.0872, Evaluation BLEU: 0.0000\n",
      "Epoch [400/2000], Evaluation Loss: 23.0879, Evaluation BLEU: 0.0000\n",
      "Epoch [401/2000], Evaluation Loss: 23.0886, Evaluation BLEU: 0.0000\n",
      "Epoch [402/2000], Evaluation Loss: 23.0893, Evaluation BLEU: 0.0000\n",
      "Epoch [403/2000], Evaluation Loss: 23.0901, Evaluation BLEU: 0.0000\n",
      "Epoch [404/2000], Evaluation Loss: 23.0908, Evaluation BLEU: 0.0000\n",
      "Epoch [405/2000], Evaluation Loss: 23.0916, Evaluation BLEU: 0.0000\n",
      "Epoch [406/2000], Evaluation Loss: 23.0923, Evaluation BLEU: 0.0000\n",
      "Epoch [407/2000], Evaluation Loss: 23.0930, Evaluation BLEU: 0.0000\n",
      "Epoch [408/2000], Evaluation Loss: 23.0938, Evaluation BLEU: 0.0000\n",
      "Epoch [409/2000], Evaluation Loss: 23.0945, Evaluation BLEU: 0.0000\n",
      "Epoch [410/2000], Evaluation Loss: 23.0952, Evaluation BLEU: 0.0000\n",
      "Epoch [411/2000], Evaluation Loss: 23.0960, Evaluation BLEU: 0.0000\n",
      "Epoch [412/2000], Evaluation Loss: 23.0967, Evaluation BLEU: 0.0000\n",
      "Epoch [413/2000], Evaluation Loss: 23.0975, Evaluation BLEU: 0.0000\n",
      "Epoch [414/2000], Evaluation Loss: 23.0982, Evaluation BLEU: 0.0000\n",
      "Epoch [415/2000], Evaluation Loss: 23.0989, Evaluation BLEU: 0.0000\n",
      "Epoch [416/2000], Evaluation Loss: 23.0997, Evaluation BLEU: 0.0000\n",
      "Epoch [417/2000], Evaluation Loss: 23.1004, Evaluation BLEU: 0.0000\n",
      "Epoch [418/2000], Evaluation Loss: 23.1012, Evaluation BLEU: 0.0000\n",
      "Epoch [419/2000], Evaluation Loss: 23.1019, Evaluation BLEU: 0.0000\n",
      "Epoch [420/2000], Evaluation Loss: 23.1026, Evaluation BLEU: 0.0000\n",
      "Epoch [421/2000], Evaluation Loss: 23.1034, Evaluation BLEU: 0.0000\n",
      "Epoch [422/2000], Evaluation Loss: 23.1041, Evaluation BLEU: 0.0000\n",
      "Epoch [423/2000], Evaluation Loss: 23.1049, Evaluation BLEU: 0.0000\n",
      "Epoch [424/2000], Evaluation Loss: 23.1056, Evaluation BLEU: 0.0000\n",
      "Epoch [425/2000], Evaluation Loss: 23.1063, Evaluation BLEU: 0.0000\n",
      "Epoch [426/2000], Evaluation Loss: 23.1070, Evaluation BLEU: 0.0000\n",
      "Epoch [427/2000], Evaluation Loss: 23.1077, Evaluation BLEU: 0.0000\n",
      "Epoch [428/2000], Evaluation Loss: 23.1084, Evaluation BLEU: 0.0000\n",
      "Epoch [429/2000], Evaluation Loss: 23.1091, Evaluation BLEU: 0.0000\n",
      "Epoch [430/2000], Evaluation Loss: 23.1098, Evaluation BLEU: 0.0000\n",
      "Epoch [431/2000], Evaluation Loss: 23.1105, Evaluation BLEU: 0.0000\n",
      "Epoch [432/2000], Evaluation Loss: 23.1112, Evaluation BLEU: 0.0000\n",
      "Epoch [433/2000], Evaluation Loss: 23.1119, Evaluation BLEU: 0.0000\n",
      "Epoch [434/2000], Evaluation Loss: 23.1126, Evaluation BLEU: 0.0000\n",
      "Epoch [435/2000], Evaluation Loss: 23.1133, Evaluation BLEU: 0.0000\n",
      "Epoch [436/2000], Evaluation Loss: 23.1140, Evaluation BLEU: 0.0000\n",
      "Epoch [437/2000], Evaluation Loss: 23.1147, Evaluation BLEU: 0.0000\n",
      "Epoch [438/2000], Evaluation Loss: 23.1154, Evaluation BLEU: 0.0000\n",
      "Epoch [439/2000], Evaluation Loss: 23.1161, Evaluation BLEU: 0.0000\n",
      "Epoch [440/2000], Evaluation Loss: 23.1168, Evaluation BLEU: 0.0000\n",
      "Epoch [441/2000], Evaluation Loss: 23.1175, Evaluation BLEU: 0.0000\n",
      "Epoch [442/2000], Evaluation Loss: 23.1182, Evaluation BLEU: 0.0000\n",
      "Epoch [443/2000], Evaluation Loss: 23.1189, Evaluation BLEU: 0.0000\n",
      "Epoch [444/2000], Evaluation Loss: 23.1195, Evaluation BLEU: 0.0000\n",
      "Epoch [445/2000], Evaluation Loss: 23.1202, Evaluation BLEU: 0.0000\n",
      "Epoch [446/2000], Evaluation Loss: 23.1209, Evaluation BLEU: 0.0000\n",
      "Epoch [447/2000], Evaluation Loss: 23.1216, Evaluation BLEU: 0.0000\n",
      "Epoch [448/2000], Evaluation Loss: 23.1223, Evaluation BLEU: 0.0000\n",
      "Epoch [449/2000], Evaluation Loss: 23.1229, Evaluation BLEU: 0.0000\n",
      "Epoch [450/2000], Evaluation Loss: 23.1236, Evaluation BLEU: 0.0000\n",
      "Epoch [451/2000], Evaluation Loss: 23.1242, Evaluation BLEU: 0.0000\n",
      "Epoch [452/2000], Evaluation Loss: 23.1249, Evaluation BLEU: 0.0000\n",
      "Epoch [453/2000], Evaluation Loss: 23.1256, Evaluation BLEU: 0.0000\n",
      "Epoch [454/2000], Evaluation Loss: 23.1262, Evaluation BLEU: 0.0000\n",
      "Epoch [455/2000], Evaluation Loss: 23.1269, Evaluation BLEU: 0.0000\n",
      "Epoch [456/2000], Evaluation Loss: 23.1275, Evaluation BLEU: 0.0000\n",
      "Epoch [457/2000], Evaluation Loss: 23.1281, Evaluation BLEU: 0.0000\n",
      "Epoch [458/2000], Evaluation Loss: 23.1288, Evaluation BLEU: 0.0000\n",
      "Epoch [459/2000], Evaluation Loss: 23.1294, Evaluation BLEU: 0.0000\n",
      "Epoch [460/2000], Evaluation Loss: 23.1300, Evaluation BLEU: 0.0000\n",
      "Epoch [461/2000], Evaluation Loss: 23.1306, Evaluation BLEU: 0.0000\n",
      "Epoch [462/2000], Evaluation Loss: 23.1313, Evaluation BLEU: 0.0000\n",
      "Epoch [463/2000], Evaluation Loss: 23.1319, Evaluation BLEU: 0.0000\n",
      "Epoch [464/2000], Evaluation Loss: 23.1325, Evaluation BLEU: 0.0000\n",
      "Epoch [465/2000], Evaluation Loss: 23.1331, Evaluation BLEU: 0.0000\n",
      "Epoch [466/2000], Evaluation Loss: 23.1337, Evaluation BLEU: 0.0000\n",
      "Epoch [467/2000], Evaluation Loss: 23.1343, Evaluation BLEU: 0.0000\n",
      "Epoch [468/2000], Evaluation Loss: 23.1349, Evaluation BLEU: 0.0000\n",
      "Epoch [469/2000], Evaluation Loss: 23.1355, Evaluation BLEU: 0.0000\n",
      "Epoch [470/2000], Evaluation Loss: 23.1361, Evaluation BLEU: 0.0000\n",
      "Epoch [471/2000], Evaluation Loss: 23.1367, Evaluation BLEU: 0.0000\n",
      "Epoch [472/2000], Evaluation Loss: 23.1373, Evaluation BLEU: 0.0000\n",
      "Epoch [473/2000], Evaluation Loss: 23.1379, Evaluation BLEU: 0.0000\n",
      "Epoch [474/2000], Evaluation Loss: 23.1385, Evaluation BLEU: 0.0000\n",
      "Epoch [475/2000], Evaluation Loss: 23.1391, Evaluation BLEU: 0.0000\n",
      "Epoch [476/2000], Evaluation Loss: 23.1397, Evaluation BLEU: 0.0000\n",
      "Epoch [477/2000], Evaluation Loss: 23.1403, Evaluation BLEU: 0.0000\n",
      "Epoch [478/2000], Evaluation Loss: 23.1410, Evaluation BLEU: 0.0000\n",
      "Epoch [479/2000], Evaluation Loss: 23.1416, Evaluation BLEU: 0.0000\n",
      "Epoch [480/2000], Evaluation Loss: 23.1422, Evaluation BLEU: 0.0000\n",
      "Epoch [481/2000], Evaluation Loss: 23.1428, Evaluation BLEU: 0.0000\n",
      "Epoch [482/2000], Evaluation Loss: 23.1434, Evaluation BLEU: 0.0000\n",
      "Epoch [483/2000], Evaluation Loss: 23.1440, Evaluation BLEU: 0.0000\n",
      "Epoch [484/2000], Evaluation Loss: 23.1446, Evaluation BLEU: 0.0000\n",
      "Epoch [485/2000], Evaluation Loss: 23.1452, Evaluation BLEU: 0.0000\n",
      "Epoch [486/2000], Evaluation Loss: 23.1459, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [487/2000], Evaluation Loss: 23.1465, Evaluation BLEU: 0.0000\n",
      "Epoch [488/2000], Evaluation Loss: 23.1471, Evaluation BLEU: 0.0000\n",
      "Epoch [489/2000], Evaluation Loss: 23.1477, Evaluation BLEU: 0.0000\n",
      "Epoch [490/2000], Evaluation Loss: 23.1483, Evaluation BLEU: 0.0000\n",
      "Epoch [491/2000], Evaluation Loss: 23.1490, Evaluation BLEU: 0.0000\n",
      "Epoch [492/2000], Evaluation Loss: 23.1496, Evaluation BLEU: 0.0000\n",
      "Epoch [493/2000], Evaluation Loss: 23.1502, Evaluation BLEU: 0.0000\n",
      "Epoch [494/2000], Evaluation Loss: 23.1508, Evaluation BLEU: 0.0000\n",
      "Epoch [495/2000], Evaluation Loss: 23.1514, Evaluation BLEU: 0.0000\n",
      "Epoch [496/2000], Evaluation Loss: 23.1520, Evaluation BLEU: 0.0000\n",
      "Epoch [497/2000], Evaluation Loss: 23.1526, Evaluation BLEU: 0.0000\n",
      "Epoch [498/2000], Evaluation Loss: 23.1532, Evaluation BLEU: 0.0000\n",
      "Epoch [499/2000], Evaluation Loss: 23.1537, Evaluation BLEU: 0.0000\n",
      "Epoch [500/2000], Evaluation Loss: 23.1543, Evaluation BLEU: 0.0000\n",
      "Epoch [501/2000], Evaluation Loss: 23.1549, Evaluation BLEU: 0.0000\n",
      "Epoch [502/2000], Evaluation Loss: 23.1554, Evaluation BLEU: 0.0000\n",
      "Epoch [503/2000], Evaluation Loss: 23.1560, Evaluation BLEU: 0.0000\n",
      "Epoch [504/2000], Evaluation Loss: 23.1566, Evaluation BLEU: 0.0000\n",
      "Epoch [505/2000], Evaluation Loss: 23.1571, Evaluation BLEU: 0.0000\n",
      "Epoch [506/2000], Evaluation Loss: 23.1576, Evaluation BLEU: 0.0000\n",
      "Epoch [507/2000], Evaluation Loss: 23.1581, Evaluation BLEU: 0.0000\n",
      "Epoch [508/2000], Evaluation Loss: 23.1587, Evaluation BLEU: 0.0000\n",
      "Epoch [509/2000], Evaluation Loss: 23.1592, Evaluation BLEU: 0.0000\n",
      "Epoch [510/2000], Evaluation Loss: 23.1597, Evaluation BLEU: 0.0000\n",
      "Epoch [511/2000], Evaluation Loss: 23.1602, Evaluation BLEU: 0.0000\n",
      "Epoch [512/2000], Evaluation Loss: 23.1607, Evaluation BLEU: 0.0000\n",
      "Epoch [513/2000], Evaluation Loss: 23.1611, Evaluation BLEU: 0.0000\n",
      "Epoch [514/2000], Evaluation Loss: 23.1616, Evaluation BLEU: 0.0000\n",
      "Epoch [515/2000], Evaluation Loss: 23.1621, Evaluation BLEU: 0.0000\n",
      "Epoch [516/2000], Evaluation Loss: 23.1626, Evaluation BLEU: 0.0000\n",
      "Epoch [517/2000], Evaluation Loss: 23.1631, Evaluation BLEU: 0.0000\n",
      "Epoch [518/2000], Evaluation Loss: 23.1636, Evaluation BLEU: 0.0000\n",
      "Epoch [519/2000], Evaluation Loss: 23.1640, Evaluation BLEU: 0.0000\n",
      "Epoch [520/2000], Evaluation Loss: 23.1645, Evaluation BLEU: 0.0000\n",
      "Epoch [521/2000], Evaluation Loss: 23.1650, Evaluation BLEU: 0.0000\n",
      "Epoch [522/2000], Evaluation Loss: 23.1655, Evaluation BLEU: 0.0000\n",
      "Epoch [523/2000], Evaluation Loss: 23.1660, Evaluation BLEU: 0.0000\n",
      "Epoch [524/2000], Evaluation Loss: 23.1665, Evaluation BLEU: 0.0000\n",
      "Epoch [525/2000], Evaluation Loss: 23.1670, Evaluation BLEU: 0.0000\n",
      "Epoch [526/2000], Evaluation Loss: 23.1675, Evaluation BLEU: 0.0000\n",
      "Epoch [527/2000], Evaluation Loss: 23.1680, Evaluation BLEU: 0.0000\n",
      "Epoch [528/2000], Evaluation Loss: 23.1685, Evaluation BLEU: 0.0000\n",
      "Epoch [529/2000], Evaluation Loss: 23.1690, Evaluation BLEU: 0.0000\n",
      "Epoch [530/2000], Evaluation Loss: 23.1695, Evaluation BLEU: 0.0000\n",
      "Epoch [531/2000], Evaluation Loss: 23.1700, Evaluation BLEU: 0.0000\n",
      "Epoch [532/2000], Evaluation Loss: 23.1705, Evaluation BLEU: 0.0000\n",
      "Epoch [533/2000], Evaluation Loss: 23.1710, Evaluation BLEU: 0.0000\n",
      "Epoch [534/2000], Evaluation Loss: 23.1714, Evaluation BLEU: 0.0000\n",
      "Epoch [535/2000], Evaluation Loss: 23.1719, Evaluation BLEU: 0.0000\n",
      "Epoch [536/2000], Evaluation Loss: 23.1724, Evaluation BLEU: 0.0000\n",
      "Epoch [537/2000], Evaluation Loss: 23.1729, Evaluation BLEU: 0.0000\n",
      "Epoch [538/2000], Evaluation Loss: 23.1733, Evaluation BLEU: 0.0000\n",
      "Epoch [539/2000], Evaluation Loss: 23.1738, Evaluation BLEU: 0.0000\n",
      "Epoch [540/2000], Evaluation Loss: 23.1743, Evaluation BLEU: 0.0000\n",
      "Epoch [541/2000], Evaluation Loss: 23.1748, Evaluation BLEU: 0.0000\n",
      "Epoch [542/2000], Evaluation Loss: 23.1752, Evaluation BLEU: 0.0000\n",
      "Epoch [543/2000], Evaluation Loss: 23.1757, Evaluation BLEU: 0.0000\n",
      "Epoch [544/2000], Evaluation Loss: 23.1762, Evaluation BLEU: 0.0000\n",
      "Epoch [545/2000], Evaluation Loss: 23.1767, Evaluation BLEU: 0.0000\n",
      "Epoch [546/2000], Evaluation Loss: 23.1772, Evaluation BLEU: 0.0000\n",
      "Epoch [547/2000], Evaluation Loss: 23.1777, Evaluation BLEU: 0.0000\n",
      "Epoch [548/2000], Evaluation Loss: 23.1782, Evaluation BLEU: 0.0000\n",
      "Epoch [549/2000], Evaluation Loss: 23.1787, Evaluation BLEU: 0.0000\n",
      "Epoch [550/2000], Evaluation Loss: 23.1792, Evaluation BLEU: 0.0000\n",
      "Epoch [551/2000], Evaluation Loss: 23.1798, Evaluation BLEU: 0.0000\n",
      "Epoch [552/2000], Evaluation Loss: 23.1803, Evaluation BLEU: 0.0000\n",
      "Epoch [553/2000], Evaluation Loss: 23.1808, Evaluation BLEU: 0.0000\n",
      "Epoch [554/2000], Evaluation Loss: 23.1813, Evaluation BLEU: 0.0000\n",
      "Epoch [555/2000], Evaluation Loss: 23.1819, Evaluation BLEU: 0.0000\n",
      "Epoch [556/2000], Evaluation Loss: 23.1824, Evaluation BLEU: 0.0000\n",
      "Epoch [557/2000], Evaluation Loss: 23.1829, Evaluation BLEU: 0.0000\n",
      "Epoch [558/2000], Evaluation Loss: 23.1834, Evaluation BLEU: 0.0000\n",
      "Epoch [559/2000], Evaluation Loss: 23.1839, Evaluation BLEU: 0.0000\n",
      "Epoch [560/2000], Evaluation Loss: 23.1845, Evaluation BLEU: 0.0000\n",
      "Epoch [561/2000], Evaluation Loss: 23.1850, Evaluation BLEU: 0.0000\n",
      "Epoch [562/2000], Evaluation Loss: 23.1854, Evaluation BLEU: 0.0000\n",
      "Epoch [563/2000], Evaluation Loss: 23.1859, Evaluation BLEU: 0.0000\n",
      "Epoch [564/2000], Evaluation Loss: 23.1864, Evaluation BLEU: 0.0000\n",
      "Epoch [565/2000], Evaluation Loss: 23.1869, Evaluation BLEU: 0.0000\n",
      "Epoch [566/2000], Evaluation Loss: 23.1873, Evaluation BLEU: 0.0000\n",
      "Epoch [567/2000], Evaluation Loss: 23.1878, Evaluation BLEU: 0.0000\n",
      "Epoch [568/2000], Evaluation Loss: 23.1883, Evaluation BLEU: 0.0000\n",
      "Epoch [569/2000], Evaluation Loss: 23.1887, Evaluation BLEU: 0.0000\n",
      "Epoch [570/2000], Evaluation Loss: 23.1892, Evaluation BLEU: 0.0000\n",
      "Epoch [571/2000], Evaluation Loss: 23.1896, Evaluation BLEU: 0.0000\n",
      "Epoch [572/2000], Evaluation Loss: 23.1901, Evaluation BLEU: 0.0000\n",
      "Epoch [573/2000], Evaluation Loss: 23.1905, Evaluation BLEU: 0.0000\n",
      "Epoch [574/2000], Evaluation Loss: 23.1910, Evaluation BLEU: 0.0000\n",
      "Epoch [575/2000], Evaluation Loss: 23.1915, Evaluation BLEU: 0.0000\n",
      "Epoch [576/2000], Evaluation Loss: 23.1919, Evaluation BLEU: 0.0000\n",
      "Epoch [577/2000], Evaluation Loss: 23.1924, Evaluation BLEU: 0.0000\n",
      "Epoch [578/2000], Evaluation Loss: 23.1928, Evaluation BLEU: 0.0000\n",
      "Epoch [579/2000], Evaluation Loss: 23.1933, Evaluation BLEU: 0.0000\n",
      "Epoch [580/2000], Evaluation Loss: 23.1937, Evaluation BLEU: 0.0000\n",
      "Epoch [581/2000], Evaluation Loss: 23.1942, Evaluation BLEU: 0.0000\n",
      "Epoch [582/2000], Evaluation Loss: 23.1946, Evaluation BLEU: 0.0000\n",
      "Epoch [583/2000], Evaluation Loss: 23.1951, Evaluation BLEU: 0.0000\n",
      "Epoch [584/2000], Evaluation Loss: 23.1955, Evaluation BLEU: 0.0000\n",
      "Epoch [585/2000], Evaluation Loss: 23.1960, Evaluation BLEU: 0.0000\n",
      "Epoch [586/2000], Evaluation Loss: 23.1964, Evaluation BLEU: 0.0000\n",
      "Epoch [587/2000], Evaluation Loss: 23.1969, Evaluation BLEU: 0.0000\n",
      "Epoch [588/2000], Evaluation Loss: 23.1973, Evaluation BLEU: 0.0000\n",
      "Epoch [589/2000], Evaluation Loss: 23.1977, Evaluation BLEU: 0.0000\n",
      "Epoch [590/2000], Evaluation Loss: 23.1982, Evaluation BLEU: 0.0000\n",
      "Epoch [591/2000], Evaluation Loss: 23.1987, Evaluation BLEU: 0.0000\n",
      "Epoch [592/2000], Evaluation Loss: 23.1991, Evaluation BLEU: 0.0000\n",
      "Epoch [593/2000], Evaluation Loss: 23.1996, Evaluation BLEU: 0.0000\n",
      "Epoch [594/2000], Evaluation Loss: 23.2000, Evaluation BLEU: 0.0000\n",
      "Epoch [595/2000], Evaluation Loss: 23.2005, Evaluation BLEU: 0.0000\n",
      "Epoch [596/2000], Evaluation Loss: 23.2009, Evaluation BLEU: 0.0000\n",
      "Epoch [597/2000], Evaluation Loss: 23.2014, Evaluation BLEU: 0.0000\n",
      "Epoch [598/2000], Evaluation Loss: 23.2018, Evaluation BLEU: 0.0000\n",
      "Epoch [599/2000], Evaluation Loss: 23.2023, Evaluation BLEU: 0.0000\n",
      "Epoch [600/2000], Evaluation Loss: 23.2027, Evaluation BLEU: 0.0000\n",
      "Epoch [601/2000], Evaluation Loss: 23.2032, Evaluation BLEU: 0.0000\n",
      "Epoch [602/2000], Evaluation Loss: 23.2036, Evaluation BLEU: 0.0000\n",
      "Epoch [603/2000], Evaluation Loss: 23.2041, Evaluation BLEU: 0.0000\n",
      "Epoch [604/2000], Evaluation Loss: 23.2046, Evaluation BLEU: 0.0000\n",
      "Epoch [605/2000], Evaluation Loss: 23.2050, Evaluation BLEU: 0.0000\n",
      "Epoch [606/2000], Evaluation Loss: 23.2055, Evaluation BLEU: 0.0000\n",
      "Epoch [607/2000], Evaluation Loss: 23.2059, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [608/2000], Evaluation Loss: 23.2064, Evaluation BLEU: 0.0000\n",
      "Epoch [609/2000], Evaluation Loss: 23.2068, Evaluation BLEU: 0.0000\n",
      "Epoch [610/2000], Evaluation Loss: 23.2073, Evaluation BLEU: 0.0000\n",
      "Epoch [611/2000], Evaluation Loss: 23.2077, Evaluation BLEU: 0.0000\n",
      "Epoch [612/2000], Evaluation Loss: 23.2082, Evaluation BLEU: 0.0000\n",
      "Epoch [613/2000], Evaluation Loss: 23.2086, Evaluation BLEU: 0.0000\n",
      "Epoch [614/2000], Evaluation Loss: 23.2091, Evaluation BLEU: 0.0000\n",
      "Epoch [615/2000], Evaluation Loss: 23.2095, Evaluation BLEU: 0.0000\n",
      "Epoch [616/2000], Evaluation Loss: 23.2100, Evaluation BLEU: 0.0000\n",
      "Epoch [617/2000], Evaluation Loss: 23.2104, Evaluation BLEU: 0.0000\n",
      "Epoch [618/2000], Evaluation Loss: 23.2109, Evaluation BLEU: 0.0000\n",
      "Epoch [619/2000], Evaluation Loss: 23.2113, Evaluation BLEU: 0.0000\n",
      "Epoch [620/2000], Evaluation Loss: 23.2118, Evaluation BLEU: 0.0000\n",
      "Epoch [621/2000], Evaluation Loss: 23.2122, Evaluation BLEU: 0.0000\n",
      "Epoch [622/2000], Evaluation Loss: 23.2126, Evaluation BLEU: 0.0000\n",
      "Epoch [623/2000], Evaluation Loss: 23.2130, Evaluation BLEU: 0.0000\n",
      "Epoch [624/2000], Evaluation Loss: 23.2134, Evaluation BLEU: 0.0000\n",
      "Epoch [625/2000], Evaluation Loss: 23.2138, Evaluation BLEU: 0.0000\n",
      "Epoch [626/2000], Evaluation Loss: 23.2142, Evaluation BLEU: 0.0000\n",
      "Epoch [627/2000], Evaluation Loss: 23.2146, Evaluation BLEU: 0.0000\n",
      "Epoch [628/2000], Evaluation Loss: 23.2150, Evaluation BLEU: 0.0000\n",
      "Epoch [629/2000], Evaluation Loss: 23.2154, Evaluation BLEU: 0.0000\n",
      "Epoch [630/2000], Evaluation Loss: 23.2158, Evaluation BLEU: 0.0000\n",
      "Epoch [631/2000], Evaluation Loss: 23.2162, Evaluation BLEU: 0.0000\n",
      "Epoch [632/2000], Evaluation Loss: 23.2165, Evaluation BLEU: 0.0000\n",
      "Epoch [633/2000], Evaluation Loss: 23.2169, Evaluation BLEU: 0.0000\n",
      "Epoch [634/2000], Evaluation Loss: 23.2173, Evaluation BLEU: 0.0000\n",
      "Epoch [635/2000], Evaluation Loss: 23.2177, Evaluation BLEU: 0.0000\n",
      "Epoch [636/2000], Evaluation Loss: 23.2180, Evaluation BLEU: 0.0000\n",
      "Epoch [637/2000], Evaluation Loss: 23.2184, Evaluation BLEU: 0.0000\n",
      "Epoch [638/2000], Evaluation Loss: 23.2188, Evaluation BLEU: 0.0000\n",
      "Epoch [639/2000], Evaluation Loss: 23.2192, Evaluation BLEU: 0.0000\n",
      "Epoch [640/2000], Evaluation Loss: 23.2196, Evaluation BLEU: 0.0000\n",
      "Epoch [641/2000], Evaluation Loss: 23.2200, Evaluation BLEU: 0.0000\n",
      "Epoch [642/2000], Evaluation Loss: 23.2204, Evaluation BLEU: 0.0000\n",
      "Epoch [643/2000], Evaluation Loss: 23.2208, Evaluation BLEU: 0.0000\n",
      "Epoch [644/2000], Evaluation Loss: 23.2212, Evaluation BLEU: 0.0000\n",
      "Epoch [645/2000], Evaluation Loss: 23.2217, Evaluation BLEU: 0.0000\n",
      "Epoch [646/2000], Evaluation Loss: 23.2221, Evaluation BLEU: 0.0000\n",
      "Epoch [647/2000], Evaluation Loss: 23.2225, Evaluation BLEU: 0.0000\n",
      "Epoch [648/2000], Evaluation Loss: 23.2230, Evaluation BLEU: 0.0000\n",
      "Epoch [649/2000], Evaluation Loss: 23.2234, Evaluation BLEU: 0.0000\n",
      "Epoch [650/2000], Evaluation Loss: 23.2238, Evaluation BLEU: 0.0000\n",
      "Epoch [651/2000], Evaluation Loss: 23.2242, Evaluation BLEU: 0.0000\n",
      "Epoch [652/2000], Evaluation Loss: 23.2246, Evaluation BLEU: 0.0000\n",
      "Epoch [653/2000], Evaluation Loss: 23.2251, Evaluation BLEU: 0.0000\n",
      "Epoch [654/2000], Evaluation Loss: 23.2255, Evaluation BLEU: 0.0000\n",
      "Epoch [655/2000], Evaluation Loss: 23.2259, Evaluation BLEU: 0.0000\n",
      "Epoch [656/2000], Evaluation Loss: 23.2264, Evaluation BLEU: 0.0000\n",
      "Epoch [657/2000], Evaluation Loss: 23.2268, Evaluation BLEU: 0.0000\n",
      "Epoch [658/2000], Evaluation Loss: 23.2273, Evaluation BLEU: 0.0000\n",
      "Epoch [659/2000], Evaluation Loss: 23.2278, Evaluation BLEU: 0.0000\n",
      "Epoch [660/2000], Evaluation Loss: 23.2282, Evaluation BLEU: 0.0000\n",
      "Epoch [661/2000], Evaluation Loss: 23.2286, Evaluation BLEU: 0.0000\n",
      "Epoch [662/2000], Evaluation Loss: 23.2291, Evaluation BLEU: 0.0000\n",
      "Epoch [663/2000], Evaluation Loss: 23.2295, Evaluation BLEU: 0.0000\n",
      "Epoch [664/2000], Evaluation Loss: 23.2300, Evaluation BLEU: 0.0000\n",
      "Epoch [665/2000], Evaluation Loss: 23.2304, Evaluation BLEU: 0.0000\n",
      "Epoch [666/2000], Evaluation Loss: 23.2309, Evaluation BLEU: 0.0000\n",
      "Epoch [667/2000], Evaluation Loss: 23.2313, Evaluation BLEU: 0.0000\n",
      "Epoch [668/2000], Evaluation Loss: 23.2317, Evaluation BLEU: 0.0000\n",
      "Epoch [669/2000], Evaluation Loss: 23.2322, Evaluation BLEU: 0.0000\n",
      "Epoch [670/2000], Evaluation Loss: 23.2326, Evaluation BLEU: 0.0000\n",
      "Epoch [671/2000], Evaluation Loss: 23.2330, Evaluation BLEU: 0.0000\n",
      "Epoch [672/2000], Evaluation Loss: 23.2334, Evaluation BLEU: 0.0000\n",
      "Epoch [673/2000], Evaluation Loss: 23.2339, Evaluation BLEU: 0.0000\n",
      "Epoch [674/2000], Evaluation Loss: 23.2343, Evaluation BLEU: 0.0000\n",
      "Epoch [675/2000], Evaluation Loss: 23.2348, Evaluation BLEU: 0.0000\n",
      "Epoch [676/2000], Evaluation Loss: 23.2352, Evaluation BLEU: 0.0000\n",
      "Epoch [677/2000], Evaluation Loss: 23.2356, Evaluation BLEU: 0.0000\n",
      "Epoch [678/2000], Evaluation Loss: 23.2361, Evaluation BLEU: 0.0000\n",
      "Epoch [679/2000], Evaluation Loss: 23.2365, Evaluation BLEU: 0.0000\n",
      "Epoch [680/2000], Evaluation Loss: 23.2369, Evaluation BLEU: 0.0000\n",
      "Epoch [681/2000], Evaluation Loss: 23.2374, Evaluation BLEU: 0.0000\n",
      "Epoch [682/2000], Evaluation Loss: 23.2378, Evaluation BLEU: 0.0000\n",
      "Epoch [683/2000], Evaluation Loss: 23.2383, Evaluation BLEU: 0.0000\n",
      "Epoch [684/2000], Evaluation Loss: 23.2388, Evaluation BLEU: 0.0000\n",
      "Epoch [685/2000], Evaluation Loss: 23.2392, Evaluation BLEU: 0.0000\n",
      "Epoch [686/2000], Evaluation Loss: 23.2397, Evaluation BLEU: 0.0000\n",
      "Epoch [687/2000], Evaluation Loss: 23.2401, Evaluation BLEU: 0.0000\n",
      "Epoch [688/2000], Evaluation Loss: 23.2407, Evaluation BLEU: 0.0000\n",
      "Epoch [689/2000], Evaluation Loss: 23.2412, Evaluation BLEU: 0.0000\n",
      "Epoch [690/2000], Evaluation Loss: 23.2417, Evaluation BLEU: 0.0000\n",
      "Epoch [691/2000], Evaluation Loss: 23.2422, Evaluation BLEU: 0.0000\n",
      "Epoch [692/2000], Evaluation Loss: 23.2427, Evaluation BLEU: 0.0000\n",
      "Epoch [693/2000], Evaluation Loss: 23.2432, Evaluation BLEU: 0.0000\n",
      "Epoch [694/2000], Evaluation Loss: 23.2437, Evaluation BLEU: 0.0000\n",
      "Epoch [695/2000], Evaluation Loss: 23.2442, Evaluation BLEU: 0.0000\n",
      "Epoch [696/2000], Evaluation Loss: 23.2447, Evaluation BLEU: 0.0000\n",
      "Epoch [697/2000], Evaluation Loss: 23.2452, Evaluation BLEU: 0.0000\n",
      "Epoch [698/2000], Evaluation Loss: 23.2458, Evaluation BLEU: 0.0000\n",
      "Epoch [699/2000], Evaluation Loss: 23.2463, Evaluation BLEU: 0.0000\n",
      "Epoch [700/2000], Evaluation Loss: 23.2468, Evaluation BLEU: 0.0000\n",
      "Epoch [701/2000], Evaluation Loss: 23.2473, Evaluation BLEU: 0.0000\n",
      "Epoch [702/2000], Evaluation Loss: 23.2478, Evaluation BLEU: 0.0000\n",
      "Epoch [703/2000], Evaluation Loss: 23.2483, Evaluation BLEU: 0.0000\n",
      "Epoch [704/2000], Evaluation Loss: 23.2489, Evaluation BLEU: 0.0000\n",
      "Epoch [705/2000], Evaluation Loss: 23.2494, Evaluation BLEU: 0.0000\n",
      "Epoch [706/2000], Evaluation Loss: 23.2499, Evaluation BLEU: 0.0000\n",
      "Epoch [707/2000], Evaluation Loss: 23.2504, Evaluation BLEU: 0.0000\n",
      "Epoch [708/2000], Evaluation Loss: 23.2509, Evaluation BLEU: 0.0000\n",
      "Epoch [709/2000], Evaluation Loss: 23.2514, Evaluation BLEU: 0.0000\n",
      "Epoch [710/2000], Evaluation Loss: 23.2518, Evaluation BLEU: 0.0000\n",
      "Epoch [711/2000], Evaluation Loss: 23.2523, Evaluation BLEU: 0.0000\n",
      "Epoch [712/2000], Evaluation Loss: 23.2527, Evaluation BLEU: 0.0000\n",
      "Epoch [713/2000], Evaluation Loss: 23.2532, Evaluation BLEU: 0.0000\n",
      "Epoch [714/2000], Evaluation Loss: 23.2536, Evaluation BLEU: 0.0000\n",
      "Epoch [715/2000], Evaluation Loss: 23.2541, Evaluation BLEU: 0.0000\n",
      "Epoch [716/2000], Evaluation Loss: 23.2546, Evaluation BLEU: 0.0000\n",
      "Epoch [717/2000], Evaluation Loss: 23.2550, Evaluation BLEU: 0.0000\n",
      "Epoch [718/2000], Evaluation Loss: 23.2555, Evaluation BLEU: 0.0000\n",
      "Epoch [719/2000], Evaluation Loss: 23.2559, Evaluation BLEU: 0.0000\n",
      "Epoch [720/2000], Evaluation Loss: 23.2563, Evaluation BLEU: 0.0000\n",
      "Epoch [721/2000], Evaluation Loss: 23.2568, Evaluation BLEU: 0.0000\n",
      "Epoch [722/2000], Evaluation Loss: 23.2572, Evaluation BLEU: 0.0000\n",
      "Epoch [723/2000], Evaluation Loss: 23.2575, Evaluation BLEU: 0.0000\n",
      "Epoch [724/2000], Evaluation Loss: 23.2579, Evaluation BLEU: 0.0000\n",
      "Epoch [725/2000], Evaluation Loss: 23.2583, Evaluation BLEU: 0.0000\n",
      "Epoch [726/2000], Evaluation Loss: 23.2586, Evaluation BLEU: 0.0000\n",
      "Epoch [727/2000], Evaluation Loss: 23.2590, Evaluation BLEU: 0.0000\n",
      "Epoch [728/2000], Evaluation Loss: 23.2593, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [729/2000], Evaluation Loss: 23.2597, Evaluation BLEU: 0.0000\n",
      "Epoch [730/2000], Evaluation Loss: 23.2600, Evaluation BLEU: 0.0000\n",
      "Epoch [731/2000], Evaluation Loss: 23.2603, Evaluation BLEU: 0.0000\n",
      "Epoch [732/2000], Evaluation Loss: 23.2607, Evaluation BLEU: 0.0000\n",
      "Epoch [733/2000], Evaluation Loss: 23.2610, Evaluation BLEU: 0.0000\n",
      "Epoch [734/2000], Evaluation Loss: 23.2613, Evaluation BLEU: 0.0000\n",
      "Epoch [735/2000], Evaluation Loss: 23.2616, Evaluation BLEU: 0.0000\n",
      "Epoch [736/2000], Evaluation Loss: 23.2620, Evaluation BLEU: 0.0000\n",
      "Epoch [737/2000], Evaluation Loss: 23.2623, Evaluation BLEU: 0.0000\n",
      "Epoch [738/2000], Evaluation Loss: 23.2627, Evaluation BLEU: 0.0000\n",
      "Epoch [739/2000], Evaluation Loss: 23.2630, Evaluation BLEU: 0.0000\n",
      "Epoch [740/2000], Evaluation Loss: 23.2633, Evaluation BLEU: 0.0000\n",
      "Epoch [741/2000], Evaluation Loss: 23.2637, Evaluation BLEU: 0.0000\n",
      "Epoch [742/2000], Evaluation Loss: 23.2640, Evaluation BLEU: 0.0000\n",
      "Epoch [743/2000], Evaluation Loss: 23.2644, Evaluation BLEU: 0.0000\n",
      "Epoch [744/2000], Evaluation Loss: 23.2648, Evaluation BLEU: 0.0000\n",
      "Epoch [745/2000], Evaluation Loss: 23.2651, Evaluation BLEU: 0.0000\n",
      "Epoch [746/2000], Evaluation Loss: 23.2654, Evaluation BLEU: 0.0000\n",
      "Epoch [747/2000], Evaluation Loss: 23.2657, Evaluation BLEU: 0.0000\n",
      "Epoch [748/2000], Evaluation Loss: 23.2660, Evaluation BLEU: 0.0000\n",
      "Epoch [749/2000], Evaluation Loss: 23.2664, Evaluation BLEU: 0.0000\n",
      "Epoch [750/2000], Evaluation Loss: 23.2667, Evaluation BLEU: 0.0000\n",
      "Epoch [751/2000], Evaluation Loss: 23.2670, Evaluation BLEU: 0.0000\n",
      "Epoch [752/2000], Evaluation Loss: 23.2673, Evaluation BLEU: 0.0000\n",
      "Epoch [753/2000], Evaluation Loss: 23.2677, Evaluation BLEU: 0.0000\n",
      "Epoch [754/2000], Evaluation Loss: 23.2680, Evaluation BLEU: 0.0000\n",
      "Epoch [755/2000], Evaluation Loss: 23.2684, Evaluation BLEU: 0.0000\n",
      "Epoch [756/2000], Evaluation Loss: 23.2688, Evaluation BLEU: 0.0000\n",
      "Epoch [757/2000], Evaluation Loss: 23.2693, Evaluation BLEU: 0.0000\n",
      "Epoch [758/2000], Evaluation Loss: 23.2697, Evaluation BLEU: 0.0000\n",
      "Epoch [759/2000], Evaluation Loss: 23.2701, Evaluation BLEU: 0.0000\n",
      "Epoch [760/2000], Evaluation Loss: 23.2705, Evaluation BLEU: 0.0000\n",
      "Epoch [761/2000], Evaluation Loss: 23.2709, Evaluation BLEU: 0.0000\n",
      "Epoch [762/2000], Evaluation Loss: 23.2714, Evaluation BLEU: 0.0000\n",
      "Epoch [763/2000], Evaluation Loss: 23.2719, Evaluation BLEU: 0.0000\n",
      "Epoch [764/2000], Evaluation Loss: 23.2723, Evaluation BLEU: 0.0000\n",
      "Epoch [765/2000], Evaluation Loss: 23.2728, Evaluation BLEU: 0.0000\n",
      "Epoch [766/2000], Evaluation Loss: 23.2733, Evaluation BLEU: 0.0000\n",
      "Epoch [767/2000], Evaluation Loss: 23.2738, Evaluation BLEU: 0.0000\n",
      "Epoch [768/2000], Evaluation Loss: 23.2743, Evaluation BLEU: 0.0000\n",
      "Epoch [769/2000], Evaluation Loss: 23.2748, Evaluation BLEU: 0.0000\n",
      "Epoch [770/2000], Evaluation Loss: 23.2753, Evaluation BLEU: 0.0000\n",
      "Epoch [771/2000], Evaluation Loss: 23.2759, Evaluation BLEU: 0.0000\n",
      "Epoch [772/2000], Evaluation Loss: 23.2764, Evaluation BLEU: 0.0000\n",
      "Epoch [773/2000], Evaluation Loss: 23.2769, Evaluation BLEU: 0.0000\n",
      "Epoch [774/2000], Evaluation Loss: 23.2774, Evaluation BLEU: 0.0000\n",
      "Epoch [775/2000], Evaluation Loss: 23.2779, Evaluation BLEU: 0.0000\n",
      "Epoch [776/2000], Evaluation Loss: 23.2784, Evaluation BLEU: 0.0000\n",
      "Epoch [777/2000], Evaluation Loss: 23.2789, Evaluation BLEU: 0.0000\n",
      "Epoch [778/2000], Evaluation Loss: 23.2794, Evaluation BLEU: 0.0000\n",
      "Epoch [779/2000], Evaluation Loss: 23.2799, Evaluation BLEU: 0.0000\n",
      "Epoch [780/2000], Evaluation Loss: 23.2804, Evaluation BLEU: 0.0000\n",
      "Epoch [781/2000], Evaluation Loss: 23.2809, Evaluation BLEU: 0.0000\n",
      "Epoch [782/2000], Evaluation Loss: 23.2813, Evaluation BLEU: 0.0000\n",
      "Epoch [783/2000], Evaluation Loss: 23.2818, Evaluation BLEU: 0.0000\n",
      "Epoch [784/2000], Evaluation Loss: 23.2822, Evaluation BLEU: 0.0000\n",
      "Epoch [785/2000], Evaluation Loss: 23.2827, Evaluation BLEU: 0.0000\n",
      "Epoch [786/2000], Evaluation Loss: 23.2831, Evaluation BLEU: 0.0000\n",
      "Epoch [787/2000], Evaluation Loss: 23.2835, Evaluation BLEU: 0.0000\n",
      "Epoch [788/2000], Evaluation Loss: 23.2839, Evaluation BLEU: 0.0000\n",
      "Epoch [789/2000], Evaluation Loss: 23.2843, Evaluation BLEU: 0.0000\n",
      "Epoch [790/2000], Evaluation Loss: 23.2847, Evaluation BLEU: 0.0000\n",
      "Epoch [791/2000], Evaluation Loss: 23.2850, Evaluation BLEU: 0.0000\n",
      "Epoch [792/2000], Evaluation Loss: 23.2854, Evaluation BLEU: 0.0000\n",
      "Epoch [793/2000], Evaluation Loss: 23.2858, Evaluation BLEU: 0.0000\n",
      "Epoch [794/2000], Evaluation Loss: 23.2862, Evaluation BLEU: 0.0000\n",
      "Epoch [795/2000], Evaluation Loss: 23.2865, Evaluation BLEU: 0.0000\n",
      "Epoch [796/2000], Evaluation Loss: 23.2869, Evaluation BLEU: 0.0000\n",
      "Epoch [797/2000], Evaluation Loss: 23.2873, Evaluation BLEU: 0.0000\n",
      "Epoch [798/2000], Evaluation Loss: 23.2876, Evaluation BLEU: 0.0000\n",
      "Epoch [799/2000], Evaluation Loss: 23.2879, Evaluation BLEU: 0.0000\n",
      "Epoch [800/2000], Evaluation Loss: 23.2882, Evaluation BLEU: 0.0000\n",
      "Epoch [801/2000], Evaluation Loss: 23.2885, Evaluation BLEU: 0.0000\n",
      "Epoch [802/2000], Evaluation Loss: 23.2888, Evaluation BLEU: 0.0000\n",
      "Epoch [803/2000], Evaluation Loss: 23.2891, Evaluation BLEU: 0.0000\n",
      "Epoch [804/2000], Evaluation Loss: 23.2894, Evaluation BLEU: 0.0000\n",
      "Epoch [805/2000], Evaluation Loss: 23.2897, Evaluation BLEU: 0.0000\n",
      "Epoch [806/2000], Evaluation Loss: 23.2900, Evaluation BLEU: 0.0000\n",
      "Epoch [807/2000], Evaluation Loss: 23.2902, Evaluation BLEU: 0.0000\n",
      "Epoch [808/2000], Evaluation Loss: 23.2905, Evaluation BLEU: 0.0000\n",
      "Epoch [809/2000], Evaluation Loss: 23.2908, Evaluation BLEU: 0.0000\n",
      "Epoch [810/2000], Evaluation Loss: 23.2911, Evaluation BLEU: 0.0000\n",
      "Epoch [811/2000], Evaluation Loss: 23.2914, Evaluation BLEU: 0.0000\n",
      "Epoch [812/2000], Evaluation Loss: 23.2917, Evaluation BLEU: 0.0000\n",
      "Epoch [813/2000], Evaluation Loss: 23.2920, Evaluation BLEU: 0.0000\n",
      "Epoch [814/2000], Evaluation Loss: 23.2922, Evaluation BLEU: 0.0000\n",
      "Epoch [815/2000], Evaluation Loss: 23.2925, Evaluation BLEU: 0.0000\n",
      "Epoch [816/2000], Evaluation Loss: 23.2927, Evaluation BLEU: 0.0000\n",
      "Epoch [817/2000], Evaluation Loss: 23.2930, Evaluation BLEU: 0.0000\n",
      "Epoch [818/2000], Evaluation Loss: 23.2932, Evaluation BLEU: 0.0000\n",
      "Epoch [819/2000], Evaluation Loss: 23.2935, Evaluation BLEU: 0.0000\n",
      "Epoch [820/2000], Evaluation Loss: 23.2938, Evaluation BLEU: 0.0000\n",
      "Epoch [821/2000], Evaluation Loss: 23.2940, Evaluation BLEU: 0.0000\n",
      "Epoch [822/2000], Evaluation Loss: 23.2943, Evaluation BLEU: 0.0000\n",
      "Epoch [823/2000], Evaluation Loss: 23.2946, Evaluation BLEU: 0.0000\n",
      "Epoch [824/2000], Evaluation Loss: 23.2948, Evaluation BLEU: 0.0000\n",
      "Epoch [825/2000], Evaluation Loss: 23.2951, Evaluation BLEU: 0.0000\n",
      "Epoch [826/2000], Evaluation Loss: 23.2953, Evaluation BLEU: 0.0000\n",
      "Epoch [827/2000], Evaluation Loss: 23.2955, Evaluation BLEU: 0.0000\n",
      "Epoch [828/2000], Evaluation Loss: 23.2956, Evaluation BLEU: 0.0000\n",
      "Epoch [829/2000], Evaluation Loss: 23.2958, Evaluation BLEU: 0.0000\n",
      "Epoch [830/2000], Evaluation Loss: 23.2960, Evaluation BLEU: 0.0000\n",
      "Epoch [831/2000], Evaluation Loss: 23.2962, Evaluation BLEU: 0.0000\n",
      "Epoch [832/2000], Evaluation Loss: 23.2964, Evaluation BLEU: 0.0000\n",
      "Epoch [833/2000], Evaluation Loss: 23.2966, Evaluation BLEU: 0.0000\n",
      "Epoch [834/2000], Evaluation Loss: 23.2967, Evaluation BLEU: 0.0000\n",
      "Epoch [835/2000], Evaluation Loss: 23.2969, Evaluation BLEU: 0.0000\n",
      "Epoch [836/2000], Evaluation Loss: 23.2971, Evaluation BLEU: 0.0000\n",
      "Epoch [837/2000], Evaluation Loss: 23.2972, Evaluation BLEU: 0.0000\n",
      "Epoch [838/2000], Evaluation Loss: 23.2974, Evaluation BLEU: 0.0000\n",
      "Epoch [839/2000], Evaluation Loss: 23.2975, Evaluation BLEU: 0.0000\n",
      "Epoch [840/2000], Evaluation Loss: 23.2977, Evaluation BLEU: 0.0000\n",
      "Epoch [841/2000], Evaluation Loss: 23.2979, Evaluation BLEU: 0.0000\n",
      "Epoch [842/2000], Evaluation Loss: 23.2981, Evaluation BLEU: 0.0000\n",
      "Epoch [843/2000], Evaluation Loss: 23.2982, Evaluation BLEU: 0.0000\n",
      "Epoch [844/2000], Evaluation Loss: 23.2984, Evaluation BLEU: 0.0000\n",
      "Epoch [845/2000], Evaluation Loss: 23.2986, Evaluation BLEU: 0.0000\n",
      "Epoch [846/2000], Evaluation Loss: 23.2988, Evaluation BLEU: 0.0000\n",
      "Epoch [847/2000], Evaluation Loss: 23.2990, Evaluation BLEU: 0.0000\n",
      "Epoch [848/2000], Evaluation Loss: 23.2992, Evaluation BLEU: 0.0000\n",
      "Epoch [849/2000], Evaluation Loss: 23.2994, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [850/2000], Evaluation Loss: 23.2996, Evaluation BLEU: 0.0000\n",
      "Epoch [851/2000], Evaluation Loss: 23.2999, Evaluation BLEU: 0.0000\n",
      "Epoch [852/2000], Evaluation Loss: 23.3001, Evaluation BLEU: 0.0000\n",
      "Epoch [853/2000], Evaluation Loss: 23.3003, Evaluation BLEU: 0.0000\n",
      "Epoch [854/2000], Evaluation Loss: 23.3005, Evaluation BLEU: 0.0000\n",
      "Epoch [855/2000], Evaluation Loss: 23.3006, Evaluation BLEU: 0.0000\n",
      "Epoch [856/2000], Evaluation Loss: 23.3008, Evaluation BLEU: 0.0000\n",
      "Epoch [857/2000], Evaluation Loss: 23.3010, Evaluation BLEU: 0.0000\n",
      "Epoch [858/2000], Evaluation Loss: 23.3012, Evaluation BLEU: 0.0000\n",
      "Epoch [859/2000], Evaluation Loss: 23.3013, Evaluation BLEU: 0.0000\n",
      "Epoch [860/2000], Evaluation Loss: 23.3014, Evaluation BLEU: 0.0000\n",
      "Epoch [861/2000], Evaluation Loss: 23.3016, Evaluation BLEU: 0.0000\n",
      "Epoch [862/2000], Evaluation Loss: 23.3017, Evaluation BLEU: 0.0000\n",
      "Epoch [863/2000], Evaluation Loss: 23.3019, Evaluation BLEU: 0.0000\n",
      "Epoch [864/2000], Evaluation Loss: 23.3020, Evaluation BLEU: 0.0000\n",
      "Epoch [865/2000], Evaluation Loss: 23.3022, Evaluation BLEU: 0.0000\n",
      "Epoch [866/2000], Evaluation Loss: 23.3024, Evaluation BLEU: 0.0000\n",
      "Epoch [867/2000], Evaluation Loss: 23.3026, Evaluation BLEU: 0.0000\n",
      "Epoch [868/2000], Evaluation Loss: 23.3028, Evaluation BLEU: 0.0000\n",
      "Epoch [869/2000], Evaluation Loss: 23.3030, Evaluation BLEU: 0.0000\n",
      "Epoch [870/2000], Evaluation Loss: 23.3032, Evaluation BLEU: 0.0000\n",
      "Epoch [871/2000], Evaluation Loss: 23.3034, Evaluation BLEU: 0.0000\n",
      "Epoch [872/2000], Evaluation Loss: 23.3035, Evaluation BLEU: 0.0000\n",
      "Epoch [873/2000], Evaluation Loss: 23.3037, Evaluation BLEU: 0.0000\n",
      "Epoch [874/2000], Evaluation Loss: 23.3039, Evaluation BLEU: 0.0000\n",
      "Epoch [875/2000], Evaluation Loss: 23.3041, Evaluation BLEU: 0.0000\n",
      "Epoch [876/2000], Evaluation Loss: 23.3043, Evaluation BLEU: 0.0000\n",
      "Epoch [877/2000], Evaluation Loss: 23.3045, Evaluation BLEU: 0.0000\n",
      "Epoch [878/2000], Evaluation Loss: 23.3048, Evaluation BLEU: 0.0000\n",
      "Epoch [879/2000], Evaluation Loss: 23.3050, Evaluation BLEU: 0.0000\n",
      "Epoch [880/2000], Evaluation Loss: 23.3052, Evaluation BLEU: 0.0000\n",
      "Epoch [881/2000], Evaluation Loss: 23.3055, Evaluation BLEU: 0.0000\n",
      "Epoch [882/2000], Evaluation Loss: 23.3057, Evaluation BLEU: 0.0000\n",
      "Epoch [883/2000], Evaluation Loss: 23.3059, Evaluation BLEU: 0.0000\n",
      "Epoch [884/2000], Evaluation Loss: 23.3062, Evaluation BLEU: 0.0000\n",
      "Epoch [885/2000], Evaluation Loss: 23.3064, Evaluation BLEU: 0.0000\n",
      "Epoch [886/2000], Evaluation Loss: 23.3066, Evaluation BLEU: 0.0000\n",
      "Epoch [887/2000], Evaluation Loss: 23.3068, Evaluation BLEU: 0.0000\n",
      "Epoch [888/2000], Evaluation Loss: 23.3070, Evaluation BLEU: 0.0000\n",
      "Epoch [889/2000], Evaluation Loss: 23.3073, Evaluation BLEU: 0.0000\n",
      "Epoch [890/2000], Evaluation Loss: 23.3075, Evaluation BLEU: 0.0000\n",
      "Epoch [891/2000], Evaluation Loss: 23.3077, Evaluation BLEU: 0.0000\n",
      "Epoch [892/2000], Evaluation Loss: 23.3080, Evaluation BLEU: 0.0000\n",
      "Epoch [893/2000], Evaluation Loss: 23.3082, Evaluation BLEU: 0.0000\n",
      "Epoch [894/2000], Evaluation Loss: 23.3085, Evaluation BLEU: 0.0000\n",
      "Epoch [895/2000], Evaluation Loss: 23.3087, Evaluation BLEU: 0.0000\n",
      "Epoch [896/2000], Evaluation Loss: 23.3090, Evaluation BLEU: 0.0000\n",
      "Epoch [897/2000], Evaluation Loss: 23.3092, Evaluation BLEU: 0.0000\n",
      "Epoch [898/2000], Evaluation Loss: 23.3095, Evaluation BLEU: 0.0000\n",
      "Epoch [899/2000], Evaluation Loss: 23.3098, Evaluation BLEU: 0.0000\n",
      "Epoch [900/2000], Evaluation Loss: 23.3101, Evaluation BLEU: 0.0000\n",
      "Epoch [901/2000], Evaluation Loss: 23.3104, Evaluation BLEU: 0.0000\n",
      "Epoch [902/2000], Evaluation Loss: 23.3107, Evaluation BLEU: 0.0000\n",
      "Epoch [903/2000], Evaluation Loss: 23.3110, Evaluation BLEU: 0.0000\n",
      "Epoch [904/2000], Evaluation Loss: 23.3113, Evaluation BLEU: 0.0000\n",
      "Epoch [905/2000], Evaluation Loss: 23.3116, Evaluation BLEU: 0.0000\n",
      "Epoch [906/2000], Evaluation Loss: 23.3119, Evaluation BLEU: 0.0000\n",
      "Epoch [907/2000], Evaluation Loss: 23.3123, Evaluation BLEU: 0.0000\n",
      "Epoch [908/2000], Evaluation Loss: 23.3126, Evaluation BLEU: 0.0000\n",
      "Epoch [909/2000], Evaluation Loss: 23.3129, Evaluation BLEU: 0.0000\n",
      "Epoch [910/2000], Evaluation Loss: 23.3132, Evaluation BLEU: 0.0000\n",
      "Epoch [911/2000], Evaluation Loss: 23.3135, Evaluation BLEU: 0.0000\n",
      "Epoch [912/2000], Evaluation Loss: 23.3138, Evaluation BLEU: 0.0000\n",
      "Epoch [913/2000], Evaluation Loss: 23.3141, Evaluation BLEU: 0.0000\n",
      "Epoch [914/2000], Evaluation Loss: 23.3144, Evaluation BLEU: 0.0000\n",
      "Epoch [915/2000], Evaluation Loss: 23.3147, Evaluation BLEU: 0.0000\n",
      "Epoch [916/2000], Evaluation Loss: 23.3150, Evaluation BLEU: 0.0000\n",
      "Epoch [917/2000], Evaluation Loss: 23.3153, Evaluation BLEU: 0.0000\n",
      "Epoch [918/2000], Evaluation Loss: 23.3157, Evaluation BLEU: 0.0000\n",
      "Epoch [919/2000], Evaluation Loss: 23.3160, Evaluation BLEU: 0.0000\n",
      "Epoch [920/2000], Evaluation Loss: 23.3163, Evaluation BLEU: 0.0000\n",
      "Epoch [921/2000], Evaluation Loss: 23.3167, Evaluation BLEU: 0.0000\n",
      "Epoch [922/2000], Evaluation Loss: 23.3170, Evaluation BLEU: 0.0000\n",
      "Epoch [923/2000], Evaluation Loss: 23.3174, Evaluation BLEU: 0.0000\n",
      "Epoch [924/2000], Evaluation Loss: 23.3178, Evaluation BLEU: 0.0000\n",
      "Epoch [925/2000], Evaluation Loss: 23.3181, Evaluation BLEU: 0.0000\n",
      "Epoch [926/2000], Evaluation Loss: 23.3185, Evaluation BLEU: 0.0000\n",
      "Epoch [927/2000], Evaluation Loss: 23.3189, Evaluation BLEU: 0.0000\n",
      "Epoch [928/2000], Evaluation Loss: 23.3193, Evaluation BLEU: 0.0000\n",
      "Epoch [929/2000], Evaluation Loss: 23.3196, Evaluation BLEU: 0.0000\n",
      "Epoch [930/2000], Evaluation Loss: 23.3200, Evaluation BLEU: 0.0000\n",
      "Epoch [931/2000], Evaluation Loss: 23.3204, Evaluation BLEU: 0.0000\n",
      "Epoch [932/2000], Evaluation Loss: 23.3208, Evaluation BLEU: 0.0000\n",
      "Epoch [933/2000], Evaluation Loss: 23.3211, Evaluation BLEU: 0.0000\n",
      "Epoch [934/2000], Evaluation Loss: 23.3214, Evaluation BLEU: 0.0000\n",
      "Epoch [935/2000], Evaluation Loss: 23.3217, Evaluation BLEU: 0.0000\n",
      "Epoch [936/2000], Evaluation Loss: 23.3221, Evaluation BLEU: 0.0000\n",
      "Epoch [937/2000], Evaluation Loss: 23.3224, Evaluation BLEU: 0.0000\n",
      "Epoch [938/2000], Evaluation Loss: 23.3227, Evaluation BLEU: 0.0000\n",
      "Epoch [939/2000], Evaluation Loss: 23.3230, Evaluation BLEU: 0.0000\n",
      "Epoch [940/2000], Evaluation Loss: 23.3233, Evaluation BLEU: 0.0000\n",
      "Epoch [941/2000], Evaluation Loss: 23.3236, Evaluation BLEU: 0.0000\n",
      "Epoch [942/2000], Evaluation Loss: 23.3240, Evaluation BLEU: 0.0000\n",
      "Epoch [943/2000], Evaluation Loss: 23.3243, Evaluation BLEU: 0.0000\n",
      "Epoch [944/2000], Evaluation Loss: 23.3246, Evaluation BLEU: 0.0000\n",
      "Epoch [945/2000], Evaluation Loss: 23.3248, Evaluation BLEU: 0.0000\n",
      "Epoch [946/2000], Evaluation Loss: 23.3251, Evaluation BLEU: 0.0000\n",
      "Epoch [947/2000], Evaluation Loss: 23.3255, Evaluation BLEU: 0.0000\n",
      "Epoch [948/2000], Evaluation Loss: 23.3258, Evaluation BLEU: 0.0000\n",
      "Epoch [949/2000], Evaluation Loss: 23.3261, Evaluation BLEU: 0.0000\n",
      "Epoch [950/2000], Evaluation Loss: 23.3264, Evaluation BLEU: 0.0000\n",
      "Epoch [951/2000], Evaluation Loss: 23.3267, Evaluation BLEU: 0.0000\n",
      "Epoch [952/2000], Evaluation Loss: 23.3271, Evaluation BLEU: 0.0000\n",
      "Epoch [953/2000], Evaluation Loss: 23.3274, Evaluation BLEU: 0.0000\n",
      "Epoch [954/2000], Evaluation Loss: 23.3277, Evaluation BLEU: 0.0000\n",
      "Epoch [955/2000], Evaluation Loss: 23.3280, Evaluation BLEU: 0.0000\n",
      "Epoch [956/2000], Evaluation Loss: 23.3283, Evaluation BLEU: 0.0000\n",
      "Epoch [957/2000], Evaluation Loss: 23.3286, Evaluation BLEU: 0.0000\n",
      "Epoch [958/2000], Evaluation Loss: 23.3289, Evaluation BLEU: 0.0000\n",
      "Epoch [959/2000], Evaluation Loss: 23.3292, Evaluation BLEU: 0.0000\n",
      "Epoch [960/2000], Evaluation Loss: 23.3295, Evaluation BLEU: 0.0000\n",
      "Epoch [961/2000], Evaluation Loss: 23.3298, Evaluation BLEU: 0.0000\n",
      "Epoch [962/2000], Evaluation Loss: 23.3300, Evaluation BLEU: 0.0000\n",
      "Epoch [963/2000], Evaluation Loss: 23.3302, Evaluation BLEU: 0.0000\n",
      "Epoch [964/2000], Evaluation Loss: 23.3304, Evaluation BLEU: 0.0000\n",
      "Epoch [965/2000], Evaluation Loss: 23.3307, Evaluation BLEU: 0.0000\n",
      "Epoch [966/2000], Evaluation Loss: 23.3309, Evaluation BLEU: 0.0000\n",
      "Epoch [967/2000], Evaluation Loss: 23.3311, Evaluation BLEU: 0.0000\n",
      "Epoch [968/2000], Evaluation Loss: 23.3312, Evaluation BLEU: 0.0000\n",
      "Epoch [969/2000], Evaluation Loss: 23.3314, Evaluation BLEU: 0.0000\n",
      "Epoch [970/2000], Evaluation Loss: 23.3315, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [971/2000], Evaluation Loss: 23.3317, Evaluation BLEU: 0.0000\n",
      "Epoch [972/2000], Evaluation Loss: 23.3319, Evaluation BLEU: 0.0000\n",
      "Epoch [973/2000], Evaluation Loss: 23.3320, Evaluation BLEU: 0.0000\n",
      "Epoch [974/2000], Evaluation Loss: 23.3321, Evaluation BLEU: 0.0000\n",
      "Epoch [975/2000], Evaluation Loss: 23.3323, Evaluation BLEU: 0.0000\n",
      "Epoch [976/2000], Evaluation Loss: 23.3324, Evaluation BLEU: 0.0000\n",
      "Epoch [977/2000], Evaluation Loss: 23.3326, Evaluation BLEU: 0.0000\n",
      "Epoch [978/2000], Evaluation Loss: 23.3327, Evaluation BLEU: 0.0000\n",
      "Epoch [979/2000], Evaluation Loss: 23.3328, Evaluation BLEU: 0.0000\n",
      "Epoch [980/2000], Evaluation Loss: 23.3330, Evaluation BLEU: 0.0000\n",
      "Epoch [981/2000], Evaluation Loss: 23.3331, Evaluation BLEU: 0.0000\n",
      "Epoch [982/2000], Evaluation Loss: 23.3332, Evaluation BLEU: 0.0000\n",
      "Epoch [983/2000], Evaluation Loss: 23.3334, Evaluation BLEU: 0.0000\n",
      "Epoch [984/2000], Evaluation Loss: 23.3335, Evaluation BLEU: 0.0000\n",
      "Epoch [985/2000], Evaluation Loss: 23.3337, Evaluation BLEU: 0.0000\n",
      "Epoch [986/2000], Evaluation Loss: 23.3339, Evaluation BLEU: 0.0000\n",
      "Epoch [987/2000], Evaluation Loss: 23.3341, Evaluation BLEU: 0.0000\n",
      "Epoch [988/2000], Evaluation Loss: 23.3343, Evaluation BLEU: 0.0000\n",
      "Epoch [989/2000], Evaluation Loss: 23.3346, Evaluation BLEU: 0.0000\n",
      "Epoch [990/2000], Evaluation Loss: 23.3349, Evaluation BLEU: 0.0000\n",
      "Epoch [991/2000], Evaluation Loss: 23.3351, Evaluation BLEU: 0.0000\n",
      "Epoch [992/2000], Evaluation Loss: 23.3354, Evaluation BLEU: 0.0000\n",
      "Epoch [993/2000], Evaluation Loss: 23.3357, Evaluation BLEU: 0.0000\n",
      "Epoch [994/2000], Evaluation Loss: 23.3361, Evaluation BLEU: 0.0000\n",
      "Epoch [995/2000], Evaluation Loss: 23.3365, Evaluation BLEU: 0.0000\n",
      "Epoch [996/2000], Evaluation Loss: 23.3368, Evaluation BLEU: 0.0000\n",
      "Epoch [997/2000], Evaluation Loss: 23.3372, Evaluation BLEU: 0.0000\n",
      "Epoch [998/2000], Evaluation Loss: 23.3375, Evaluation BLEU: 0.0000\n",
      "Epoch [999/2000], Evaluation Loss: 23.3378, Evaluation BLEU: 0.0000\n",
      "Epoch [1000/2000], Evaluation Loss: 23.3381, Evaluation BLEU: 0.0000\n",
      "Epoch [1001/2000], Evaluation Loss: 23.3384, Evaluation BLEU: 0.0000\n",
      "Epoch [1002/2000], Evaluation Loss: 23.3388, Evaluation BLEU: 0.0000\n",
      "Epoch [1003/2000], Evaluation Loss: 23.3391, Evaluation BLEU: 0.0000\n",
      "Epoch [1004/2000], Evaluation Loss: 23.3395, Evaluation BLEU: 0.0000\n",
      "Epoch [1005/2000], Evaluation Loss: 23.3398, Evaluation BLEU: 0.0000\n",
      "Epoch [1006/2000], Evaluation Loss: 23.3402, Evaluation BLEU: 0.0000\n",
      "Epoch [1007/2000], Evaluation Loss: 23.3405, Evaluation BLEU: 0.0000\n",
      "Epoch [1008/2000], Evaluation Loss: 23.3409, Evaluation BLEU: 0.0000\n",
      "Epoch [1009/2000], Evaluation Loss: 23.3412, Evaluation BLEU: 0.0000\n",
      "Epoch [1010/2000], Evaluation Loss: 23.3416, Evaluation BLEU: 0.0000\n",
      "Epoch [1011/2000], Evaluation Loss: 23.3420, Evaluation BLEU: 0.0000\n",
      "Epoch [1012/2000], Evaluation Loss: 23.3424, Evaluation BLEU: 0.0000\n",
      "Epoch [1013/2000], Evaluation Loss: 23.3427, Evaluation BLEU: 0.0000\n",
      "Epoch [1014/2000], Evaluation Loss: 23.3431, Evaluation BLEU: 0.0000\n",
      "Epoch [1015/2000], Evaluation Loss: 23.3435, Evaluation BLEU: 0.0000\n",
      "Epoch [1016/2000], Evaluation Loss: 23.3439, Evaluation BLEU: 0.0000\n",
      "Epoch [1017/2000], Evaluation Loss: 23.3443, Evaluation BLEU: 0.0000\n",
      "Epoch [1018/2000], Evaluation Loss: 23.3447, Evaluation BLEU: 0.0000\n",
      "Epoch [1019/2000], Evaluation Loss: 23.3451, Evaluation BLEU: 0.0000\n",
      "Epoch [1020/2000], Evaluation Loss: 23.3455, Evaluation BLEU: 0.0000\n",
      "Epoch [1021/2000], Evaluation Loss: 23.3458, Evaluation BLEU: 0.0000\n",
      "Epoch [1022/2000], Evaluation Loss: 23.3462, Evaluation BLEU: 0.0000\n",
      "Epoch [1023/2000], Evaluation Loss: 23.3466, Evaluation BLEU: 0.0000\n",
      "Epoch [1024/2000], Evaluation Loss: 23.3469, Evaluation BLEU: 0.0000\n",
      "Epoch [1025/2000], Evaluation Loss: 23.3472, Evaluation BLEU: 0.0000\n",
      "Epoch [1026/2000], Evaluation Loss: 23.3476, Evaluation BLEU: 0.0000\n",
      "Epoch [1027/2000], Evaluation Loss: 23.3479, Evaluation BLEU: 0.0000\n",
      "Epoch [1028/2000], Evaluation Loss: 23.3482, Evaluation BLEU: 0.0000\n",
      "Epoch [1029/2000], Evaluation Loss: 23.3485, Evaluation BLEU: 0.0000\n",
      "Epoch [1030/2000], Evaluation Loss: 23.3488, Evaluation BLEU: 0.0000\n",
      "Epoch [1031/2000], Evaluation Loss: 23.3491, Evaluation BLEU: 0.0000\n",
      "Epoch [1032/2000], Evaluation Loss: 23.3494, Evaluation BLEU: 0.0000\n",
      "Epoch [1033/2000], Evaluation Loss: 23.3497, Evaluation BLEU: 0.0000\n",
      "Epoch [1034/2000], Evaluation Loss: 23.3501, Evaluation BLEU: 0.0000\n",
      "Epoch [1035/2000], Evaluation Loss: 23.3504, Evaluation BLEU: 0.0000\n",
      "Epoch [1036/2000], Evaluation Loss: 23.3507, Evaluation BLEU: 0.0000\n",
      "Epoch [1037/2000], Evaluation Loss: 23.3510, Evaluation BLEU: 0.0000\n",
      "Epoch [1038/2000], Evaluation Loss: 23.3514, Evaluation BLEU: 0.0000\n",
      "Epoch [1039/2000], Evaluation Loss: 23.3517, Evaluation BLEU: 0.0000\n",
      "Epoch [1040/2000], Evaluation Loss: 23.3521, Evaluation BLEU: 0.0000\n",
      "Epoch [1041/2000], Evaluation Loss: 23.3524, Evaluation BLEU: 0.0000\n",
      "Epoch [1042/2000], Evaluation Loss: 23.3528, Evaluation BLEU: 0.0000\n",
      "Epoch [1043/2000], Evaluation Loss: 23.3531, Evaluation BLEU: 0.0000\n",
      "Epoch [1044/2000], Evaluation Loss: 23.3535, Evaluation BLEU: 0.0000\n",
      "Epoch [1045/2000], Evaluation Loss: 23.3539, Evaluation BLEU: 0.0000\n",
      "Epoch [1046/2000], Evaluation Loss: 23.3542, Evaluation BLEU: 0.0000\n",
      "Epoch [1047/2000], Evaluation Loss: 23.3545, Evaluation BLEU: 0.0000\n",
      "Epoch [1048/2000], Evaluation Loss: 23.3549, Evaluation BLEU: 0.0000\n",
      "Epoch [1049/2000], Evaluation Loss: 23.3553, Evaluation BLEU: 0.0000\n",
      "Epoch [1050/2000], Evaluation Loss: 23.3556, Evaluation BLEU: 0.0000\n",
      "Epoch [1051/2000], Evaluation Loss: 23.3560, Evaluation BLEU: 0.0000\n",
      "Epoch [1052/2000], Evaluation Loss: 23.3564, Evaluation BLEU: 0.0000\n",
      "Epoch [1053/2000], Evaluation Loss: 23.3567, Evaluation BLEU: 0.0000\n",
      "Epoch [1054/2000], Evaluation Loss: 23.3571, Evaluation BLEU: 0.0000\n",
      "Epoch [1055/2000], Evaluation Loss: 23.3575, Evaluation BLEU: 0.0000\n",
      "Epoch [1056/2000], Evaluation Loss: 23.3578, Evaluation BLEU: 0.0000\n",
      "Epoch [1057/2000], Evaluation Loss: 23.3582, Evaluation BLEU: 0.0000\n",
      "Epoch [1058/2000], Evaluation Loss: 23.3586, Evaluation BLEU: 0.0000\n",
      "Epoch [1059/2000], Evaluation Loss: 23.3589, Evaluation BLEU: 0.0000\n",
      "Epoch [1060/2000], Evaluation Loss: 23.3593, Evaluation BLEU: 0.0000\n",
      "Epoch [1061/2000], Evaluation Loss: 23.3597, Evaluation BLEU: 0.0000\n",
      "Epoch [1062/2000], Evaluation Loss: 23.3600, Evaluation BLEU: 0.0000\n",
      "Epoch [1063/2000], Evaluation Loss: 23.3604, Evaluation BLEU: 0.0000\n",
      "Epoch [1064/2000], Evaluation Loss: 23.3607, Evaluation BLEU: 0.0000\n",
      "Epoch [1065/2000], Evaluation Loss: 23.3611, Evaluation BLEU: 0.0000\n",
      "Epoch [1066/2000], Evaluation Loss: 23.3614, Evaluation BLEU: 0.0000\n",
      "Epoch [1067/2000], Evaluation Loss: 23.3618, Evaluation BLEU: 0.0000\n",
      "Epoch [1068/2000], Evaluation Loss: 23.3621, Evaluation BLEU: 0.0000\n",
      "Epoch [1069/2000], Evaluation Loss: 23.3625, Evaluation BLEU: 0.0000\n",
      "Epoch [1070/2000], Evaluation Loss: 23.3629, Evaluation BLEU: 0.0000\n",
      "Epoch [1071/2000], Evaluation Loss: 23.3632, Evaluation BLEU: 0.0000\n",
      "Epoch [1072/2000], Evaluation Loss: 23.3636, Evaluation BLEU: 0.0000\n",
      "Epoch [1073/2000], Evaluation Loss: 23.3640, Evaluation BLEU: 0.0000\n",
      "Epoch [1074/2000], Evaluation Loss: 23.3644, Evaluation BLEU: 0.0000\n",
      "Epoch [1075/2000], Evaluation Loss: 23.3648, Evaluation BLEU: 0.0000\n",
      "Epoch [1076/2000], Evaluation Loss: 23.3652, Evaluation BLEU: 0.0000\n",
      "Epoch [1077/2000], Evaluation Loss: 23.3655, Evaluation BLEU: 0.0000\n",
      "Epoch [1078/2000], Evaluation Loss: 23.3659, Evaluation BLEU: 0.0000\n",
      "Epoch [1079/2000], Evaluation Loss: 23.3663, Evaluation BLEU: 0.0000\n",
      "Epoch [1080/2000], Evaluation Loss: 23.3667, Evaluation BLEU: 0.0000\n",
      "Epoch [1081/2000], Evaluation Loss: 23.3671, Evaluation BLEU: 0.0000\n",
      "Epoch [1082/2000], Evaluation Loss: 23.3674, Evaluation BLEU: 0.0000\n",
      "Epoch [1083/2000], Evaluation Loss: 23.3678, Evaluation BLEU: 0.0000\n",
      "Epoch [1084/2000], Evaluation Loss: 23.3681, Evaluation BLEU: 0.0000\n",
      "Epoch [1085/2000], Evaluation Loss: 23.3685, Evaluation BLEU: 0.0000\n",
      "Epoch [1086/2000], Evaluation Loss: 23.3688, Evaluation BLEU: 0.0000\n",
      "Epoch [1087/2000], Evaluation Loss: 23.3692, Evaluation BLEU: 0.0000\n",
      "Epoch [1088/2000], Evaluation Loss: 23.3695, Evaluation BLEU: 0.0000\n",
      "Epoch [1089/2000], Evaluation Loss: 23.3698, Evaluation BLEU: 0.0000\n",
      "Epoch [1090/2000], Evaluation Loss: 23.3701, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1091/2000], Evaluation Loss: 23.3704, Evaluation BLEU: 0.0000\n",
      "Epoch [1092/2000], Evaluation Loss: 23.3708, Evaluation BLEU: 0.0000\n",
      "Epoch [1093/2000], Evaluation Loss: 23.3711, Evaluation BLEU: 0.0000\n",
      "Epoch [1094/2000], Evaluation Loss: 23.3714, Evaluation BLEU: 0.0000\n",
      "Epoch [1095/2000], Evaluation Loss: 23.3717, Evaluation BLEU: 0.0000\n",
      "Epoch [1096/2000], Evaluation Loss: 23.3721, Evaluation BLEU: 0.0000\n",
      "Epoch [1097/2000], Evaluation Loss: 23.3725, Evaluation BLEU: 0.0000\n",
      "Epoch [1098/2000], Evaluation Loss: 23.3728, Evaluation BLEU: 0.0000\n",
      "Epoch [1099/2000], Evaluation Loss: 23.3733, Evaluation BLEU: 0.0000\n",
      "Epoch [1100/2000], Evaluation Loss: 23.3736, Evaluation BLEU: 0.0000\n",
      "Epoch [1101/2000], Evaluation Loss: 23.3740, Evaluation BLEU: 0.0000\n",
      "Epoch [1102/2000], Evaluation Loss: 23.3743, Evaluation BLEU: 0.0000\n",
      "Epoch [1103/2000], Evaluation Loss: 23.3747, Evaluation BLEU: 0.0000\n",
      "Epoch [1104/2000], Evaluation Loss: 23.3750, Evaluation BLEU: 0.0000\n",
      "Epoch [1105/2000], Evaluation Loss: 23.3754, Evaluation BLEU: 0.0000\n",
      "Epoch [1106/2000], Evaluation Loss: 23.3757, Evaluation BLEU: 0.0000\n",
      "Epoch [1107/2000], Evaluation Loss: 23.3760, Evaluation BLEU: 0.0000\n",
      "Epoch [1108/2000], Evaluation Loss: 23.3764, Evaluation BLEU: 0.0000\n",
      "Epoch [1109/2000], Evaluation Loss: 23.3767, Evaluation BLEU: 0.0000\n",
      "Epoch [1110/2000], Evaluation Loss: 23.3770, Evaluation BLEU: 0.0000\n",
      "Epoch [1111/2000], Evaluation Loss: 23.3773, Evaluation BLEU: 0.0000\n",
      "Epoch [1112/2000], Evaluation Loss: 23.3776, Evaluation BLEU: 0.0000\n",
      "Epoch [1113/2000], Evaluation Loss: 23.3780, Evaluation BLEU: 0.0000\n",
      "Epoch [1114/2000], Evaluation Loss: 23.3783, Evaluation BLEU: 0.0000\n",
      "Epoch [1115/2000], Evaluation Loss: 23.3786, Evaluation BLEU: 0.0000\n",
      "Epoch [1116/2000], Evaluation Loss: 23.3789, Evaluation BLEU: 0.0000\n",
      "Epoch [1117/2000], Evaluation Loss: 23.3792, Evaluation BLEU: 0.0000\n",
      "Epoch [1118/2000], Evaluation Loss: 23.3795, Evaluation BLEU: 0.0000\n",
      "Epoch [1119/2000], Evaluation Loss: 23.3798, Evaluation BLEU: 0.0000\n",
      "Epoch [1120/2000], Evaluation Loss: 23.3801, Evaluation BLEU: 0.0000\n",
      "Epoch [1121/2000], Evaluation Loss: 23.3803, Evaluation BLEU: 0.0000\n",
      "Epoch [1122/2000], Evaluation Loss: 23.3806, Evaluation BLEU: 0.0000\n",
      "Epoch [1123/2000], Evaluation Loss: 23.3808, Evaluation BLEU: 0.0000\n",
      "Epoch [1124/2000], Evaluation Loss: 23.3811, Evaluation BLEU: 0.0000\n",
      "Epoch [1125/2000], Evaluation Loss: 23.3813, Evaluation BLEU: 0.0000\n",
      "Epoch [1126/2000], Evaluation Loss: 23.3816, Evaluation BLEU: 0.0000\n",
      "Epoch [1127/2000], Evaluation Loss: 23.3818, Evaluation BLEU: 0.0000\n",
      "Epoch [1128/2000], Evaluation Loss: 23.3821, Evaluation BLEU: 0.0000\n",
      "Epoch [1129/2000], Evaluation Loss: 23.3824, Evaluation BLEU: 0.0000\n",
      "Epoch [1130/2000], Evaluation Loss: 23.3826, Evaluation BLEU: 0.0000\n",
      "Epoch [1131/2000], Evaluation Loss: 23.3829, Evaluation BLEU: 0.0000\n",
      "Epoch [1132/2000], Evaluation Loss: 23.3831, Evaluation BLEU: 0.0000\n",
      "Epoch [1133/2000], Evaluation Loss: 23.3834, Evaluation BLEU: 0.0000\n",
      "Epoch [1134/2000], Evaluation Loss: 23.3837, Evaluation BLEU: 0.0000\n",
      "Epoch [1135/2000], Evaluation Loss: 23.3840, Evaluation BLEU: 0.0000\n",
      "Epoch [1136/2000], Evaluation Loss: 23.3843, Evaluation BLEU: 0.0000\n",
      "Epoch [1137/2000], Evaluation Loss: 23.3846, Evaluation BLEU: 0.0000\n",
      "Epoch [1138/2000], Evaluation Loss: 23.3848, Evaluation BLEU: 0.0000\n",
      "Epoch [1139/2000], Evaluation Loss: 23.3851, Evaluation BLEU: 0.0000\n",
      "Epoch [1140/2000], Evaluation Loss: 23.3854, Evaluation BLEU: 0.0000\n",
      "Epoch [1141/2000], Evaluation Loss: 23.3856, Evaluation BLEU: 0.0000\n",
      "Epoch [1142/2000], Evaluation Loss: 23.3859, Evaluation BLEU: 0.0000\n",
      "Epoch [1143/2000], Evaluation Loss: 23.3861, Evaluation BLEU: 0.0000\n",
      "Epoch [1144/2000], Evaluation Loss: 23.3864, Evaluation BLEU: 0.0000\n",
      "Epoch [1145/2000], Evaluation Loss: 23.3866, Evaluation BLEU: 0.0000\n",
      "Epoch [1146/2000], Evaluation Loss: 23.3869, Evaluation BLEU: 0.0000\n",
      "Epoch [1147/2000], Evaluation Loss: 23.3872, Evaluation BLEU: 0.0000\n",
      "Epoch [1148/2000], Evaluation Loss: 23.3875, Evaluation BLEU: 0.0000\n",
      "Epoch [1149/2000], Evaluation Loss: 23.3878, Evaluation BLEU: 0.0000\n",
      "Epoch [1150/2000], Evaluation Loss: 23.3881, Evaluation BLEU: 0.0000\n",
      "Epoch [1151/2000], Evaluation Loss: 23.3885, Evaluation BLEU: 0.0000\n",
      "Epoch [1152/2000], Evaluation Loss: 23.3888, Evaluation BLEU: 0.0000\n",
      "Epoch [1153/2000], Evaluation Loss: 23.3891, Evaluation BLEU: 0.0000\n",
      "Epoch [1154/2000], Evaluation Loss: 23.3895, Evaluation BLEU: 0.0000\n",
      "Epoch [1155/2000], Evaluation Loss: 23.3898, Evaluation BLEU: 0.0000\n",
      "Epoch [1156/2000], Evaluation Loss: 23.3902, Evaluation BLEU: 0.0000\n",
      "Epoch [1157/2000], Evaluation Loss: 23.3905, Evaluation BLEU: 0.0000\n",
      "Epoch [1158/2000], Evaluation Loss: 23.3908, Evaluation BLEU: 0.0000\n",
      "Epoch [1159/2000], Evaluation Loss: 23.3911, Evaluation BLEU: 0.0000\n",
      "Epoch [1160/2000], Evaluation Loss: 23.3914, Evaluation BLEU: 0.0000\n",
      "Epoch [1161/2000], Evaluation Loss: 23.3917, Evaluation BLEU: 0.0000\n",
      "Epoch [1162/2000], Evaluation Loss: 23.3919, Evaluation BLEU: 0.0000\n",
      "Epoch [1163/2000], Evaluation Loss: 23.3922, Evaluation BLEU: 0.0000\n",
      "Epoch [1164/2000], Evaluation Loss: 23.3924, Evaluation BLEU: 0.0000\n",
      "Epoch [1165/2000], Evaluation Loss: 23.3926, Evaluation BLEU: 0.0000\n",
      "Epoch [1166/2000], Evaluation Loss: 23.3928, Evaluation BLEU: 0.0000\n",
      "Epoch [1167/2000], Evaluation Loss: 23.3930, Evaluation BLEU: 0.0000\n",
      "Epoch [1168/2000], Evaluation Loss: 23.3932, Evaluation BLEU: 0.0000\n",
      "Epoch [1169/2000], Evaluation Loss: 23.3934, Evaluation BLEU: 0.0000\n",
      "Epoch [1170/2000], Evaluation Loss: 23.3936, Evaluation BLEU: 0.0000\n",
      "Epoch [1171/2000], Evaluation Loss: 23.3938, Evaluation BLEU: 0.0000\n",
      "Epoch [1172/2000], Evaluation Loss: 23.3940, Evaluation BLEU: 0.0000\n",
      "Epoch [1173/2000], Evaluation Loss: 23.3942, Evaluation BLEU: 0.0000\n",
      "Epoch [1174/2000], Evaluation Loss: 23.3944, Evaluation BLEU: 0.0000\n",
      "Epoch [1175/2000], Evaluation Loss: 23.3946, Evaluation BLEU: 0.0000\n",
      "Epoch [1176/2000], Evaluation Loss: 23.3949, Evaluation BLEU: 0.0000\n",
      "Epoch [1177/2000], Evaluation Loss: 23.3951, Evaluation BLEU: 0.0000\n",
      "Epoch [1178/2000], Evaluation Loss: 23.3953, Evaluation BLEU: 0.0000\n",
      "Epoch [1179/2000], Evaluation Loss: 23.3956, Evaluation BLEU: 0.0000\n",
      "Epoch [1180/2000], Evaluation Loss: 23.3958, Evaluation BLEU: 0.0000\n",
      "Epoch [1181/2000], Evaluation Loss: 23.3961, Evaluation BLEU: 0.0000\n",
      "Epoch [1182/2000], Evaluation Loss: 23.3964, Evaluation BLEU: 0.0000\n",
      "Epoch [1183/2000], Evaluation Loss: 23.3966, Evaluation BLEU: 0.0000\n",
      "Epoch [1184/2000], Evaluation Loss: 23.3969, Evaluation BLEU: 0.0000\n",
      "Epoch [1185/2000], Evaluation Loss: 23.3972, Evaluation BLEU: 0.0000\n",
      "Epoch [1186/2000], Evaluation Loss: 23.3975, Evaluation BLEU: 0.0000\n",
      "Epoch [1187/2000], Evaluation Loss: 23.3978, Evaluation BLEU: 0.0000\n",
      "Epoch [1188/2000], Evaluation Loss: 23.3981, Evaluation BLEU: 0.0000\n",
      "Epoch [1189/2000], Evaluation Loss: 23.3984, Evaluation BLEU: 0.0000\n",
      "Epoch [1190/2000], Evaluation Loss: 23.3987, Evaluation BLEU: 0.0000\n",
      "Epoch [1191/2000], Evaluation Loss: 23.3990, Evaluation BLEU: 0.0000\n",
      "Epoch [1192/2000], Evaluation Loss: 23.3992, Evaluation BLEU: 0.0000\n",
      "Epoch [1193/2000], Evaluation Loss: 23.3994, Evaluation BLEU: 0.0000\n",
      "Epoch [1194/2000], Evaluation Loss: 23.3996, Evaluation BLEU: 0.0000\n",
      "Epoch [1195/2000], Evaluation Loss: 23.3998, Evaluation BLEU: 0.0000\n",
      "Epoch [1196/2000], Evaluation Loss: 23.3999, Evaluation BLEU: 0.0000\n",
      "Epoch [1197/2000], Evaluation Loss: 23.4001, Evaluation BLEU: 0.0000\n",
      "Epoch [1198/2000], Evaluation Loss: 23.4001, Evaluation BLEU: 0.0000\n",
      "Epoch [1199/2000], Evaluation Loss: 23.4002, Evaluation BLEU: 0.0000\n",
      "Epoch [1200/2000], Evaluation Loss: 23.4003, Evaluation BLEU: 0.0000\n",
      "Epoch [1201/2000], Evaluation Loss: 23.4004, Evaluation BLEU: 0.0000\n",
      "Epoch [1202/2000], Evaluation Loss: 23.4004, Evaluation BLEU: 0.0000\n",
      "Epoch [1203/2000], Evaluation Loss: 23.4005, Evaluation BLEU: 0.0000\n",
      "Epoch [1204/2000], Evaluation Loss: 23.4005, Evaluation BLEU: 0.0000\n",
      "Epoch [1205/2000], Evaluation Loss: 23.4006, Evaluation BLEU: 0.0000\n",
      "Epoch [1206/2000], Evaluation Loss: 23.4007, Evaluation BLEU: 0.0000\n",
      "Epoch [1207/2000], Evaluation Loss: 23.4007, Evaluation BLEU: 0.0000\n",
      "Epoch [1208/2000], Evaluation Loss: 23.4008, Evaluation BLEU: 0.0000\n",
      "Epoch [1209/2000], Evaluation Loss: 23.4010, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1210/2000], Evaluation Loss: 23.4011, Evaluation BLEU: 0.0000\n",
      "Epoch [1211/2000], Evaluation Loss: 23.4012, Evaluation BLEU: 0.0000\n",
      "Epoch [1212/2000], Evaluation Loss: 23.4013, Evaluation BLEU: 0.0000\n",
      "Epoch [1213/2000], Evaluation Loss: 23.4014, Evaluation BLEU: 0.0000\n",
      "Epoch [1214/2000], Evaluation Loss: 23.4016, Evaluation BLEU: 0.0000\n",
      "Epoch [1215/2000], Evaluation Loss: 23.4018, Evaluation BLEU: 0.0000\n",
      "Epoch [1216/2000], Evaluation Loss: 23.4020, Evaluation BLEU: 0.0000\n",
      "Epoch [1217/2000], Evaluation Loss: 23.4022, Evaluation BLEU: 0.0000\n",
      "Epoch [1218/2000], Evaluation Loss: 23.4023, Evaluation BLEU: 0.0000\n",
      "Epoch [1219/2000], Evaluation Loss: 23.4025, Evaluation BLEU: 0.0000\n",
      "Epoch [1220/2000], Evaluation Loss: 23.4027, Evaluation BLEU: 0.0000\n",
      "Epoch [1221/2000], Evaluation Loss: 23.4029, Evaluation BLEU: 0.0000\n",
      "Epoch [1222/2000], Evaluation Loss: 23.4031, Evaluation BLEU: 0.0000\n",
      "Epoch [1223/2000], Evaluation Loss: 23.4033, Evaluation BLEU: 0.0000\n",
      "Epoch [1224/2000], Evaluation Loss: 23.4036, Evaluation BLEU: 0.0000\n",
      "Epoch [1225/2000], Evaluation Loss: 23.4038, Evaluation BLEU: 0.0000\n",
      "Epoch [1226/2000], Evaluation Loss: 23.4041, Evaluation BLEU: 0.0000\n",
      "Epoch [1227/2000], Evaluation Loss: 23.4043, Evaluation BLEU: 0.0000\n",
      "Epoch [1228/2000], Evaluation Loss: 23.4045, Evaluation BLEU: 0.0000\n",
      "Epoch [1229/2000], Evaluation Loss: 23.4048, Evaluation BLEU: 0.0000\n",
      "Epoch [1230/2000], Evaluation Loss: 23.4050, Evaluation BLEU: 0.0000\n",
      "Epoch [1231/2000], Evaluation Loss: 23.4052, Evaluation BLEU: 0.0000\n",
      "Epoch [1232/2000], Evaluation Loss: 23.4054, Evaluation BLEU: 0.0000\n",
      "Epoch [1233/2000], Evaluation Loss: 23.4055, Evaluation BLEU: 0.0000\n",
      "Epoch [1234/2000], Evaluation Loss: 23.4057, Evaluation BLEU: 0.0000\n",
      "Epoch [1235/2000], Evaluation Loss: 23.4059, Evaluation BLEU: 0.0000\n",
      "Epoch [1236/2000], Evaluation Loss: 23.4060, Evaluation BLEU: 0.0000\n",
      "Epoch [1237/2000], Evaluation Loss: 23.4062, Evaluation BLEU: 0.0000\n",
      "Epoch [1238/2000], Evaluation Loss: 23.4063, Evaluation BLEU: 0.0000\n",
      "Epoch [1239/2000], Evaluation Loss: 23.4064, Evaluation BLEU: 0.0000\n",
      "Epoch [1240/2000], Evaluation Loss: 23.4065, Evaluation BLEU: 0.0000\n",
      "Epoch [1241/2000], Evaluation Loss: 23.4067, Evaluation BLEU: 0.0000\n",
      "Epoch [1242/2000], Evaluation Loss: 23.4068, Evaluation BLEU: 0.0000\n",
      "Epoch [1243/2000], Evaluation Loss: 23.4069, Evaluation BLEU: 0.0000\n",
      "Epoch [1244/2000], Evaluation Loss: 23.4070, Evaluation BLEU: 0.0000\n",
      "Epoch [1245/2000], Evaluation Loss: 23.4071, Evaluation BLEU: 0.0000\n",
      "Epoch [1246/2000], Evaluation Loss: 23.4072, Evaluation BLEU: 0.0000\n",
      "Epoch [1247/2000], Evaluation Loss: 23.4074, Evaluation BLEU: 0.0000\n",
      "Epoch [1248/2000], Evaluation Loss: 23.4075, Evaluation BLEU: 0.0000\n",
      "Epoch [1249/2000], Evaluation Loss: 23.4077, Evaluation BLEU: 0.0000\n",
      "Epoch [1250/2000], Evaluation Loss: 23.4078, Evaluation BLEU: 0.0000\n",
      "Epoch [1251/2000], Evaluation Loss: 23.4079, Evaluation BLEU: 0.0000\n",
      "Epoch [1252/2000], Evaluation Loss: 23.4081, Evaluation BLEU: 0.0000\n",
      "Epoch [1253/2000], Evaluation Loss: 23.4083, Evaluation BLEU: 0.0000\n",
      "Epoch [1254/2000], Evaluation Loss: 23.4085, Evaluation BLEU: 0.0000\n",
      "Epoch [1255/2000], Evaluation Loss: 23.4087, Evaluation BLEU: 0.0000\n",
      "Epoch [1256/2000], Evaluation Loss: 23.4089, Evaluation BLEU: 0.0000\n",
      "Epoch [1257/2000], Evaluation Loss: 23.4090, Evaluation BLEU: 0.0000\n",
      "Epoch [1258/2000], Evaluation Loss: 23.4092, Evaluation BLEU: 0.0000\n",
      "Epoch [1259/2000], Evaluation Loss: 23.4093, Evaluation BLEU: 0.0000\n",
      "Epoch [1260/2000], Evaluation Loss: 23.4094, Evaluation BLEU: 0.0000\n",
      "Epoch [1261/2000], Evaluation Loss: 23.4096, Evaluation BLEU: 0.0000\n",
      "Epoch [1262/2000], Evaluation Loss: 23.4098, Evaluation BLEU: 0.0000\n",
      "Epoch [1263/2000], Evaluation Loss: 23.4099, Evaluation BLEU: 0.0000\n",
      "Epoch [1264/2000], Evaluation Loss: 23.4101, Evaluation BLEU: 0.0000\n",
      "Epoch [1265/2000], Evaluation Loss: 23.4103, Evaluation BLEU: 0.0000\n",
      "Epoch [1266/2000], Evaluation Loss: 23.4104, Evaluation BLEU: 0.0000\n",
      "Epoch [1267/2000], Evaluation Loss: 23.4106, Evaluation BLEU: 0.0000\n",
      "Epoch [1268/2000], Evaluation Loss: 23.4109, Evaluation BLEU: 0.0000\n",
      "Epoch [1269/2000], Evaluation Loss: 23.4111, Evaluation BLEU: 0.0000\n",
      "Epoch [1270/2000], Evaluation Loss: 23.4113, Evaluation BLEU: 0.0000\n",
      "Epoch [1271/2000], Evaluation Loss: 23.4116, Evaluation BLEU: 0.0000\n",
      "Epoch [1272/2000], Evaluation Loss: 23.4118, Evaluation BLEU: 0.0000\n",
      "Epoch [1273/2000], Evaluation Loss: 23.4121, Evaluation BLEU: 0.0000\n",
      "Epoch [1274/2000], Evaluation Loss: 23.4123, Evaluation BLEU: 0.0000\n",
      "Epoch [1275/2000], Evaluation Loss: 23.4126, Evaluation BLEU: 0.0000\n",
      "Epoch [1276/2000], Evaluation Loss: 23.4129, Evaluation BLEU: 0.0000\n",
      "Epoch [1277/2000], Evaluation Loss: 23.4132, Evaluation BLEU: 0.0000\n",
      "Epoch [1278/2000], Evaluation Loss: 23.4135, Evaluation BLEU: 0.0000\n",
      "Epoch [1279/2000], Evaluation Loss: 23.4137, Evaluation BLEU: 0.0000\n",
      "Epoch [1280/2000], Evaluation Loss: 23.4140, Evaluation BLEU: 0.0000\n",
      "Epoch [1281/2000], Evaluation Loss: 23.4142, Evaluation BLEU: 0.0000\n",
      "Epoch [1282/2000], Evaluation Loss: 23.4145, Evaluation BLEU: 0.0000\n",
      "Epoch [1283/2000], Evaluation Loss: 23.4147, Evaluation BLEU: 0.0000\n",
      "Epoch [1284/2000], Evaluation Loss: 23.4150, Evaluation BLEU: 0.0000\n",
      "Epoch [1285/2000], Evaluation Loss: 23.4152, Evaluation BLEU: 0.0000\n",
      "Epoch [1286/2000], Evaluation Loss: 23.4155, Evaluation BLEU: 0.0000\n",
      "Epoch [1287/2000], Evaluation Loss: 23.4157, Evaluation BLEU: 0.0000\n",
      "Epoch [1288/2000], Evaluation Loss: 23.4159, Evaluation BLEU: 0.0000\n",
      "Epoch [1289/2000], Evaluation Loss: 23.4160, Evaluation BLEU: 0.0000\n",
      "Epoch [1290/2000], Evaluation Loss: 23.4162, Evaluation BLEU: 0.0000\n",
      "Epoch [1291/2000], Evaluation Loss: 23.4164, Evaluation BLEU: 0.0000\n",
      "Epoch [1292/2000], Evaluation Loss: 23.4166, Evaluation BLEU: 0.0000\n",
      "Epoch [1293/2000], Evaluation Loss: 23.4168, Evaluation BLEU: 0.0000\n",
      "Epoch [1294/2000], Evaluation Loss: 23.4170, Evaluation BLEU: 0.0000\n",
      "Epoch [1295/2000], Evaluation Loss: 23.4172, Evaluation BLEU: 0.0000\n",
      "Epoch [1296/2000], Evaluation Loss: 23.4174, Evaluation BLEU: 0.0000\n",
      "Epoch [1297/2000], Evaluation Loss: 23.4176, Evaluation BLEU: 0.0000\n",
      "Epoch [1298/2000], Evaluation Loss: 23.4179, Evaluation BLEU: 0.0000\n",
      "Epoch [1299/2000], Evaluation Loss: 23.4181, Evaluation BLEU: 0.0000\n",
      "Epoch [1300/2000], Evaluation Loss: 23.4184, Evaluation BLEU: 0.0000\n",
      "Epoch [1301/2000], Evaluation Loss: 23.4187, Evaluation BLEU: 0.0000\n",
      "Epoch [1302/2000], Evaluation Loss: 23.4189, Evaluation BLEU: 0.0000\n",
      "Epoch [1303/2000], Evaluation Loss: 23.4193, Evaluation BLEU: 0.0000\n",
      "Epoch [1304/2000], Evaluation Loss: 23.4196, Evaluation BLEU: 0.0000\n",
      "Epoch [1305/2000], Evaluation Loss: 23.4200, Evaluation BLEU: 0.0000\n",
      "Epoch [1306/2000], Evaluation Loss: 23.4203, Evaluation BLEU: 0.0000\n",
      "Epoch [1307/2000], Evaluation Loss: 23.4207, Evaluation BLEU: 0.0000\n",
      "Epoch [1308/2000], Evaluation Loss: 23.4210, Evaluation BLEU: 0.0000\n",
      "Epoch [1309/2000], Evaluation Loss: 23.4212, Evaluation BLEU: 0.0000\n",
      "Epoch [1310/2000], Evaluation Loss: 23.4215, Evaluation BLEU: 0.0000\n",
      "Epoch [1311/2000], Evaluation Loss: 23.4218, Evaluation BLEU: 0.0000\n",
      "Epoch [1312/2000], Evaluation Loss: 23.4220, Evaluation BLEU: 0.0000\n",
      "Epoch [1313/2000], Evaluation Loss: 23.4223, Evaluation BLEU: 0.0000\n",
      "Epoch [1314/2000], Evaluation Loss: 23.4226, Evaluation BLEU: 0.0000\n",
      "Epoch [1315/2000], Evaluation Loss: 23.4229, Evaluation BLEU: 0.0000\n",
      "Epoch [1316/2000], Evaluation Loss: 23.4232, Evaluation BLEU: 0.0000\n",
      "Epoch [1317/2000], Evaluation Loss: 23.4235, Evaluation BLEU: 0.0000\n",
      "Epoch [1318/2000], Evaluation Loss: 23.4238, Evaluation BLEU: 0.0000\n",
      "Epoch [1319/2000], Evaluation Loss: 23.4240, Evaluation BLEU: 0.0000\n",
      "Epoch [1320/2000], Evaluation Loss: 23.4243, Evaluation BLEU: 0.0000\n",
      "Epoch [1321/2000], Evaluation Loss: 23.4245, Evaluation BLEU: 0.0000\n",
      "Epoch [1322/2000], Evaluation Loss: 23.4248, Evaluation BLEU: 0.0000\n",
      "Epoch [1323/2000], Evaluation Loss: 23.4251, Evaluation BLEU: 0.0000\n",
      "Epoch [1324/2000], Evaluation Loss: 23.4254, Evaluation BLEU: 0.0000\n",
      "Epoch [1325/2000], Evaluation Loss: 23.4256, Evaluation BLEU: 0.0000\n",
      "Epoch [1326/2000], Evaluation Loss: 23.4259, Evaluation BLEU: 0.0000\n",
      "Epoch [1327/2000], Evaluation Loss: 23.4262, Evaluation BLEU: 0.0000\n",
      "Epoch [1328/2000], Evaluation Loss: 23.4265, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1329/2000], Evaluation Loss: 23.4268, Evaluation BLEU: 0.0000\n",
      "Epoch [1330/2000], Evaluation Loss: 23.4272, Evaluation BLEU: 0.0000\n",
      "Epoch [1331/2000], Evaluation Loss: 23.4275, Evaluation BLEU: 0.0000\n",
      "Epoch [1332/2000], Evaluation Loss: 23.4278, Evaluation BLEU: 0.0000\n",
      "Epoch [1333/2000], Evaluation Loss: 23.4282, Evaluation BLEU: 0.0000\n",
      "Epoch [1334/2000], Evaluation Loss: 23.4284, Evaluation BLEU: 0.0000\n",
      "Epoch [1335/2000], Evaluation Loss: 23.4287, Evaluation BLEU: 0.0000\n",
      "Epoch [1336/2000], Evaluation Loss: 23.4290, Evaluation BLEU: 0.0000\n",
      "Epoch [1337/2000], Evaluation Loss: 23.4293, Evaluation BLEU: 0.0000\n",
      "Epoch [1338/2000], Evaluation Loss: 23.4296, Evaluation BLEU: 0.0000\n",
      "Epoch [1339/2000], Evaluation Loss: 23.4299, Evaluation BLEU: 0.0000\n",
      "Epoch [1340/2000], Evaluation Loss: 23.4301, Evaluation BLEU: 0.0000\n",
      "Epoch [1341/2000], Evaluation Loss: 23.4304, Evaluation BLEU: 0.0000\n",
      "Epoch [1342/2000], Evaluation Loss: 23.4307, Evaluation BLEU: 0.0000\n",
      "Epoch [1343/2000], Evaluation Loss: 23.4310, Evaluation BLEU: 0.0000\n",
      "Epoch [1344/2000], Evaluation Loss: 23.4313, Evaluation BLEU: 0.0000\n",
      "Epoch [1345/2000], Evaluation Loss: 23.4316, Evaluation BLEU: 0.0000\n",
      "Epoch [1346/2000], Evaluation Loss: 23.4319, Evaluation BLEU: 0.0000\n",
      "Epoch [1347/2000], Evaluation Loss: 23.4322, Evaluation BLEU: 0.0000\n",
      "Epoch [1348/2000], Evaluation Loss: 23.4325, Evaluation BLEU: 0.0000\n",
      "Epoch [1349/2000], Evaluation Loss: 23.4328, Evaluation BLEU: 0.0000\n",
      "Epoch [1350/2000], Evaluation Loss: 23.4331, Evaluation BLEU: 0.0000\n",
      "Epoch [1351/2000], Evaluation Loss: 23.4334, Evaluation BLEU: 0.0000\n",
      "Epoch [1352/2000], Evaluation Loss: 23.4336, Evaluation BLEU: 0.0000\n",
      "Epoch [1353/2000], Evaluation Loss: 23.4339, Evaluation BLEU: 0.0000\n",
      "Epoch [1354/2000], Evaluation Loss: 23.4341, Evaluation BLEU: 0.0000\n",
      "Epoch [1355/2000], Evaluation Loss: 23.4343, Evaluation BLEU: 0.0000\n",
      "Epoch [1356/2000], Evaluation Loss: 23.4346, Evaluation BLEU: 0.0000\n",
      "Epoch [1357/2000], Evaluation Loss: 23.4348, Evaluation BLEU: 0.0000\n",
      "Epoch [1358/2000], Evaluation Loss: 23.4350, Evaluation BLEU: 0.0000\n",
      "Epoch [1359/2000], Evaluation Loss: 23.4352, Evaluation BLEU: 0.0000\n",
      "Epoch [1360/2000], Evaluation Loss: 23.4354, Evaluation BLEU: 0.0000\n",
      "Epoch [1361/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1362/2000], Evaluation Loss: 23.4357, Evaluation BLEU: 0.0000\n",
      "Epoch [1363/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1364/2000], Evaluation Loss: 23.4359, Evaluation BLEU: 0.0000\n",
      "Epoch [1365/2000], Evaluation Loss: 23.4359, Evaluation BLEU: 0.0000\n",
      "Epoch [1366/2000], Evaluation Loss: 23.4360, Evaluation BLEU: 0.0000\n",
      "Epoch [1367/2000], Evaluation Loss: 23.4360, Evaluation BLEU: 0.0000\n",
      "Epoch [1368/2000], Evaluation Loss: 23.4360, Evaluation BLEU: 0.0000\n",
      "Epoch [1369/2000], Evaluation Loss: 23.4360, Evaluation BLEU: 0.0000\n",
      "Epoch [1370/2000], Evaluation Loss: 23.4360, Evaluation BLEU: 0.0000\n",
      "Epoch [1371/2000], Evaluation Loss: 23.4360, Evaluation BLEU: 0.0000\n",
      "Epoch [1372/2000], Evaluation Loss: 23.4360, Evaluation BLEU: 0.0000\n",
      "Epoch [1373/2000], Evaluation Loss: 23.4359, Evaluation BLEU: 0.0000\n",
      "Epoch [1374/2000], Evaluation Loss: 23.4359, Evaluation BLEU: 0.0000\n",
      "Epoch [1375/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1376/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1377/2000], Evaluation Loss: 23.4357, Evaluation BLEU: 0.0000\n",
      "Epoch [1378/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1379/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1380/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1381/2000], Evaluation Loss: 23.4355, Evaluation BLEU: 0.0000\n",
      "Epoch [1382/2000], Evaluation Loss: 23.4355, Evaluation BLEU: 0.0000\n",
      "Epoch [1383/2000], Evaluation Loss: 23.4355, Evaluation BLEU: 0.0000\n",
      "Epoch [1384/2000], Evaluation Loss: 23.4355, Evaluation BLEU: 0.0000\n",
      "Epoch [1385/2000], Evaluation Loss: 23.4355, Evaluation BLEU: 0.0000\n",
      "Epoch [1386/2000], Evaluation Loss: 23.4355, Evaluation BLEU: 0.0000\n",
      "Epoch [1387/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1388/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1389/2000], Evaluation Loss: 23.4357, Evaluation BLEU: 0.0000\n",
      "Epoch [1390/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1391/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1392/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1393/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1394/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1395/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1396/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1397/2000], Evaluation Loss: 23.4357, Evaluation BLEU: 0.0000\n",
      "Epoch [1398/2000], Evaluation Loss: 23.4357, Evaluation BLEU: 0.0000\n",
      "Epoch [1399/2000], Evaluation Loss: 23.4357, Evaluation BLEU: 0.0000\n",
      "Epoch [1400/2000], Evaluation Loss: 23.4357, Evaluation BLEU: 0.0000\n",
      "Epoch [1401/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1402/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1403/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1404/2000], Evaluation Loss: 23.4355, Evaluation BLEU: 0.0000\n",
      "Epoch [1405/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1406/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1407/2000], Evaluation Loss: 23.4356, Evaluation BLEU: 0.0000\n",
      "Epoch [1408/2000], Evaluation Loss: 23.4357, Evaluation BLEU: 0.0000\n",
      "Epoch [1409/2000], Evaluation Loss: 23.4358, Evaluation BLEU: 0.0000\n",
      "Epoch [1410/2000], Evaluation Loss: 23.4359, Evaluation BLEU: 0.0000\n",
      "Epoch [1411/2000], Evaluation Loss: 23.4360, Evaluation BLEU: 0.0000\n",
      "Epoch [1412/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1413/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1414/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1415/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1416/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1417/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1418/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1419/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1420/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1421/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1422/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1423/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1424/2000], Evaluation Loss: 23.4362, Evaluation BLEU: 0.0000\n",
      "Epoch [1425/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1426/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1427/2000], Evaluation Loss: 23.4361, Evaluation BLEU: 0.0000\n",
      "Epoch [1428/2000], Evaluation Loss: 23.4362, Evaluation BLEU: 0.0000\n",
      "Epoch [1429/2000], Evaluation Loss: 23.4362, Evaluation BLEU: 0.0000\n",
      "Epoch [1430/2000], Evaluation Loss: 23.4363, Evaluation BLEU: 0.0000\n",
      "Epoch [1431/2000], Evaluation Loss: 23.4363, Evaluation BLEU: 0.0000\n",
      "Epoch [1432/2000], Evaluation Loss: 23.4364, Evaluation BLEU: 0.0000\n",
      "Epoch [1433/2000], Evaluation Loss: 23.4365, Evaluation BLEU: 0.0000\n",
      "Epoch [1434/2000], Evaluation Loss: 23.4366, Evaluation BLEU: 0.0000\n",
      "Epoch [1435/2000], Evaluation Loss: 23.4367, Evaluation BLEU: 0.0000\n",
      "Epoch [1436/2000], Evaluation Loss: 23.4368, Evaluation BLEU: 0.0000\n",
      "Epoch [1437/2000], Evaluation Loss: 23.4369, Evaluation BLEU: 0.0000\n",
      "Epoch [1438/2000], Evaluation Loss: 23.4369, Evaluation BLEU: 0.0000\n",
      "Epoch [1439/2000], Evaluation Loss: 23.4369, Evaluation BLEU: 0.0000\n",
      "Epoch [1440/2000], Evaluation Loss: 23.4369, Evaluation BLEU: 0.0000\n",
      "Epoch [1441/2000], Evaluation Loss: 23.4369, Evaluation BLEU: 0.0000\n",
      "Epoch [1442/2000], Evaluation Loss: 23.4370, Evaluation BLEU: 0.0000\n",
      "Epoch [1443/2000], Evaluation Loss: 23.4370, Evaluation BLEU: 0.0000\n",
      "Epoch [1444/2000], Evaluation Loss: 23.4371, Evaluation BLEU: 0.0000\n",
      "Epoch [1445/2000], Evaluation Loss: 23.4373, Evaluation BLEU: 0.0000\n",
      "Epoch [1446/2000], Evaluation Loss: 23.4374, Evaluation BLEU: 0.0000\n",
      "Epoch [1447/2000], Evaluation Loss: 23.4376, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1448/2000], Evaluation Loss: 23.4377, Evaluation BLEU: 0.0000\n",
      "Epoch [1449/2000], Evaluation Loss: 23.4379, Evaluation BLEU: 0.0000\n",
      "Epoch [1450/2000], Evaluation Loss: 23.4380, Evaluation BLEU: 0.0000\n",
      "Epoch [1451/2000], Evaluation Loss: 23.4382, Evaluation BLEU: 0.0000\n",
      "Epoch [1452/2000], Evaluation Loss: 23.4384, Evaluation BLEU: 0.0000\n",
      "Epoch [1453/2000], Evaluation Loss: 23.4385, Evaluation BLEU: 0.0000\n",
      "Epoch [1454/2000], Evaluation Loss: 23.4387, Evaluation BLEU: 0.0000\n",
      "Epoch [1455/2000], Evaluation Loss: 23.4389, Evaluation BLEU: 0.0000\n",
      "Epoch [1456/2000], Evaluation Loss: 23.4391, Evaluation BLEU: 0.0000\n",
      "Epoch [1457/2000], Evaluation Loss: 23.4393, Evaluation BLEU: 0.0000\n",
      "Epoch [1458/2000], Evaluation Loss: 23.4395, Evaluation BLEU: 0.0000\n",
      "Epoch [1459/2000], Evaluation Loss: 23.4397, Evaluation BLEU: 0.0000\n",
      "Epoch [1460/2000], Evaluation Loss: 23.4398, Evaluation BLEU: 0.0000\n",
      "Epoch [1461/2000], Evaluation Loss: 23.4400, Evaluation BLEU: 0.0000\n",
      "Epoch [1462/2000], Evaluation Loss: 23.4401, Evaluation BLEU: 0.0000\n",
      "Epoch [1463/2000], Evaluation Loss: 23.4402, Evaluation BLEU: 0.0000\n",
      "Epoch [1464/2000], Evaluation Loss: 23.4403, Evaluation BLEU: 0.0000\n",
      "Epoch [1465/2000], Evaluation Loss: 23.4404, Evaluation BLEU: 0.0000\n",
      "Epoch [1466/2000], Evaluation Loss: 23.4406, Evaluation BLEU: 0.0000\n",
      "Epoch [1467/2000], Evaluation Loss: 23.4407, Evaluation BLEU: 0.0000\n",
      "Epoch [1468/2000], Evaluation Loss: 23.4409, Evaluation BLEU: 0.0000\n",
      "Epoch [1469/2000], Evaluation Loss: 23.4410, Evaluation BLEU: 0.0000\n",
      "Epoch [1470/2000], Evaluation Loss: 23.4412, Evaluation BLEU: 0.0000\n",
      "Epoch [1471/2000], Evaluation Loss: 23.4413, Evaluation BLEU: 0.0000\n",
      "Epoch [1472/2000], Evaluation Loss: 23.4414, Evaluation BLEU: 0.0000\n",
      "Epoch [1473/2000], Evaluation Loss: 23.4415, Evaluation BLEU: 0.0000\n",
      "Epoch [1474/2000], Evaluation Loss: 23.4415, Evaluation BLEU: 0.0000\n",
      "Epoch [1475/2000], Evaluation Loss: 23.4415, Evaluation BLEU: 0.0000\n",
      "Epoch [1476/2000], Evaluation Loss: 23.4415, Evaluation BLEU: 0.0000\n",
      "Epoch [1477/2000], Evaluation Loss: 23.4414, Evaluation BLEU: 0.0000\n",
      "Epoch [1478/2000], Evaluation Loss: 23.4414, Evaluation BLEU: 0.0000\n",
      "Epoch [1479/2000], Evaluation Loss: 23.4413, Evaluation BLEU: 0.0000\n",
      "Epoch [1480/2000], Evaluation Loss: 23.4413, Evaluation BLEU: 0.0000\n",
      "Epoch [1481/2000], Evaluation Loss: 23.4412, Evaluation BLEU: 0.0000\n",
      "Epoch [1482/2000], Evaluation Loss: 23.4412, Evaluation BLEU: 0.0000\n",
      "Epoch [1483/2000], Evaluation Loss: 23.4412, Evaluation BLEU: 0.0000\n",
      "Epoch [1484/2000], Evaluation Loss: 23.4413, Evaluation BLEU: 0.0000\n",
      "Epoch [1485/2000], Evaluation Loss: 23.4413, Evaluation BLEU: 0.0000\n",
      "Epoch [1486/2000], Evaluation Loss: 23.4414, Evaluation BLEU: 0.0000\n",
      "Epoch [1487/2000], Evaluation Loss: 23.4414, Evaluation BLEU: 0.0000\n",
      "Epoch [1488/2000], Evaluation Loss: 23.4414, Evaluation BLEU: 0.0000\n",
      "Epoch [1489/2000], Evaluation Loss: 23.4414, Evaluation BLEU: 0.0000\n",
      "Epoch [1490/2000], Evaluation Loss: 23.4415, Evaluation BLEU: 0.0000\n",
      "Epoch [1491/2000], Evaluation Loss: 23.4415, Evaluation BLEU: 0.0000\n",
      "Epoch [1492/2000], Evaluation Loss: 23.4416, Evaluation BLEU: 0.0000\n",
      "Epoch [1493/2000], Evaluation Loss: 23.4416, Evaluation BLEU: 0.0000\n",
      "Epoch [1494/2000], Evaluation Loss: 23.4417, Evaluation BLEU: 0.0000\n",
      "Epoch [1495/2000], Evaluation Loss: 23.4419, Evaluation BLEU: 0.0000\n",
      "Epoch [1496/2000], Evaluation Loss: 23.4420, Evaluation BLEU: 0.0000\n",
      "Epoch [1497/2000], Evaluation Loss: 23.4422, Evaluation BLEU: 0.0000\n",
      "Epoch [1498/2000], Evaluation Loss: 23.4423, Evaluation BLEU: 0.0000\n",
      "Epoch [1499/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1500/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1501/2000], Evaluation Loss: 23.4427, Evaluation BLEU: 0.0000\n",
      "Epoch [1502/2000], Evaluation Loss: 23.4429, Evaluation BLEU: 0.0000\n",
      "Epoch [1503/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1504/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1505/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1506/2000], Evaluation Loss: 23.4432, Evaluation BLEU: 0.0000\n",
      "Epoch [1507/2000], Evaluation Loss: 23.4432, Evaluation BLEU: 0.0000\n",
      "Epoch [1508/2000], Evaluation Loss: 23.4432, Evaluation BLEU: 0.0000\n",
      "Epoch [1509/2000], Evaluation Loss: 23.4432, Evaluation BLEU: 0.0000\n",
      "Epoch [1510/2000], Evaluation Loss: 23.4432, Evaluation BLEU: 0.0000\n",
      "Epoch [1511/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1512/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1513/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1514/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1515/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1516/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1517/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1518/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1519/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1520/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1521/2000], Evaluation Loss: 23.4429, Evaluation BLEU: 0.0000\n",
      "Epoch [1522/2000], Evaluation Loss: 23.4429, Evaluation BLEU: 0.0000\n",
      "Epoch [1523/2000], Evaluation Loss: 23.4428, Evaluation BLEU: 0.0000\n",
      "Epoch [1524/2000], Evaluation Loss: 23.4428, Evaluation BLEU: 0.0000\n",
      "Epoch [1525/2000], Evaluation Loss: 23.4427, Evaluation BLEU: 0.0000\n",
      "Epoch [1526/2000], Evaluation Loss: 23.4427, Evaluation BLEU: 0.0000\n",
      "Epoch [1527/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1528/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1529/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1530/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1531/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1532/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1533/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1534/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1535/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1536/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1537/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1538/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1539/2000], Evaluation Loss: 23.4425, Evaluation BLEU: 0.0000\n",
      "Epoch [1540/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1541/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1542/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1543/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1544/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1545/2000], Evaluation Loss: 23.4426, Evaluation BLEU: 0.0000\n",
      "Epoch [1546/2000], Evaluation Loss: 23.4427, Evaluation BLEU: 0.0000\n",
      "Epoch [1547/2000], Evaluation Loss: 23.4427, Evaluation BLEU: 0.0000\n",
      "Epoch [1548/2000], Evaluation Loss: 23.4428, Evaluation BLEU: 0.0000\n",
      "Epoch [1549/2000], Evaluation Loss: 23.4428, Evaluation BLEU: 0.0000\n",
      "Epoch [1550/2000], Evaluation Loss: 23.4429, Evaluation BLEU: 0.0000\n",
      "Epoch [1551/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1552/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1553/2000], Evaluation Loss: 23.4430, Evaluation BLEU: 0.0000\n",
      "Epoch [1554/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1555/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1556/2000], Evaluation Loss: 23.4431, Evaluation BLEU: 0.0000\n",
      "Epoch [1557/2000], Evaluation Loss: 23.4432, Evaluation BLEU: 0.0000\n",
      "Epoch [1558/2000], Evaluation Loss: 23.4434, Evaluation BLEU: 0.0000\n",
      "Epoch [1559/2000], Evaluation Loss: 23.4435, Evaluation BLEU: 0.0000\n",
      "Epoch [1560/2000], Evaluation Loss: 23.4437, Evaluation BLEU: 0.0000\n",
      "Epoch [1561/2000], Evaluation Loss: 23.4439, Evaluation BLEU: 0.0000\n",
      "Epoch [1562/2000], Evaluation Loss: 23.4441, Evaluation BLEU: 0.0000\n",
      "Epoch [1563/2000], Evaluation Loss: 23.4444, Evaluation BLEU: 0.0000\n",
      "Epoch [1564/2000], Evaluation Loss: 23.4447, Evaluation BLEU: 0.0000\n",
      "Epoch [1565/2000], Evaluation Loss: 23.4449, Evaluation BLEU: 0.0000\n",
      "Epoch [1566/2000], Evaluation Loss: 23.4452, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1567/2000], Evaluation Loss: 23.4455, Evaluation BLEU: 0.0000\n",
      "Epoch [1568/2000], Evaluation Loss: 23.4458, Evaluation BLEU: 0.0000\n",
      "Epoch [1569/2000], Evaluation Loss: 23.4461, Evaluation BLEU: 0.0000\n",
      "Epoch [1570/2000], Evaluation Loss: 23.4463, Evaluation BLEU: 0.0000\n",
      "Epoch [1571/2000], Evaluation Loss: 23.4465, Evaluation BLEU: 0.0000\n",
      "Epoch [1572/2000], Evaluation Loss: 23.4467, Evaluation BLEU: 0.0000\n",
      "Epoch [1573/2000], Evaluation Loss: 23.4468, Evaluation BLEU: 0.0000\n",
      "Epoch [1574/2000], Evaluation Loss: 23.4470, Evaluation BLEU: 0.0000\n",
      "Epoch [1575/2000], Evaluation Loss: 23.4471, Evaluation BLEU: 0.0000\n",
      "Epoch [1576/2000], Evaluation Loss: 23.4473, Evaluation BLEU: 0.0000\n",
      "Epoch [1577/2000], Evaluation Loss: 23.4476, Evaluation BLEU: 0.0000\n",
      "Epoch [1578/2000], Evaluation Loss: 23.4478, Evaluation BLEU: 0.0000\n",
      "Epoch [1579/2000], Evaluation Loss: 23.4481, Evaluation BLEU: 0.0000\n",
      "Epoch [1580/2000], Evaluation Loss: 23.4483, Evaluation BLEU: 0.0000\n",
      "Epoch [1581/2000], Evaluation Loss: 23.4486, Evaluation BLEU: 0.0000\n",
      "Epoch [1582/2000], Evaluation Loss: 23.4489, Evaluation BLEU: 0.0000\n",
      "Epoch [1583/2000], Evaluation Loss: 23.4491, Evaluation BLEU: 0.0000\n",
      "Epoch [1584/2000], Evaluation Loss: 23.4493, Evaluation BLEU: 0.0000\n",
      "Epoch [1585/2000], Evaluation Loss: 23.4495, Evaluation BLEU: 0.0000\n",
      "Epoch [1586/2000], Evaluation Loss: 23.4496, Evaluation BLEU: 0.0000\n",
      "Epoch [1587/2000], Evaluation Loss: 23.4497, Evaluation BLEU: 0.0000\n",
      "Epoch [1588/2000], Evaluation Loss: 23.4498, Evaluation BLEU: 0.0000\n",
      "Epoch [1589/2000], Evaluation Loss: 23.4500, Evaluation BLEU: 0.0000\n",
      "Epoch [1590/2000], Evaluation Loss: 23.4502, Evaluation BLEU: 0.0000\n",
      "Epoch [1591/2000], Evaluation Loss: 23.4503, Evaluation BLEU: 0.0000\n",
      "Epoch [1592/2000], Evaluation Loss: 23.4505, Evaluation BLEU: 0.0000\n",
      "Epoch [1593/2000], Evaluation Loss: 23.4507, Evaluation BLEU: 0.0000\n",
      "Epoch [1594/2000], Evaluation Loss: 23.4510, Evaluation BLEU: 0.0000\n",
      "Epoch [1595/2000], Evaluation Loss: 23.4512, Evaluation BLEU: 0.0000\n",
      "Epoch [1596/2000], Evaluation Loss: 23.4514, Evaluation BLEU: 0.0000\n",
      "Epoch [1597/2000], Evaluation Loss: 23.4516, Evaluation BLEU: 0.0000\n",
      "Epoch [1598/2000], Evaluation Loss: 23.4518, Evaluation BLEU: 0.0000\n",
      "Epoch [1599/2000], Evaluation Loss: 23.4519, Evaluation BLEU: 0.0000\n",
      "Epoch [1600/2000], Evaluation Loss: 23.4520, Evaluation BLEU: 0.0000\n",
      "Epoch [1601/2000], Evaluation Loss: 23.4521, Evaluation BLEU: 0.0000\n",
      "Epoch [1602/2000], Evaluation Loss: 23.4522, Evaluation BLEU: 0.0000\n",
      "Epoch [1603/2000], Evaluation Loss: 23.4523, Evaluation BLEU: 0.0000\n",
      "Epoch [1604/2000], Evaluation Loss: 23.4524, Evaluation BLEU: 0.0000\n",
      "Epoch [1605/2000], Evaluation Loss: 23.4524, Evaluation BLEU: 0.0000\n",
      "Epoch [1606/2000], Evaluation Loss: 23.4524, Evaluation BLEU: 0.0000\n",
      "Epoch [1607/2000], Evaluation Loss: 23.4525, Evaluation BLEU: 0.0000\n",
      "Epoch [1608/2000], Evaluation Loss: 23.4526, Evaluation BLEU: 0.0000\n",
      "Epoch [1609/2000], Evaluation Loss: 23.4527, Evaluation BLEU: 0.0000\n",
      "Epoch [1610/2000], Evaluation Loss: 23.4528, Evaluation BLEU: 0.0000\n",
      "Epoch [1611/2000], Evaluation Loss: 23.4530, Evaluation BLEU: 0.0000\n",
      "Epoch [1612/2000], Evaluation Loss: 23.4532, Evaluation BLEU: 0.0000\n",
      "Epoch [1613/2000], Evaluation Loss: 23.4534, Evaluation BLEU: 0.0000\n",
      "Epoch [1614/2000], Evaluation Loss: 23.4537, Evaluation BLEU: 0.0000\n",
      "Epoch [1615/2000], Evaluation Loss: 23.4540, Evaluation BLEU: 0.0000\n",
      "Epoch [1616/2000], Evaluation Loss: 23.4542, Evaluation BLEU: 0.0000\n",
      "Epoch [1617/2000], Evaluation Loss: 23.4545, Evaluation BLEU: 0.0000\n",
      "Epoch [1618/2000], Evaluation Loss: 23.4547, Evaluation BLEU: 0.0000\n",
      "Epoch [1619/2000], Evaluation Loss: 23.4548, Evaluation BLEU: 0.0000\n",
      "Epoch [1620/2000], Evaluation Loss: 23.4549, Evaluation BLEU: 0.0000\n",
      "Epoch [1621/2000], Evaluation Loss: 23.4550, Evaluation BLEU: 0.0000\n",
      "Epoch [1622/2000], Evaluation Loss: 23.4550, Evaluation BLEU: 0.0000\n",
      "Epoch [1623/2000], Evaluation Loss: 23.4551, Evaluation BLEU: 0.0000\n",
      "Epoch [1624/2000], Evaluation Loss: 23.4551, Evaluation BLEU: 0.0000\n",
      "Epoch [1625/2000], Evaluation Loss: 23.4551, Evaluation BLEU: 0.0000\n",
      "Epoch [1626/2000], Evaluation Loss: 23.4552, Evaluation BLEU: 0.0000\n",
      "Epoch [1627/2000], Evaluation Loss: 23.4552, Evaluation BLEU: 0.0000\n",
      "Epoch [1628/2000], Evaluation Loss: 23.4553, Evaluation BLEU: 0.0000\n",
      "Epoch [1629/2000], Evaluation Loss: 23.4554, Evaluation BLEU: 0.0000\n",
      "Epoch [1630/2000], Evaluation Loss: 23.4555, Evaluation BLEU: 0.0000\n",
      "Epoch [1631/2000], Evaluation Loss: 23.4556, Evaluation BLEU: 0.0000\n",
      "Epoch [1632/2000], Evaluation Loss: 23.4558, Evaluation BLEU: 0.0000\n",
      "Epoch [1633/2000], Evaluation Loss: 23.4560, Evaluation BLEU: 0.0000\n",
      "Epoch [1634/2000], Evaluation Loss: 23.4561, Evaluation BLEU: 0.0000\n",
      "Epoch [1635/2000], Evaluation Loss: 23.4563, Evaluation BLEU: 0.0000\n",
      "Epoch [1636/2000], Evaluation Loss: 23.4564, Evaluation BLEU: 0.0000\n",
      "Epoch [1637/2000], Evaluation Loss: 23.4566, Evaluation BLEU: 0.0000\n",
      "Epoch [1638/2000], Evaluation Loss: 23.4567, Evaluation BLEU: 0.0000\n",
      "Epoch [1639/2000], Evaluation Loss: 23.4568, Evaluation BLEU: 0.0000\n",
      "Epoch [1640/2000], Evaluation Loss: 23.4569, Evaluation BLEU: 0.0000\n",
      "Epoch [1641/2000], Evaluation Loss: 23.4570, Evaluation BLEU: 0.0000\n",
      "Epoch [1642/2000], Evaluation Loss: 23.4570, Evaluation BLEU: 0.0000\n",
      "Epoch [1643/2000], Evaluation Loss: 23.4571, Evaluation BLEU: 0.0000\n",
      "Epoch [1644/2000], Evaluation Loss: 23.4571, Evaluation BLEU: 0.0000\n",
      "Epoch [1645/2000], Evaluation Loss: 23.4572, Evaluation BLEU: 0.0000\n",
      "Epoch [1646/2000], Evaluation Loss: 23.4572, Evaluation BLEU: 0.0000\n",
      "Epoch [1647/2000], Evaluation Loss: 23.4572, Evaluation BLEU: 0.0000\n",
      "Epoch [1648/2000], Evaluation Loss: 23.4573, Evaluation BLEU: 0.0000\n",
      "Epoch [1649/2000], Evaluation Loss: 23.4574, Evaluation BLEU: 0.0000\n",
      "Epoch [1650/2000], Evaluation Loss: 23.4575, Evaluation BLEU: 0.0000\n",
      "Epoch [1651/2000], Evaluation Loss: 23.4577, Evaluation BLEU: 0.0000\n",
      "Epoch [1652/2000], Evaluation Loss: 23.4578, Evaluation BLEU: 0.0000\n",
      "Epoch [1653/2000], Evaluation Loss: 23.4580, Evaluation BLEU: 0.0000\n",
      "Epoch [1654/2000], Evaluation Loss: 23.4582, Evaluation BLEU: 0.0000\n",
      "Epoch [1655/2000], Evaluation Loss: 23.4583, Evaluation BLEU: 0.0000\n",
      "Epoch [1656/2000], Evaluation Loss: 23.4584, Evaluation BLEU: 0.0000\n",
      "Epoch [1657/2000], Evaluation Loss: 23.4584, Evaluation BLEU: 0.0000\n",
      "Epoch [1658/2000], Evaluation Loss: 23.4585, Evaluation BLEU: 0.0000\n",
      "Epoch [1659/2000], Evaluation Loss: 23.4586, Evaluation BLEU: 0.0000\n",
      "Epoch [1660/2000], Evaluation Loss: 23.4587, Evaluation BLEU: 0.0000\n",
      "Epoch [1661/2000], Evaluation Loss: 23.4587, Evaluation BLEU: 0.0000\n",
      "Epoch [1662/2000], Evaluation Loss: 23.4588, Evaluation BLEU: 0.0000\n",
      "Epoch [1663/2000], Evaluation Loss: 23.4590, Evaluation BLEU: 0.0000\n",
      "Epoch [1664/2000], Evaluation Loss: 23.4590, Evaluation BLEU: 0.0000\n",
      "Epoch [1665/2000], Evaluation Loss: 23.4591, Evaluation BLEU: 0.0000\n",
      "Epoch [1666/2000], Evaluation Loss: 23.4592, Evaluation BLEU: 0.0000\n",
      "Epoch [1667/2000], Evaluation Loss: 23.4592, Evaluation BLEU: 0.0000\n",
      "Epoch [1668/2000], Evaluation Loss: 23.4593, Evaluation BLEU: 0.0000\n",
      "Epoch [1669/2000], Evaluation Loss: 23.4594, Evaluation BLEU: 0.0000\n",
      "Epoch [1670/2000], Evaluation Loss: 23.4594, Evaluation BLEU: 0.0000\n",
      "Epoch [1671/2000], Evaluation Loss: 23.4595, Evaluation BLEU: 0.0000\n",
      "Epoch [1672/2000], Evaluation Loss: 23.4596, Evaluation BLEU: 0.0000\n",
      "Epoch [1673/2000], Evaluation Loss: 23.4597, Evaluation BLEU: 0.0000\n",
      "Epoch [1674/2000], Evaluation Loss: 23.4598, Evaluation BLEU: 0.0000\n",
      "Epoch [1675/2000], Evaluation Loss: 23.4600, Evaluation BLEU: 0.0000\n",
      "Epoch [1676/2000], Evaluation Loss: 23.4602, Evaluation BLEU: 0.0000\n",
      "Epoch [1677/2000], Evaluation Loss: 23.4603, Evaluation BLEU: 0.0000\n",
      "Epoch [1678/2000], Evaluation Loss: 23.4604, Evaluation BLEU: 0.0000\n",
      "Epoch [1679/2000], Evaluation Loss: 23.4605, Evaluation BLEU: 0.0000\n",
      "Epoch [1680/2000], Evaluation Loss: 23.4605, Evaluation BLEU: 0.0000\n",
      "Epoch [1681/2000], Evaluation Loss: 23.4605, Evaluation BLEU: 0.0000\n",
      "Epoch [1682/2000], Evaluation Loss: 23.4604, Evaluation BLEU: 0.0000\n",
      "Epoch [1683/2000], Evaluation Loss: 23.4604, Evaluation BLEU: 0.0000\n",
      "Epoch [1684/2000], Evaluation Loss: 23.4604, Evaluation BLEU: 0.0000\n",
      "Epoch [1685/2000], Evaluation Loss: 23.4604, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1686/2000], Evaluation Loss: 23.4604, Evaluation BLEU: 0.0000\n",
      "Epoch [1687/2000], Evaluation Loss: 23.4604, Evaluation BLEU: 0.0000\n",
      "Epoch [1688/2000], Evaluation Loss: 23.4604, Evaluation BLEU: 0.0000\n",
      "Epoch [1689/2000], Evaluation Loss: 23.4605, Evaluation BLEU: 0.0000\n",
      "Epoch [1690/2000], Evaluation Loss: 23.4606, Evaluation BLEU: 0.0000\n",
      "Epoch [1691/2000], Evaluation Loss: 23.4607, Evaluation BLEU: 0.0000\n",
      "Epoch [1692/2000], Evaluation Loss: 23.4608, Evaluation BLEU: 0.0000\n",
      "Epoch [1693/2000], Evaluation Loss: 23.4609, Evaluation BLEU: 0.0000\n",
      "Epoch [1694/2000], Evaluation Loss: 23.4611, Evaluation BLEU: 0.0000\n",
      "Epoch [1695/2000], Evaluation Loss: 23.4612, Evaluation BLEU: 0.0000\n",
      "Epoch [1696/2000], Evaluation Loss: 23.4614, Evaluation BLEU: 0.0000\n",
      "Epoch [1697/2000], Evaluation Loss: 23.4617, Evaluation BLEU: 0.0000\n",
      "Epoch [1698/2000], Evaluation Loss: 23.4619, Evaluation BLEU: 0.0000\n",
      "Epoch [1699/2000], Evaluation Loss: 23.4621, Evaluation BLEU: 0.0000\n",
      "Epoch [1700/2000], Evaluation Loss: 23.4623, Evaluation BLEU: 0.0000\n",
      "Epoch [1701/2000], Evaluation Loss: 23.4626, Evaluation BLEU: 0.0000\n",
      "Epoch [1702/2000], Evaluation Loss: 23.4628, Evaluation BLEU: 0.0000\n",
      "Epoch [1703/2000], Evaluation Loss: 23.4630, Evaluation BLEU: 0.0000\n",
      "Epoch [1704/2000], Evaluation Loss: 23.4631, Evaluation BLEU: 0.0000\n",
      "Epoch [1705/2000], Evaluation Loss: 23.4633, Evaluation BLEU: 0.0000\n",
      "Epoch [1706/2000], Evaluation Loss: 23.4635, Evaluation BLEU: 0.0000\n",
      "Epoch [1707/2000], Evaluation Loss: 23.4637, Evaluation BLEU: 0.0000\n",
      "Epoch [1708/2000], Evaluation Loss: 23.4639, Evaluation BLEU: 0.0000\n",
      "Epoch [1709/2000], Evaluation Loss: 23.4641, Evaluation BLEU: 0.0000\n",
      "Epoch [1710/2000], Evaluation Loss: 23.4643, Evaluation BLEU: 0.0000\n",
      "Epoch [1711/2000], Evaluation Loss: 23.4644, Evaluation BLEU: 0.0000\n",
      "Epoch [1712/2000], Evaluation Loss: 23.4645, Evaluation BLEU: 0.0000\n",
      "Epoch [1713/2000], Evaluation Loss: 23.4647, Evaluation BLEU: 0.0000\n",
      "Epoch [1714/2000], Evaluation Loss: 23.4649, Evaluation BLEU: 0.0000\n",
      "Epoch [1715/2000], Evaluation Loss: 23.4650, Evaluation BLEU: 0.0000\n",
      "Epoch [1716/2000], Evaluation Loss: 23.4651, Evaluation BLEU: 0.0000\n",
      "Epoch [1717/2000], Evaluation Loss: 23.4652, Evaluation BLEU: 0.0000\n",
      "Epoch [1718/2000], Evaluation Loss: 23.4654, Evaluation BLEU: 0.0000\n",
      "Epoch [1719/2000], Evaluation Loss: 23.4655, Evaluation BLEU: 0.0000\n",
      "Epoch [1720/2000], Evaluation Loss: 23.4656, Evaluation BLEU: 0.0000\n",
      "Epoch [1721/2000], Evaluation Loss: 23.4657, Evaluation BLEU: 0.0000\n",
      "Epoch [1722/2000], Evaluation Loss: 23.4659, Evaluation BLEU: 0.0000\n",
      "Epoch [1723/2000], Evaluation Loss: 23.4660, Evaluation BLEU: 0.0000\n",
      "Epoch [1724/2000], Evaluation Loss: 23.4661, Evaluation BLEU: 0.0000\n",
      "Epoch [1725/2000], Evaluation Loss: 23.4662, Evaluation BLEU: 0.0000\n",
      "Epoch [1726/2000], Evaluation Loss: 23.4663, Evaluation BLEU: 0.0000\n",
      "Epoch [1727/2000], Evaluation Loss: 23.4663, Evaluation BLEU: 0.0000\n",
      "Epoch [1728/2000], Evaluation Loss: 23.4662, Evaluation BLEU: 0.0000\n",
      "Epoch [1729/2000], Evaluation Loss: 23.4661, Evaluation BLEU: 0.0000\n",
      "Epoch [1730/2000], Evaluation Loss: 23.4659, Evaluation BLEU: 0.0000\n",
      "Epoch [1731/2000], Evaluation Loss: 23.4657, Evaluation BLEU: 0.0000\n",
      "Epoch [1732/2000], Evaluation Loss: 23.4655, Evaluation BLEU: 0.0000\n",
      "Epoch [1733/2000], Evaluation Loss: 23.4653, Evaluation BLEU: 0.0000\n",
      "Epoch [1734/2000], Evaluation Loss: 23.4651, Evaluation BLEU: 0.0000\n",
      "Epoch [1735/2000], Evaluation Loss: 23.4649, Evaluation BLEU: 0.0000\n",
      "Epoch [1736/2000], Evaluation Loss: 23.4648, Evaluation BLEU: 0.0000\n",
      "Epoch [1737/2000], Evaluation Loss: 23.4647, Evaluation BLEU: 0.0000\n",
      "Epoch [1738/2000], Evaluation Loss: 23.4647, Evaluation BLEU: 0.0000\n",
      "Epoch [1739/2000], Evaluation Loss: 23.4646, Evaluation BLEU: 0.0000\n",
      "Epoch [1740/2000], Evaluation Loss: 23.4646, Evaluation BLEU: 0.0000\n",
      "Epoch [1741/2000], Evaluation Loss: 23.4646, Evaluation BLEU: 0.0000\n",
      "Epoch [1742/2000], Evaluation Loss: 23.4646, Evaluation BLEU: 0.0000\n",
      "Epoch [1743/2000], Evaluation Loss: 23.4647, Evaluation BLEU: 0.0000\n",
      "Epoch [1744/2000], Evaluation Loss: 23.4648, Evaluation BLEU: 0.0000\n",
      "Epoch [1745/2000], Evaluation Loss: 23.4650, Evaluation BLEU: 0.0000\n",
      "Epoch [1746/2000], Evaluation Loss: 23.4652, Evaluation BLEU: 0.0000\n",
      "Epoch [1747/2000], Evaluation Loss: 23.4654, Evaluation BLEU: 0.0000\n",
      "Epoch [1748/2000], Evaluation Loss: 23.4656, Evaluation BLEU: 0.0000\n",
      "Epoch [1749/2000], Evaluation Loss: 23.4658, Evaluation BLEU: 0.0000\n",
      "Epoch [1750/2000], Evaluation Loss: 23.4661, Evaluation BLEU: 0.0000\n",
      "Epoch [1751/2000], Evaluation Loss: 23.4663, Evaluation BLEU: 0.0000\n",
      "Epoch [1752/2000], Evaluation Loss: 23.4666, Evaluation BLEU: 0.0000\n",
      "Epoch [1753/2000], Evaluation Loss: 23.4669, Evaluation BLEU: 0.0000\n",
      "Epoch [1754/2000], Evaluation Loss: 23.4673, Evaluation BLEU: 0.0000\n",
      "Epoch [1755/2000], Evaluation Loss: 23.4676, Evaluation BLEU: 0.0000\n",
      "Epoch [1756/2000], Evaluation Loss: 23.4680, Evaluation BLEU: 0.0000\n",
      "Epoch [1757/2000], Evaluation Loss: 23.4684, Evaluation BLEU: 0.0000\n",
      "Epoch [1758/2000], Evaluation Loss: 23.4687, Evaluation BLEU: 0.0000\n",
      "Epoch [1759/2000], Evaluation Loss: 23.4691, Evaluation BLEU: 0.0000\n",
      "Epoch [1760/2000], Evaluation Loss: 23.4695, Evaluation BLEU: 0.0000\n",
      "Epoch [1761/2000], Evaluation Loss: 23.4700, Evaluation BLEU: 0.0000\n",
      "Epoch [1762/2000], Evaluation Loss: 23.4704, Evaluation BLEU: 0.0000\n",
      "Epoch [1763/2000], Evaluation Loss: 23.4709, Evaluation BLEU: 0.0000\n",
      "Epoch [1764/2000], Evaluation Loss: 23.4714, Evaluation BLEU: 0.0000\n",
      "Epoch [1765/2000], Evaluation Loss: 23.4719, Evaluation BLEU: 0.0000\n",
      "Epoch [1766/2000], Evaluation Loss: 23.4724, Evaluation BLEU: 0.0000\n",
      "Epoch [1767/2000], Evaluation Loss: 23.4730, Evaluation BLEU: 0.0000\n",
      "Epoch [1768/2000], Evaluation Loss: 23.4736, Evaluation BLEU: 0.0000\n",
      "Epoch [1769/2000], Evaluation Loss: 23.4742, Evaluation BLEU: 0.0000\n",
      "Epoch [1770/2000], Evaluation Loss: 23.4748, Evaluation BLEU: 0.0000\n",
      "Epoch [1771/2000], Evaluation Loss: 23.4755, Evaluation BLEU: 0.0000\n",
      "Epoch [1772/2000], Evaluation Loss: 23.4762, Evaluation BLEU: 0.0000\n",
      "Epoch [1773/2000], Evaluation Loss: 23.4769, Evaluation BLEU: 0.0000\n",
      "Epoch [1774/2000], Evaluation Loss: 23.4776, Evaluation BLEU: 0.0000\n",
      "Epoch [1775/2000], Evaluation Loss: 23.4782, Evaluation BLEU: 0.0000\n",
      "Epoch [1776/2000], Evaluation Loss: 23.4789, Evaluation BLEU: 0.0000\n",
      "Epoch [1777/2000], Evaluation Loss: 23.4795, Evaluation BLEU: 0.0000\n",
      "Epoch [1778/2000], Evaluation Loss: 23.4802, Evaluation BLEU: 0.0000\n",
      "Epoch [1779/2000], Evaluation Loss: 23.4807, Evaluation BLEU: 0.0000\n",
      "Epoch [1780/2000], Evaluation Loss: 23.4813, Evaluation BLEU: 0.0000\n",
      "Epoch [1781/2000], Evaluation Loss: 23.4817, Evaluation BLEU: 0.0000\n",
      "Epoch [1782/2000], Evaluation Loss: 23.4822, Evaluation BLEU: 0.0000\n",
      "Epoch [1783/2000], Evaluation Loss: 23.4826, Evaluation BLEU: 0.0000\n",
      "Epoch [1784/2000], Evaluation Loss: 23.4829, Evaluation BLEU: 0.0000\n",
      "Epoch [1785/2000], Evaluation Loss: 23.4832, Evaluation BLEU: 0.0000\n",
      "Epoch [1786/2000], Evaluation Loss: 23.4835, Evaluation BLEU: 0.0000\n",
      "Epoch [1787/2000], Evaluation Loss: 23.4837, Evaluation BLEU: 0.0000\n",
      "Epoch [1788/2000], Evaluation Loss: 23.4839, Evaluation BLEU: 0.0000\n",
      "Epoch [1789/2000], Evaluation Loss: 23.4841, Evaluation BLEU: 0.0000\n",
      "Epoch [1790/2000], Evaluation Loss: 23.4843, Evaluation BLEU: 0.0000\n",
      "Epoch [1791/2000], Evaluation Loss: 23.4844, Evaluation BLEU: 0.0000\n",
      "Epoch [1792/2000], Evaluation Loss: 23.4845, Evaluation BLEU: 0.0000\n",
      "Epoch [1793/2000], Evaluation Loss: 23.4847, Evaluation BLEU: 0.0000\n",
      "Epoch [1794/2000], Evaluation Loss: 23.4848, Evaluation BLEU: 0.0000\n",
      "Epoch [1795/2000], Evaluation Loss: 23.4849, Evaluation BLEU: 0.0000\n",
      "Epoch [1796/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1797/2000], Evaluation Loss: 23.4851, Evaluation BLEU: 0.0000\n",
      "Epoch [1798/2000], Evaluation Loss: 23.4852, Evaluation BLEU: 0.0000\n",
      "Epoch [1799/2000], Evaluation Loss: 23.4853, Evaluation BLEU: 0.0000\n",
      "Epoch [1800/2000], Evaluation Loss: 23.4855, Evaluation BLEU: 0.0000\n",
      "Epoch [1801/2000], Evaluation Loss: 23.4856, Evaluation BLEU: 0.0000\n",
      "Epoch [1802/2000], Evaluation Loss: 23.4858, Evaluation BLEU: 0.0000\n",
      "Epoch [1803/2000], Evaluation Loss: 23.4860, Evaluation BLEU: 0.0000\n",
      "Epoch [1804/2000], Evaluation Loss: 23.4861, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1805/2000], Evaluation Loss: 23.4862, Evaluation BLEU: 0.0000\n",
      "Epoch [1806/2000], Evaluation Loss: 23.4863, Evaluation BLEU: 0.0000\n",
      "Epoch [1807/2000], Evaluation Loss: 23.4864, Evaluation BLEU: 0.0000\n",
      "Epoch [1808/2000], Evaluation Loss: 23.4864, Evaluation BLEU: 0.0000\n",
      "Epoch [1809/2000], Evaluation Loss: 23.4864, Evaluation BLEU: 0.0000\n",
      "Epoch [1810/2000], Evaluation Loss: 23.4863, Evaluation BLEU: 0.0000\n",
      "Epoch [1811/2000], Evaluation Loss: 23.4862, Evaluation BLEU: 0.0000\n",
      "Epoch [1812/2000], Evaluation Loss: 23.4861, Evaluation BLEU: 0.0000\n",
      "Epoch [1813/2000], Evaluation Loss: 23.4860, Evaluation BLEU: 0.0000\n",
      "Epoch [1814/2000], Evaluation Loss: 23.4859, Evaluation BLEU: 0.0000\n",
      "Epoch [1815/2000], Evaluation Loss: 23.4857, Evaluation BLEU: 0.0000\n",
      "Epoch [1816/2000], Evaluation Loss: 23.4855, Evaluation BLEU: 0.0000\n",
      "Epoch [1817/2000], Evaluation Loss: 23.4854, Evaluation BLEU: 0.0000\n",
      "Epoch [1818/2000], Evaluation Loss: 23.4852, Evaluation BLEU: 0.0000\n",
      "Epoch [1819/2000], Evaluation Loss: 23.4851, Evaluation BLEU: 0.0000\n",
      "Epoch [1820/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1821/2000], Evaluation Loss: 23.4848, Evaluation BLEU: 0.0000\n",
      "Epoch [1822/2000], Evaluation Loss: 23.4848, Evaluation BLEU: 0.0000\n",
      "Epoch [1823/2000], Evaluation Loss: 23.4847, Evaluation BLEU: 0.0000\n",
      "Epoch [1824/2000], Evaluation Loss: 23.4847, Evaluation BLEU: 0.0000\n",
      "Epoch [1825/2000], Evaluation Loss: 23.4846, Evaluation BLEU: 0.0000\n",
      "Epoch [1826/2000], Evaluation Loss: 23.4846, Evaluation BLEU: 0.0000\n",
      "Epoch [1827/2000], Evaluation Loss: 23.4845, Evaluation BLEU: 0.0000\n",
      "Epoch [1828/2000], Evaluation Loss: 23.4845, Evaluation BLEU: 0.0000\n",
      "Epoch [1829/2000], Evaluation Loss: 23.4844, Evaluation BLEU: 0.0000\n",
      "Epoch [1830/2000], Evaluation Loss: 23.4843, Evaluation BLEU: 0.0000\n",
      "Epoch [1831/2000], Evaluation Loss: 23.4843, Evaluation BLEU: 0.0000\n",
      "Epoch [1832/2000], Evaluation Loss: 23.4842, Evaluation BLEU: 0.0000\n",
      "Epoch [1833/2000], Evaluation Loss: 23.4842, Evaluation BLEU: 0.0000\n",
      "Epoch [1834/2000], Evaluation Loss: 23.4842, Evaluation BLEU: 0.0000\n",
      "Epoch [1835/2000], Evaluation Loss: 23.4842, Evaluation BLEU: 0.0000\n",
      "Epoch [1836/2000], Evaluation Loss: 23.4843, Evaluation BLEU: 0.0000\n",
      "Epoch [1837/2000], Evaluation Loss: 23.4844, Evaluation BLEU: 0.0000\n",
      "Epoch [1838/2000], Evaluation Loss: 23.4844, Evaluation BLEU: 0.0000\n",
      "Epoch [1839/2000], Evaluation Loss: 23.4845, Evaluation BLEU: 0.0000\n",
      "Epoch [1840/2000], Evaluation Loss: 23.4847, Evaluation BLEU: 0.0000\n",
      "Epoch [1841/2000], Evaluation Loss: 23.4848, Evaluation BLEU: 0.0000\n",
      "Epoch [1842/2000], Evaluation Loss: 23.4848, Evaluation BLEU: 0.0000\n",
      "Epoch [1843/2000], Evaluation Loss: 23.4849, Evaluation BLEU: 0.0000\n",
      "Epoch [1844/2000], Evaluation Loss: 23.4849, Evaluation BLEU: 0.0000\n",
      "Epoch [1845/2000], Evaluation Loss: 23.4849, Evaluation BLEU: 0.0000\n",
      "Epoch [1846/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1847/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1848/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1849/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1850/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1851/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1852/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1853/2000], Evaluation Loss: 23.4849, Evaluation BLEU: 0.0000\n",
      "Epoch [1854/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1855/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1856/2000], Evaluation Loss: 23.4850, Evaluation BLEU: 0.0000\n",
      "Epoch [1857/2000], Evaluation Loss: 23.4851, Evaluation BLEU: 0.0000\n",
      "Epoch [1858/2000], Evaluation Loss: 23.4851, Evaluation BLEU: 0.0000\n",
      "Epoch [1859/2000], Evaluation Loss: 23.4852, Evaluation BLEU: 0.0000\n",
      "Epoch [1860/2000], Evaluation Loss: 23.4853, Evaluation BLEU: 0.0000\n",
      "Epoch [1861/2000], Evaluation Loss: 23.4854, Evaluation BLEU: 0.0000\n",
      "Epoch [1862/2000], Evaluation Loss: 23.4856, Evaluation BLEU: 0.0000\n",
      "Epoch [1863/2000], Evaluation Loss: 23.4857, Evaluation BLEU: 0.0000\n",
      "Epoch [1864/2000], Evaluation Loss: 23.4859, Evaluation BLEU: 0.0000\n",
      "Epoch [1865/2000], Evaluation Loss: 23.4861, Evaluation BLEU: 0.0000\n",
      "Epoch [1866/2000], Evaluation Loss: 23.4863, Evaluation BLEU: 0.0000\n",
      "Epoch [1867/2000], Evaluation Loss: 23.4865, Evaluation BLEU: 0.0000\n",
      "Epoch [1868/2000], Evaluation Loss: 23.4867, Evaluation BLEU: 0.0000\n",
      "Epoch [1869/2000], Evaluation Loss: 23.4869, Evaluation BLEU: 0.0000\n",
      "Epoch [1870/2000], Evaluation Loss: 23.4871, Evaluation BLEU: 0.0000\n",
      "Epoch [1871/2000], Evaluation Loss: 23.4873, Evaluation BLEU: 0.0000\n",
      "Epoch [1872/2000], Evaluation Loss: 23.4875, Evaluation BLEU: 0.0000\n",
      "Epoch [1873/2000], Evaluation Loss: 23.4876, Evaluation BLEU: 0.0000\n",
      "Epoch [1874/2000], Evaluation Loss: 23.4878, Evaluation BLEU: 0.0000\n",
      "Epoch [1875/2000], Evaluation Loss: 23.4880, Evaluation BLEU: 0.0000\n",
      "Epoch [1876/2000], Evaluation Loss: 23.4881, Evaluation BLEU: 0.0000\n",
      "Epoch [1877/2000], Evaluation Loss: 23.4883, Evaluation BLEU: 0.0000\n",
      "Epoch [1878/2000], Evaluation Loss: 23.4884, Evaluation BLEU: 0.0000\n",
      "Epoch [1879/2000], Evaluation Loss: 23.4886, Evaluation BLEU: 0.0000\n",
      "Epoch [1880/2000], Evaluation Loss: 23.4887, Evaluation BLEU: 0.0000\n",
      "Epoch [1881/2000], Evaluation Loss: 23.4889, Evaluation BLEU: 0.0000\n",
      "Epoch [1882/2000], Evaluation Loss: 23.4890, Evaluation BLEU: 0.0000\n",
      "Epoch [1883/2000], Evaluation Loss: 23.4892, Evaluation BLEU: 0.0000\n",
      "Epoch [1884/2000], Evaluation Loss: 23.4893, Evaluation BLEU: 0.0000\n",
      "Epoch [1885/2000], Evaluation Loss: 23.4895, Evaluation BLEU: 0.0000\n",
      "Epoch [1886/2000], Evaluation Loss: 23.4897, Evaluation BLEU: 0.0000\n",
      "Epoch [1887/2000], Evaluation Loss: 23.4899, Evaluation BLEU: 0.0000\n",
      "Epoch [1888/2000], Evaluation Loss: 23.4901, Evaluation BLEU: 0.0000\n",
      "Epoch [1889/2000], Evaluation Loss: 23.4903, Evaluation BLEU: 0.0000\n",
      "Epoch [1890/2000], Evaluation Loss: 23.4905, Evaluation BLEU: 0.0000\n",
      "Epoch [1891/2000], Evaluation Loss: 23.4907, Evaluation BLEU: 0.0000\n",
      "Epoch [1892/2000], Evaluation Loss: 23.4910, Evaluation BLEU: 0.0000\n",
      "Epoch [1893/2000], Evaluation Loss: 23.4912, Evaluation BLEU: 0.0000\n",
      "Epoch [1894/2000], Evaluation Loss: 23.4915, Evaluation BLEU: 0.0000\n",
      "Epoch [1895/2000], Evaluation Loss: 23.4918, Evaluation BLEU: 0.0000\n",
      "Epoch [1896/2000], Evaluation Loss: 23.4921, Evaluation BLEU: 0.0000\n",
      "Epoch [1897/2000], Evaluation Loss: 23.4924, Evaluation BLEU: 0.0000\n",
      "Epoch [1898/2000], Evaluation Loss: 23.4927, Evaluation BLEU: 0.0000\n",
      "Epoch [1899/2000], Evaluation Loss: 23.4930, Evaluation BLEU: 0.0000\n",
      "Epoch [1900/2000], Evaluation Loss: 23.4932, Evaluation BLEU: 0.0000\n",
      "Epoch [1901/2000], Evaluation Loss: 23.4935, Evaluation BLEU: 0.0000\n",
      "Epoch [1902/2000], Evaluation Loss: 23.4937, Evaluation BLEU: 0.0000\n",
      "Epoch [1903/2000], Evaluation Loss: 23.4940, Evaluation BLEU: 0.0000\n",
      "Epoch [1904/2000], Evaluation Loss: 23.4942, Evaluation BLEU: 0.0000\n",
      "Epoch [1905/2000], Evaluation Loss: 23.4944, Evaluation BLEU: 0.0000\n",
      "Epoch [1906/2000], Evaluation Loss: 23.4946, Evaluation BLEU: 0.0000\n",
      "Epoch [1907/2000], Evaluation Loss: 23.4949, Evaluation BLEU: 0.0000\n",
      "Epoch [1908/2000], Evaluation Loss: 23.4952, Evaluation BLEU: 0.0000\n",
      "Epoch [1909/2000], Evaluation Loss: 23.4954, Evaluation BLEU: 0.0000\n",
      "Epoch [1910/2000], Evaluation Loss: 23.4957, Evaluation BLEU: 0.0000\n",
      "Epoch [1911/2000], Evaluation Loss: 23.4960, Evaluation BLEU: 0.0000\n",
      "Epoch [1912/2000], Evaluation Loss: 23.4963, Evaluation BLEU: 0.0000\n",
      "Epoch [1913/2000], Evaluation Loss: 23.4966, Evaluation BLEU: 0.0000\n",
      "Epoch [1914/2000], Evaluation Loss: 23.4967, Evaluation BLEU: 0.0000\n",
      "Epoch [1915/2000], Evaluation Loss: 23.4969, Evaluation BLEU: 0.0000\n",
      "Epoch [1916/2000], Evaluation Loss: 23.4970, Evaluation BLEU: 0.0000\n",
      "Epoch [1917/2000], Evaluation Loss: 23.4971, Evaluation BLEU: 0.0000\n",
      "Epoch [1918/2000], Evaluation Loss: 23.4972, Evaluation BLEU: 0.0000\n",
      "Epoch [1919/2000], Evaluation Loss: 23.4973, Evaluation BLEU: 0.0000\n",
      "Epoch [1920/2000], Evaluation Loss: 23.4973, Evaluation BLEU: 0.0000\n",
      "Epoch [1921/2000], Evaluation Loss: 23.4974, Evaluation BLEU: 0.0000\n",
      "Epoch [1922/2000], Evaluation Loss: 23.4974, Evaluation BLEU: 0.0000\n",
      "Epoch [1923/2000], Evaluation Loss: 23.4974, Evaluation BLEU: 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1924/2000], Evaluation Loss: 23.4975, Evaluation BLEU: 0.0000\n",
      "Epoch [1925/2000], Evaluation Loss: 23.4975, Evaluation BLEU: 0.0000\n",
      "Epoch [1926/2000], Evaluation Loss: 23.4975, Evaluation BLEU: 0.0000\n",
      "Epoch [1927/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1928/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1929/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1930/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1931/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1932/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1933/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1934/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1935/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1936/2000], Evaluation Loss: 23.4976, Evaluation BLEU: 0.0000\n",
      "Epoch [1937/2000], Evaluation Loss: 23.4977, Evaluation BLEU: 0.0000\n",
      "Epoch [1938/2000], Evaluation Loss: 23.4977, Evaluation BLEU: 0.0000\n",
      "Epoch [1939/2000], Evaluation Loss: 23.4978, Evaluation BLEU: 0.0000\n",
      "Epoch [1940/2000], Evaluation Loss: 23.4978, Evaluation BLEU: 0.0000\n",
      "Epoch [1941/2000], Evaluation Loss: 23.4979, Evaluation BLEU: 0.0000\n",
      "Epoch [1942/2000], Evaluation Loss: 23.4980, Evaluation BLEU: 0.0000\n",
      "Epoch [1943/2000], Evaluation Loss: 23.4981, Evaluation BLEU: 0.0000\n",
      "Epoch [1944/2000], Evaluation Loss: 23.4983, Evaluation BLEU: 0.0000\n",
      "Epoch [1945/2000], Evaluation Loss: 23.4984, Evaluation BLEU: 0.0000\n",
      "Epoch [1946/2000], Evaluation Loss: 23.4986, Evaluation BLEU: 0.0000\n",
      "Epoch [1947/2000], Evaluation Loss: 23.4988, Evaluation BLEU: 0.0000\n",
      "Epoch [1948/2000], Evaluation Loss: 23.4990, Evaluation BLEU: 0.0000\n",
      "Epoch [1949/2000], Evaluation Loss: 23.4992, Evaluation BLEU: 0.0000\n",
      "Epoch [1950/2000], Evaluation Loss: 23.4994, Evaluation BLEU: 0.0000\n",
      "Epoch [1951/2000], Evaluation Loss: 23.4996, Evaluation BLEU: 0.0000\n",
      "Epoch [1952/2000], Evaluation Loss: 23.4997, Evaluation BLEU: 0.0000\n",
      "Epoch [1953/2000], Evaluation Loss: 23.4998, Evaluation BLEU: 0.0000\n",
      "Epoch [1954/2000], Evaluation Loss: 23.4999, Evaluation BLEU: 0.0000\n",
      "Epoch [1955/2000], Evaluation Loss: 23.5000, Evaluation BLEU: 0.0000\n",
      "Epoch [1956/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1957/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1958/2000], Evaluation Loss: 23.5003, Evaluation BLEU: 0.0000\n",
      "Epoch [1959/2000], Evaluation Loss: 23.5003, Evaluation BLEU: 0.0000\n",
      "Epoch [1960/2000], Evaluation Loss: 23.5003, Evaluation BLEU: 0.0000\n",
      "Epoch [1961/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1962/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1963/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1964/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1965/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1966/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1967/2000], Evaluation Loss: 23.5001, Evaluation BLEU: 0.0000\n",
      "Epoch [1968/2000], Evaluation Loss: 23.5001, Evaluation BLEU: 0.0000\n",
      "Epoch [1969/2000], Evaluation Loss: 23.5000, Evaluation BLEU: 0.0000\n",
      "Epoch [1970/2000], Evaluation Loss: 23.5000, Evaluation BLEU: 0.0000\n",
      "Epoch [1971/2000], Evaluation Loss: 23.5000, Evaluation BLEU: 0.0000\n",
      "Epoch [1972/2000], Evaluation Loss: 23.5000, Evaluation BLEU: 0.0000\n",
      "Epoch [1973/2000], Evaluation Loss: 23.5001, Evaluation BLEU: 0.0000\n",
      "Epoch [1974/2000], Evaluation Loss: 23.5001, Evaluation BLEU: 0.0000\n",
      "Epoch [1975/2000], Evaluation Loss: 23.5002, Evaluation BLEU: 0.0000\n",
      "Epoch [1976/2000], Evaluation Loss: 23.5003, Evaluation BLEU: 0.0000\n",
      "Epoch [1977/2000], Evaluation Loss: 23.5004, Evaluation BLEU: 0.0000\n",
      "Epoch [1978/2000], Evaluation Loss: 23.5005, Evaluation BLEU: 0.0000\n",
      "Epoch [1979/2000], Evaluation Loss: 23.5007, Evaluation BLEU: 0.0000\n",
      "Epoch [1980/2000], Evaluation Loss: 23.5009, Evaluation BLEU: 0.0000\n",
      "Epoch [1981/2000], Evaluation Loss: 23.5011, Evaluation BLEU: 0.0000\n",
      "Epoch [1982/2000], Evaluation Loss: 23.5013, Evaluation BLEU: 0.0000\n",
      "Epoch [1983/2000], Evaluation Loss: 23.5016, Evaluation BLEU: 0.0000\n",
      "Epoch [1984/2000], Evaluation Loss: 23.5019, Evaluation BLEU: 0.0000\n",
      "Epoch [1985/2000], Evaluation Loss: 23.5022, Evaluation BLEU: 0.0000\n",
      "Epoch [1986/2000], Evaluation Loss: 23.5025, Evaluation BLEU: 0.0000\n",
      "Epoch [1987/2000], Evaluation Loss: 23.5028, Evaluation BLEU: 0.0000\n",
      "Epoch [1988/2000], Evaluation Loss: 23.5032, Evaluation BLEU: 0.0000\n",
      "Epoch [1989/2000], Evaluation Loss: 23.5035, Evaluation BLEU: 0.0000\n",
      "Epoch [1990/2000], Evaluation Loss: 23.5039, Evaluation BLEU: 0.0000\n",
      "Epoch [1991/2000], Evaluation Loss: 23.5043, Evaluation BLEU: 0.0000\n",
      "Epoch [1992/2000], Evaluation Loss: 23.5046, Evaluation BLEU: 0.0000\n",
      "Epoch [1993/2000], Evaluation Loss: 23.5050, Evaluation BLEU: 0.0000\n",
      "Epoch [1994/2000], Evaluation Loss: 23.5054, Evaluation BLEU: 0.0000\n",
      "Epoch [1995/2000], Evaluation Loss: 23.5058, Evaluation BLEU: 0.0000\n",
      "Epoch [1996/2000], Evaluation Loss: 23.5062, Evaluation BLEU: 0.0000\n",
      "Epoch [1997/2000], Evaluation Loss: 23.5066, Evaluation BLEU: 0.0000\n",
      "Epoch [1998/2000], Evaluation Loss: 23.5071, Evaluation BLEU: 0.0000\n",
      "Epoch [1999/2000], Evaluation Loss: 23.5075, Evaluation BLEU: 0.0000\n",
      "Epoch [2000/2000], Evaluation Loss: 23.5079, Evaluation BLEU: 0.0000\n"
     ]
    }
   ],
   "source": [
    "##################################### TRAINING ######################################################\n",
    "\n",
    "################### constants\n",
    "TRAIN_BATCH_SIZE = 3 # kept as in the source paper\n",
    "N_EPOCHS = 20 # they trained with 20 epochs\n",
    "max_token_len = 80 # can be configured as needed (Classification data?)\n",
    "LOG_EVERY = 10000\n",
    "\n",
    "################### encode source and target data using encode_batch function\n",
    "source_train_data_encoded = encode_batch((src_train))\n",
    "target_train_data_encoded = encode_batch((tgt_train))\n",
    "\n",
    "################### logging during training\n",
    "'''\n",
    "Configures logging settings using the logging.basicConfig method,\n",
    "to write log messages to a file named \"log_file.log\" with the INFO level.\n",
    "format: timestamp, log level, message\n",
    "'''\n",
    "logging.basicConfig(filename=\"log_file.log\", level=logging.INFO,\n",
    "                format=\"%(asctime)s:%(levelname)s: %(message)s\")\n",
    "CONTEXT_SETTINGS = dict(help_option_names = ['-h', '--help'])\n",
    "\n",
    "#################### set device\n",
    "'''\n",
    "Checks for GPU availability,\n",
    "selects the appropriate device (\"cuda\" or \"cpu\"),\n",
    "prints the chosen device,\n",
    "and then moves the model to that device for computation.\n",
    "TAllows the code to leverage GPU acceleration if a compatible GPU is available,\n",
    "enhancing computational performance for deep learning tasks.\n",
    "'''\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} as device\")\n",
    "model.to(device)\n",
    "\n",
    "################### collate data batches\n",
    "def collate_fn(batch):\n",
    "    '''\n",
    "    custom collation function used in PyTorch for data preprocessing in DataLoader\n",
    "    It defines how individual samples within a batch are combined and transformed\n",
    "    before being fed into the model during training and evaluation.\n",
    "    '''\n",
    "    data_list, label_list, ref_list = [], [], []\n",
    "    for _data, _label, _ref in batch:\n",
    "        data_list.append(_data)\n",
    "        label_list.append(_label)\n",
    "        ref_list.append(_ref)\n",
    "    return data_list, label_list, ref_list\n",
    "\n",
    "################### initialize models\n",
    "'''\n",
    "Initializes the model as an Encoder-Decoder architecture using the EncoderDecoderModel from Hugging Face's transformers library.\n",
    "Model combines an encoder (BERT in this case) and a decoder (GPT-2 in this case) for sequence-to-sequence task.\n",
    "'''\n",
    "bert_model = BertModel.from_pretrained('bert-base-cased')\n",
    "gpt2_model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "################### configuration\n",
    "max_token_len = 80\n",
    "start_token_id = bert_tokenizer.cls_token_id\n",
    "end_token_id = gpt2_tokenizer.eos_token_id\n",
    "\n",
    "################### initialize the Encoder-Decoder model with cross-attention enabled\n",
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained('bert-base-cased', 'gpt2')\n",
    "\n",
    "################### update configuration for the model\n",
    "'''\n",
    "Updates the configuration for the encoder-decoder model:\n",
    "- Configure the start and end token IDs for the encoder and decoder\n",
    "- Set the maximum length for tokenized sequences\n",
    "- Enable cross-attention in the model (model.config.add_cross_attention = True),\n",
    "  allowing the decoder to attend to different parts of the encoded input sequence during decoding.\n",
    "'''\n",
    "model.config.decoder_start_token_id = start_token_id\n",
    "model.config.eos_token_id = end_token_id\n",
    "model.config.max_length = max_token_len\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.add_cross_attention = True\n",
    "\n",
    "################## loss function and optimizer\n",
    "'''\n",
    "Loss Function\n",
    "- nn.CrossEntropyLoss():\n",
    "Initializes the cross-entropy loss function from the torch.nn module.\n",
    "Cross-entropy loss is commonly used for multi-class classification tasks, where the model's output represents class probabilities.\n",
    "\n",
    "Optimizer:\n",
    "- torch.optim.Adam(model.parameters(), lr=0.001):\n",
    "Initializes the Adam optimizer from the torch.optim module.\n",
    "It optimizes the model's parameters (model.parameters()) using the Adam optimization algorithm, a popular variant of stochastic gradient descent (SGD).\n",
    "'''\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-5)\n",
    "\n",
    "################## training loop\n",
    "for epoch in range(N_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for i in range(0, len(source_train_data_encoded[0]), TRAIN_BATCH_SIZE):\n",
    "        batch_source = source_train_data_encoded[0][i:i+TRAIN_BATCH_SIZE]\n",
    "        batch_target = target_train_data_encoded[0][i:i+TRAIN_BATCH_SIZE]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids=batch_source, decoder_input_ids=batch_target)\n",
    "\n",
    "        logits_flat = outputs.logits.view(-1, outputs.logits.size(-1))\n",
    "        target_flat = batch_target.view(-1)\n",
    "\n",
    "        loss = criterion(logits_flat, target_flat)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % LOG_EVERY == 0 and i > 0:\n",
    "            print(f'Epoch [{epoch + 1}/{N_EPOCHS}], Batch [{i + 1}/{len(source_train_data_encoded[0])}], Loss: {running_loss / LOG_EVERY:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    # Evaluate\n",
    "    val_loss, val_bleu = evaluate_model(model, eval_data_loader, criterion, gpt2_tokenizer)\n",
    "    print(f'Epoch [{epoch + 1}/{N_EPOCHS}], Evaluation Loss: {val_loss:.4f}, Evaluation BLEU: {val_bleu:.4f}')\n",
    "    #val_loss, val_bleu, val_sari = evaluate_model(model, eval_data_loader, criterion, gpt2_tokenizer, ref_sentences[0])\n",
    "    #print(f'Epoch [{epoch + 1}/{N_EPOCHS}], Evaluation Loss: {val_loss:.4f}, Evaluation BLEU: {val_bleu:.4f}, Evaluation SARI: {val_sari:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A hunting dog refers to any dog who assists humans in hunting .'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = WikiDataset.open_file(\"../dataset/src_valid.txt\")\n",
    "\n",
    "data[10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
